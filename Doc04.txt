Palabras del documento Doc04.txt

técnicas
análisis
sentimientos
aplicadas
la
valoración
opiniones
lenguaje
español
germán
rosenbrock
sebastián
trossero
andrés
pascal
fac
ciencia
tecnología
univ
autónoma
entre
ríos
ruta
km
oro
verde
entre
ríos
argentina
fac
regional
concepción
uruguay
utn
ing
pereira
concepción
uruguay
entre
ríos
argentina
rosenbrockgermanuadereduar
trosserosebastianuadereduar
andrespascalgmailcom
abstract
en
presente
existen
grandes
cantidades
datos
formato
texto
escritos
lenguaje
natural
disponibles
principalmente
sitios
web
redes
sociales
crece
día
día
el
análisis
manual
volúmenes
de
información
actualmente
impráctico
costoso
hace
necesario
el
uso
técnicas
automatizadas
procesamiento
análisis
la
minería
de
opinión
análisis
sentimientos
estudia
extracción
información
partir
de
datos
subjetivos
relativamente
reciente
en
últimos
años
han
propuesto
varios
modelos
procesamiento
lenguaje
natural
resolver
el
problema
particular
clasificación
sentimientos
en
trabajo
examinamos
rendimiento
varios
modelos
aplicados
caso
donde
textos
escritos
lenguaje
castellano
coloquial
que
representa
desafío
adicional
el
caso
propuesto
conjunto
de
reseñas
películas
extraídas
sitio
wwwcinesargentinoscomar
palabras
claves
minería
opinión
análisis
sentimientos
procesamiento
del
lenguaje
natural
español
data
mining
análisis
subjetivo
introducción
en
proceso
toma
decisiones
fundamental
contar
información
oportuna
confiable
completa
permita
análisis
real
situación
en
ciertos
casos
datos
origen
opiniones
personales
en
forma
previa
web
su
importancia
alta
debido
escasa
cantidad
textos
registraban
opiniones
en
presente
disponibilidad
masiva
tipo
información
surgen
nuevas
oportunidades
desafíos
búsqueda
comprensión
interpretación
de
misma
sin
embargo
búsqueda
sitios
posterior
valoración
las
opiniones
forma
manual
trabajo
intenso
costoso
necesario
contar
sistemas
automaticen
proceso
el
análisis
sentimientos
minería
opiniones
estudia
interpretación
automática
opiniones
sentimientos
expresados
mediante
lenguaje
natural
es
utilizada
organizaciones
ejemplo
análisis
imagen
para
determinar
necesidades
grado
aceptación
nuevos
productos
la
literatura
además
muestra
varios
tipos
aplicaciones
incluyendo
valoración
de
películas
opiniones
deportes
turismo
política
educación
salud
finanzas
automóviles
isbn
este
trabajo
presenta
aplicación
comparación
distintas
técnicas
de
aprendizaje
automático
máquinas
vectores
soporte
svm
clasificador
bayesiano
ingenuo
naïvebayes
máxima
entropía
random
forest
el
enfoque
clásico
bolsa
palabras
técnicas
actuales
utilización
de
embeddings
redes
neuronales
recurrentes
transformers
conocidos
como
modelos
lenguaje
el
caso
estudio
realiza
comentarios
y
valoraciones
usuarios
acerca
películas
extraídas
sitio
wwwcinesargentinoscomar
la
selección
sitio
realizó
como
criterio
disponibilidad
datos
cantidad
opiniones
nivel
de
informalidad
uso
lenguaje
disponibilidad
valoración
registrada
para
cada
opinión
puntuaciones
estrellas
existencia
distintos
aspectos
a
evaluar
cada
opinión
marco
teórico
el
análisis
sentimiento
minería
opinión
estudio
computacional
de
opiniones
sentimientos
emociones
expresadas
través
texto
en
general
las
opiniones
pueden
centrarse
producto
servicio
individuo
una
organización
evento
tema
utilizamos
término
objeto
denotar
la
entidad
destino
comentado
un
objeto
puede
además
tener
conjunto
de
componentes
o
partes
conjunto
atributos
propiedades
cada
componente
puede
tener
propios
subcomponentes
conjunto
atributos
así
sucesivamente
lui
formaliza
conceptos
mediante
siguientes
definiciones
objeto
objeto
entidad
puede
ser
producto
persona
evento
organización
tema
está
asociado
par
o
t
a
t
una
jerarquía
componentes
o
partes
a
conjunto
atributos
o
cada
componente
propio
conjunto
componentes
atributos
opinión
opinión
característica
f
actitud
emoción
o
valoración
positiva
negativa
f
orientación
opinión
orientación
opinión
una
característica
f
indica
si
opinión
positiva
negativa
neutral
asimismo
opinión
puede
ser
directa
respecto
único
objeto
bien
comparativa
expresa
relación
similitudes
diferencias
yo
preferencias
entre
dos
objetos
emitida
titular
opinión
las
características
compartidas
objetos
nuestro
problema
establecer
si
documento
expresa
opinión
positiva
o
negativa
objeto
aplicando
diferentes
técnicas
evaluación
opiniones
sobre
una
misma
base
datos
analizar
desempeños
forma
comparativa
los
métodos
seleccionados
estudio
aprendizaje
supervisado
significa
requiere
conocer
clase
pertenece
la
observación
momento
entrenamiento
los
métodos
naive
bayes
random
forest
regresión
logística
svm
representación
clásica
bolsa
de
palabras
redes
neuronales
recurrentes
embedding
wordvec
último
para
arquitectura
transformers
utilizó
modelo
lenguaje
beto
una
versión
español
modelo
original
bert
a
continuación
realiza
breve
descripción
cada
técnicas
isbn
naïve
bayes
este
algoritmo
clasificación
basa
teorema
bayes
probabilidad
condicional
además
supone
independencia
variables
predictoras
ya
que
en
casos
independencia
real
denomina
naïve
ingenuo
la
clasificación
realiza
método
dada
probabilidad
de
observación
pertenezca
clase
dadas
probabilidades
sus
variables
predictoras
es
técnica
utilizada
base
comparación
random
forest
es
clasificador
consiste
ensamble
múltiples
árboles
decisión
cada
árboles
entrena
subconjunto
registros
un
subconjunto
variables
conjunto
datos
tomados
forma
aleatoria
este
algoritmo
puede
manejar
conjuntos
datos
gran
dimensionalidad
sin
verse
afectado
colinealidad
otra
cualidad
posee
algoritmo
se
puede
obtener
salida
importancia
variables
decir
más
influyen
modelo
es
difícil
interpretar
modelo
caja
negra
dependiendo
los
parámetros
utilizados
casos
puede
caer
overfitting
regresión
logística
la
regresión
logística
también
conocido
clasificador
máxima
entropía
modelo
matemático
utilizado
predecir
resultado
una
variable
categórica
general
dicotómica
función
variables
independientes
predictoras
la
predicción
obtiene
probabilidad
de
pertenecer
cada
clase
una
ventajas
fundamentales
regresión
logística
técnicas
es
que
resultado
modelo
entrenado
puede
interpretar
fácilmente
esto
debe
a
que
coeficiente
obtenido
cada
variable
dependiente
indica
manera
influye
modelo
dicha
variable
otras
ventajas
simplicidad
eficacia
svm
svm
support
vector
machine
algoritmo
clasificación
binario
consiste
encontrar
hiperplano
maximice
separación
las
clases
svm
puede
utilizar
diferentes
kernels
dependiendo
si
datos
son
linealmente
separables
no
parámetro
definir
el
entrenamiento
de
svm
grandes
conjuntos
datos
recomendable
eficiente
wordveclstm
los
word
embeddings
forma
representación
palabras
documento
que
además
representar
palabra
aporta
información
contexto
dentro
del
documento
similaridad
palabras
wordvec
técnica
word
embedding
desarrollada
mikolov
utiliza
representación
de
palabras
vector
multidimensional
de
forma
palabras
relacionadas
o
similares
encuentran
zonas
cercanas
dentro
representación
estos
isbn
vectores
utilizan
luego
entrada
redes
neuronales
realizar
tareas
como
clasificación
traducción
resumen
textos
las
redes
neuronales
recurrentes
lstm
long
short
term
memory
la
capacidad
persistir
información
anteriores
calcular
siguientes
estados
es
útiles
trabajar
secuencias
por
ejemplo
modelos
procesamiento
lenguaje
natural
trata
de
secuencia
palabras
la
limitación
capacidad
recordar
estados
previos
corto
plazo
las
lstm
cambio
tipo
redes
neurales
recurrentes
mismo
comportamiento
largo
plazo
bert
a
finales
google
presenta
nueva
arquitectura
denominada
transformer
propone
quitar
capas
recurrentes
convolucionales
de
las
redes
utilizadas
momento
cambio
mecanismos
capas
atención
estas
capas
atención
codifican
palabras
función
demás
palabras
la
frase
permitiendo
introducir
información
contexto
junto
representación
de
cada
palabra
bert
bidirectional
encoder
representations
from
transformers
un
modelo
lenguaje
diseñado
entrenar
representaciones
bidireccionales
profundas
partir
textos
etiquetar
tomando
cuenta
contexto
izquierdo
derecho
todas
capas
bert
preentrenado
mediante
aprendizaje
supervisado
partir
corpus
gran
tamaño
idioma
inglés
a
diferencia
modelos
secuenciales
recurrentes
tradicionales
arquitectura
de
atención
procesa
toda
secuencia
entrada
vez
permitiendo
los
tokens
entrada
procesen
paralelo
para
superar
limitación
inicial
funcionamiento
sólo
inglés
han
surgido
versiones
soportan
distintos
lenguajes
inclusive
múltiples
lenguajes
en
uno
caso
mbert
para
lenguaje
español
particular
de
los
modelos
conocidos
llama
beto
mismas
características
antes
mencionadas
bert
diferencia
preentrenamiento
se
realizó
textos
español
experimentos
realizados
conjunto
datos
este
estudio
realizado
base
datos
comentarios
extraídos
sitio
web
wwwcinesagentinoscomar
comentarios
reseñas
distintas
películas
que
usuarios
aportan
ninguna
estructura
definida
además
pondera
la
película
puntaje
cinco
estrellas
se
definió
comentario
se
clasifica
positivo
si
posee
cuatro
estrellas
más
el
lote
datos
final
de
comentarios
cuales
etiquetados
positivos
aproximadamente
isbn
métricas
utilizadas
para
evaluar
capacidad
predictiva
modelos
utilizaron
métricas
usuales
para
casos
estudio
definidas
siguiente
manera
accuracy
tptn
tpfptnfn
precission
tp
tpfp
recall
tptpfn
f_score
precision
recall
precisionrecall
donde
tptrue
positive
tntrue
negative
fpfalse
positive
fnfalse
negative
descripción
experimentos
con
fin
obtener
mejor
modelo
cada
algoritmos
realizó
una
búsqueda
hiperparámetros
medio
método
grid
search
entrenando
modelos
con
distintos
valores
parámetros
propios
cada
algoritmo
quitando
dejando
las
stop
words
distintas
cantidades
palabras
frecuentes
los
comentarios
a
continuación
describen
hiperparámetros
ajuste
naïve
bayes
ajusta
parámetro
alpha
cero
uno
un
parámetro
corrección
regularización
evitar
problemas
la
probabilidad
cero
eventos
ocultos
random
forest
define
cantidad
estimadores
corresponde
la
cantidad
árboles
decisión
utilizan
los
valores
posibles
van
en
adelante
límite
superior
regresión
logística
ajusta
parámetro
llamado
solver
posibles
valores
liblinearsag
saga
cada
ajusta
modelo
tomando
distintas
métricas
penalización
svm
caso
algoritmo
define
tipo
kernel
utiliza
los
posibles
valores
son
linear
polynomial
rbf
como
representación
texto
entrada
cuatro
primeros
algoritmos
se
utilizaron
bolsas
palabras
en
adelante
bdp
definen
mediante
vectores
cuyas
columnas
indexadas
cada
palabras
encuentran
el
conjunto
datos
completo
almacena
valores
concurrencia
esas
palabras
comentario
a
método
ajustaron
siguientes
parámetros
grid
search
la
cantidad
palabras
o
columnas
vectores
define
n
la
cantidad
máxima
palabras
utilizar
bdp
cuenta
las
n
palabras
mayor
concurrencia
conjunto
datos
este
parámetro
fue
ajustado
palabras
eliminar
stop
words
stop
words
en
adelante
sw
palabras
lenguaje
que
poseen
riqueza
semántica
ejemplo
conectores
en
los
experimentos
utilizaron
dos
diccionarios
distintos
sw
lenguaje
español
incluido
librería
nltk
generado
partir
mismo
los
valores
ajuste
hiperparámetro
fueron
no
borrar
sw
borrar
diccionario
completo
borrar
diccionario
alternativo
isbn
para
caso
wordvec
embedding
generado
partir
palabras
los
mismos
comentarios
mientras
modelo
transformers
beto
cuenta
con
embedding
preentrenado
palabras
español
para
cada
iteración
parámetros
grid
search
entrenaron
cinco
modelos
distintos
utilizando
técnica
monte
carlo
cross
validation
las
divisiones
del
conjunto
datos
entrenamiento
prueba
realizaron
respectivamente
se
calcularon
métricas
accuracy
presicion
recall
fscore
tomando
última
referencia
determinar
mejor
modelo
seleccionar
sus
hiperparámetros
óptimos
resultados
los
resultados
obtenidos
distintos
algoritmos
muestran
tabla
a
continuación
realiza
breve
descripción
mismos
hiperparámetros
con
obtuvieron
mejores
valores
tabla
puntajes
modelos
clasificación
conjunto
datos
prueba
modelos
accuracy
precision
recall
fscore
naïve
bayes
random
forest
regresión
logística
svm
lstm
wordvec
bert
beto
naïve
bayes
el
resultado
búsqueda
grid
search
algoritmo
obtuvo
mejor
fscore
de
cuatro
primeros
algoritmos
ponderación
aproximada
de
clasificación
correcta
ver
tabla
el
modelo
entrenado
valor
el
parámetro
alpha
palabras
bdp
eliminar
ninguna
stop
word
los
distintos
experimentos
realizados
arrojan
resultados
demuestran
para
esta
aplicación
aumento
parámetro
alpha
aumenta
potencia
predictiva
modelo
resultante
del
mismo
modo
observa
cuantas
más
palabras
utilicen
entrenar
modelo
lleva
aumento
fscore
por
otro
lado
eliminación
sw
resultados
positivos
cuanto
fscore
el
contrario
eliminarlas
mejora
resultados
random
forest
en
caso
algoritmo
realizaron
búsquedas
grid
search
ampliando
la
cantidad
estimadores
aumento
puntajes
significativo
el
modelo
óptimo
encontramos
estimadores
bdp
palabras
isbn
sin
quitar
sws
el
porcentaje
comentarios
correctamente
clasificados
este
modelo
regresión
logística
el
solver
maximizó
fscore
problema
saga
un
puntaje
aproximado
en
caso
cantidad
óptima
palabras
fueron
conformar
bdp
igual
otros
quitar
sws
svm
este
algoritmo
optimizó
kernel
linear
palabras
bdp
sin
quitar
sws
tampoco
de
cuatro
modelos
emplearon
bdp
que
obtuvo
puntaje
bajo
fscore
aproximadamente
la
clasificación
correcta
lstmwordvec
el
mejor
resultado
obtenido
generando
embedding
palabras
quitar
sws
learning
rate
entrenamiento
red
neuronal
se
obtuvo
un
fscore
beto
bert
el
fscore
obtenido
experimento
clasificando
forma
incorrecta
solo
total
comentarios
la
tasa
learning
rate
óptima
fue
batch
size
cantidad
palabras
seleccionadas
por
comentario
tampoco
quitaron
sws
análisis
resultados
tal
esperaba
dos
técnicas
recientes
obtuvieron
mejores
resultados
alrededor
primeras
cuatro
aunque
hay
diferencias
significativa
caso
en
cuanto
preproceso
datos
observó
que
eliminación
stop
words
diccionario
original
librería
nltk
como
modificado
generó
mejores
resultados
si
que
contrario
disminuyó
rendimiento
en
literatura
reciente
existen
distintos
trabajos
clasificación
comentarios
de
películas
escritos
inglés
utilizando
bert
obtuvieron
como
resultado
accuracy
mientras
caso
de
estudio
valor
alcanzado
decir
menos
esta
diferencia
puede
tener
varias
causas
diferencias
propias
lenguaje
pre
entrenamiento
corpus
menor
tamaño
diferencias
nivel
de
informalidad
lenguaje
coloquial
utilizado
incluso
mejor
ajuste
algunos
hiperparámetros
isbn
comentarios
mal
clasificados
para
comparar
comentarios
mal
clasificados
tomamos
cuenta
solo
mejores
modelos
obtenidos
lstmwordvec
beto
del
total
comentarios
del
conjunto
testing
mal
clasificados
utilizando
primer
algoritmo
mientras
bert
teniendo
cuenta
utilizó
mismo
conjunto
testing
experimentos
observó
comentarios
mal
clasificados
ambos
algoritmos
vez
causas
clasificación
errónea
analizando
comentarios
mal
clasificados
encontramos
menos
cinco
posibles
causas
cuales
comentario
obtuvo
clasificación
correcta
casos
que
pesar
comentario
connotación
positiva
etiqueta
original
mismo
negativa
es
decir
autor
del
comentario
escribió
opinión
positiva
película
calificó
negativamente
por
ejemplo
la
película
pareció
buena
mantiene
suspenso
está
muy
bien
filmada
efecto
d
bien
logrado
comedia
entretenida
divertida
pasar
buen
rato
reírse
bastante
cameron
díaz
buena
comedia
elenco
bien
linda
comedia
buenas
actuaciones
actores
complementan
bien
pero
lo
mejor
película
opinión
elección
música
mejor
soundtrack
visto
tiempo
casos
comentarios
calificados
positivamente
usuario
pero
acompañado
comentario
mensaje
negativo
no
terminó
de
convencer
a
peli
pasa
factura
problemas
hora
de
realizarse
la
trama
pesar
ser
interesante
hace
momentos
algo
aburrida
decepcionante
se
nota
falta
media
hora
para
pasar
rato
más
está
hecha
ganas
comentarios
ambiguos
decir
cierto
balance
positivo
y
negativo
por
ejemplo
supero
expectativas
escenas
susto
un
poco
predecibles
primeros
minutos
aburridos
al
pasar
minutos
pelicula
cada
vez
entretenida
frases
sentido
figurado
probablemente
aprendidas
correctamente
modelo
se
paso
suspiro
navegando
aguas
misteriosas
debería
ser
frase
saga
sin
tramos
baches
negación
veces
doble
triple
negación
misma
frase
probable
que
modelos
problemas
invierte
sentido
frase
a
través
negación
no
pelicula
arrepientas
haber
visto
esta
nueva
entrega
aporta
suma
nada
los
primeros
dos
casos
asociados
modelos
sino
datos
sólo
son
problemáticos
entrenamiento
realiza
corpus
posee
una
cantidad
significativa
ellos
respecto
comentarios
ambiguos
solución
parcial
presenta
en
distintos
trabajos
definir
tercer
clase
neutral
casos
cuales
no
está
claro
si
comentarios
positivo
negativo
las
últimas
dos
causas
son
isbn
conocidas
limitaciones
mayoría
modelos
momento
ningún
modelo
comprende
realmente
significado
texto
sino
basan
las
relaciones
coocurrencia
encuentran
palabras
conclusiones
trabajo
futuro
en
trabajo
presenta
aplicación
búsqueda
hiperparámetros
comparación
y
análisis
resultados
distintas
técnicas
aprendizaje
automático
utilizadas
el
procesamiento
lenguaje
natural
el
caso
estudio
conjunto
de
comentarios
lenguaje
español
coloquial
películas
extraídos
sitio
wwwcinesargentinoscomar
los
resultados
indican
técnicas
nuevas
wordveclstm
bert
superiores
modelos
anteriores
aunque
los
porcentajes
acierto
obtenidos
estudio
menores
publicados
sobre
casos
similares
cuales
textos
encuentran
idioma
inglés
algunas
tareas
plantean
trabajo
futuro
son
reetiquetar
comentarios
mal
etiquetados
conjunto
datos
volver
a
ejecutar
experimentos
realizar
ajuste
fino
modelo
beto
utilizando
porcentaje
los
comentarios
conjunto
entrenamiento
agregar
clase
neutra
procesos
entrenamiento
clasificación
discriminar
frases
sentido
literal
figurado
entrenar
clasificadores
separados
cada
caso
referencias
kuatyessenov
sentiment
analysis
of
movie
review
comments
n
li
and
d
d
w
using
text
mining
and
sentiment
analysis
for
online
forums
hotspot
detection
and
forecast
decisionsupportsystems
vol
nº
pp
l
c
fiol
j
s
garcía
m
m
t
miguel
and
s
f
coll
la
importancia
las
comunidades
virtuales
análisis
valor
marca
el
caso
tripadvisor
hong
kong
parís
papers
turisme
nº
pp
c
henriquez
j
guzmán
and
d
salcedo
minería
opiniones
basado
adaptación
al
español
anew
opiniones
acerca
hoteles
procesamiento
lenguaje
natural
vol
pp
s
rill
d
reinel
j
scheidt
and
r
v
zicari
politwi
early
detection
of
emerging
political
topics
on
twitter
and
the
impact
on
conceptlevel
sentiment
analysis
knowledgebased
systems
vol
pp
a
ortigosa
j
m
martín
and
r
m
carro
sentiment
analysis
in
facebook
and
its
application
to
elearning
computers
in
human
behavior
vol
pp
f
greaves
d
ramirezcano
c
millett
a
darzi
and
l
donaldson
use
of
sentiment
analysis
for
capturing
patient
experience
from
freetext
comments
posted
online
journal
of
medical
internet
research
vol
nº
x
dong
q
zou
and
y
guan
setsimilarity
joins
based
semisupervised
sentiment
analysis
neural
information
processing
springer
berlin
heidelberg
from
neural
information
processing
springer
berlin
heidelberg
pp
p
d
turney
thumbs
up
or
thumbs
down
semantic
orientation
applied
to
unsupervised
classification
of
reviews
proceedings
of
the
th
annual
meeting
on
association
for
computational
linguistics
stroudsburg
pa
usa
isbn
liu
b
zhang
l
a
survey
of
opinion
mining
and
sentiment
analysis
in
aggarwal
c
zhai
c
eds
mining
text
data
springer
boston
ma
n
li
and
d
d
w
using
text
mining
and
sentiment
analysis
for
online
forums
hotspot
detection
and
forecast
decision
support
systems
vol
nº
pp
a
abbasi
h
chen
and
a
salem
sentiment
analysis
in
multiple
languages
feature
selection
for
opinion
classification
in
web
forums
acm
transactions
on
information
systems
tois
vol
nº
p
f
pla
and
lf
hurtado
sentiment
analysis
in
twitter
for
spanish
natural
language
processing
and
information
systems
pp
gutiérrez
esparza
guadalupe
margain
fuentes
maría
lourdes
ramírez
real
tania
aglaé
canul
reich
juana
un
modelo
basado
clasificador
naïve
bayes
la
evaluación
desempeño
docente
ried
revista
iberoamericana
educación
a
distancia
volumen
núm
pp
belgiu
m
dragut
l
random
forest
in
remote
sensing
a
review
of
applications
and
future
directions
isprs
journal
of
photogrammetry
and
remote
sensing
volume
pang
bo
lee
lillian
vaithyanathan
shivakumar
thumbs
up
sentiment
classification
using
machine
learning
techniques
emnlp
wang
z
document
classification
algorithm
based
on
kernel
logistic
regression
industrial
and
information
systems
iis
nd
international
conference
on
volume
págs
dalian
ieee
kamran
kowsari
kiana
jafari
meimandi
text
classification
algorithms
a
survey
information
open
access
journals
david
meyer
support
vector
machines
the
interface
to
libsvm
in
package
e
fh
technikum
wien
austria
t
mikolov
i
sutskever
k
chen
et
al
distributed
representations
of
words
and
phrases
and
their
compositionality
arxivv
a
aubaid
a
mishra
text
classification
using
word
embedding
in
rulebased
methodologies
a
systematic
mapping
tem
journal
volume
issue
pages
issn
t
lópez
solaz
j
troyano
j
ortega
f
enríquez
una
aproximación
uso
word
embeddings
tarea
similitud
textos
espanol
procesamiento
lenguaje
natural
revista
n
pág
t
sainath
o
vinyals
a
senior
h
sak
convolutional
long
shortterm
memory
fully
connected
deep
neural
networks
a
vaswani
n
shazeer
n
parmar
et
al
attention
is
all
you
need
j
devlin
m
chang
k
lee
k
toutanova
bert
pretraining
of
deep
bidirectional
transformers
for
language
understanding
arxivv
t
pires
e
schlinger
d
garrette
how
multilingual
is
multilingual
bert
arxivv
j
cañete
g
chaperon
r
fuentes
and
j
ho
spanish
pretrained
bert
model
and
evaluation
data
pmldc
at
iclr
m
munikar
s
shakya
and
a
shrestha
finegrained
sentiment
classification
using
bert
arxivv
l
maltoudoglou
a
paisios
h
papadopoulos
bertbased
conformal
predictor
for
sentiment
analysis
proceedings
of
machine
learning
research
s
garg
and
g
ramakrishnan
bae
bertbased
adversarial
examples
for
text
classification
arxivv
isbn