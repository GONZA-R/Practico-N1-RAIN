Palabras del documento Doc03.txt

detección
clasificación
zeroday
malware
través
data
mining
machine
learning
tamaño
dado
constante
incremento
número
complejidad
ataques
informáticos
mecanismos
convencionales
detección
resultan
ineficientes
mayoría
escenarios
en
contexto
presente
investigación
propone
determinar
si
técnicas
data
mining
machine
learning
pueden
ser
utilizadas
efectivamente
para
entrenamiento
algoritmos
capaces
detectar
clasificar
correctamente
nuevos
tipos
amenazas
machine
learning
malware
seguridad
informática
virus
zeroday
data
mining
inteligencia
artificial
redes
neuronales
teniendo
cuenta
resultados
obtenidos
posible
afirmar
utilización
técnicas
data
mining
machine
learning
clasificar
familias
malware
resultado
efectiva
en
cuanto
problemática
detección
cree
que
contar
con
conjunto
datos
considerablemente
grande
algoritmos
podrían
lograr
resultados
ampliamente
superiores
para
tareas
clasificación
partir
conjunto
de
once
mil
muestras
malwares
construyó
dataset
conformado
atributos
considerados
relevantes
cada
familia
con
datos
implementaron
ejecutaron
distintos
algoritmos
machine
learning
clasificación
en
cuanto
detección
malware
aplicaron
mismas
técnicas
tareas
clasificación
utilizando
como
datos
de
partida
programas
considerados
benignos
cuales
ser
desensamblados
en
cuanto
tarea
clasificación
malware
puede
extender
trabajo
realizado
comprendiendo
mayor
número
familias
mayor
cantidad
muestras
para
aquellas
familias
con
menos
observaciones
para
detección
malware
recomienda
volver
ejecutar
algoritmos
aquí
implementados
cantidad
significativamente
mayor
de
archivos
benignos
conjunto
datos
octubre
augusto
recordon
silvia
ruiz
diaz
licenciatura
sistemas
dra
claudia
pons
facultad
de
inform
atica
universidad
nacional
de
la
plata
detecci
on
y
clasificaci
on
de
zeroday
malware
a
traves
de
data
mining
y
machine
learning
tesina
de
grado
autores
augusto
recordon
silvia
ruiz
diaz
directora
dra
claudia
pons
octubre
ii
success
is
not
ﬁnal
failure
is
not
fatal
it
is
the
courage
to
continue
that
counts
sir
winston
churchill
iii
universidad
nacional
de
la
plata
facultad
informatica
resumen
tesis
licenciatura
sistemas
detecci
on
y
clasificaci
on
de
zeroday
malware
a
traves
de
data
mining
y
machine
learning
por
augusto
recordon
silvia
ruiz
diaz
muchos
estudios
sugieren
que
ultimos
anos
incremento
exponencial
ataques
informaticos
causando
organizaciones
perdidas
ﬁ
nancieras
orden
millones
mientras
muchas
companıas
dedican
tiempo
y
recursos
desarrollo
antivirus
complejidad
velocidad
propagacion
la
capacidad
polimorﬁca
poseen
virus
modernos
representan
enormes
desafıos
para
empresas
motivados
encontrar
nuevas
alternativas
comunidad
de
cientıﬁcos
datos
descubierto
utilizacion
tecnicas
machine
learning
y
deep
learning
deteccion
clasiﬁcacion
malware
puede
ofrecer
opcion
mas
competitiva
para
investigacion
comenzara
realizando
extrac
cion
informacion
conjunto
datos
compuesto
once
mil
archivos
asm
y
bytes
correspondientes
nueve
familias
distintas
malwares
luego
median
te
implementacion
algoritmos
machine
learning
intentara
clasiﬁcar
estos
malwares
correspondientes
familias
de
forma
complementaria
realizara
una
clasiﬁcacion
binaria
deteccion
malwareno
malware
conjunto
redu
cido
programas
benignos
ﬁnalizando
ası
elaboracion
comparaciones
y
conclusiones
v
indice
general
resumen
iii
introduccion
i
marco
teorico
vision
historica
los
riesgos
inherentes
tecnologıa
los
ataques
informaticos
ciberdelito
los
avances
inteligencia
artiﬁcial
gran
capacidad
almacenamiento
alto
poder
procesamiento
software
requerido
resumen
conceptos
seguridad
informatica
hackers
malware
tipos
malware
metodos
deteccion
analisis
estatico
analisis
dinamico
signaturebased
vs
behaviorbased
la
necesidad
machine
learning
resumen
data
mining
tratamiento
datos
el
proceso
data
mining
obtencion
datos
tipos
datos
preprocesamiento
datos
seleccion
ingenierıa
atributos
ingenierıa
atributos
datos
categoricos
normalizacion
atributos
vi
seleccion
atributos
visualizacion
datos
resumen
conceptos
machine
learning
deﬁnicion
surgimiento
machine
learning
etapas
proceso
machine
learning
el
conjunto
datos
tipos
estimacion
predicciones
inferencias
metodos
estimacion
f
metodo
parametrico
metodo
parametrico
el
balance
precision
interpretabilidad
evaluacion
precision
modelo
calidad
ajuste
quality
of
ﬁt
calidad
ajuste
clasiﬁcacion
overﬁtting
underﬁtting
balance
sesgo
varianza
clasiﬁcacion
metodos
aprendizaje
categorizacion
metodos
aprendizaje
resumen
modelos
clasiﬁcacion
logistic
regression
knearest
neighbors
naıve
bayes
support
vector
machines
decision
trees
metodos
ensamble
random
forest
xgboost
redes
neuronales
artiﬁciales
redes
neuronales
profundas
evaluacion
modelos
clasiﬁcacion
matriz
confusion
receiver
operating
characteristic
curve
resumen
vii
ii
implementacion
data
mining
generacion
dataset
conjunto
inicial
datos
archivos
asm
archivos
bytes
las
familias
malware
analisis
seleccion
caracterısticas
mas
relevantes
extraccion
datos
dlls
secciones
codigos
operacion
procesamiento
archivos
asm
totalizacion
ocurrencias
calculo
proporciones
determinacion
features
mas
relevantes
consolidacion
resultados
resumen
snapshots
archivos
asm
captura
snapshots
entrenamiento
red
neuronal
tamanos
archivos
compression
rate
ngramas
detalles
tecnicos
conclusion
analisis
exploratorio
preprocesamiento
datos
estructura
contenido
dataset
valores
nulos
datos
faltantes
analisis
valores
nulos
resolucion
valores
nulos
distribucion
valores
importancia
variables
seleccion
variables
estandarizacion
atributos
correlacion
extraccion
atributos
kernel
pca
resumen
clasiﬁcacion
malware
algoritmos
clasiﬁcacion
knearest
neighbors
implementacion
metricas
evaluacion
random
forest
implementacion
viii
metricas
evaluacion
xgboost
implementacion
metricas
evaluacion
artiﬁcial
neural
networks
implementacion
metricas
evaluacion
comparaciones
conclusiones
deteccion
malware
obtencion
desensamblado
archivos
benignos
generacion
nuevo
dataset
analisis
exploratorio
preprocesamiento
implementacion
solucion
modelo
base
tratamiento
sobreajuste
overﬁtting
complejidad
modelo
tasa
aprendizaje
learning
rate
parada
temprana
early
stop
regularizacion
pesos
weight
regularization
agregado
dropout
resultados
obtenidos
xgboost
solucion
alternativa
modelado
resultados
obtenidos
comparaciones
conclusiones
conclusiones
trabajos
futuros
bibliografıa
ix
indice
ﬁguras
estadıstica
total
ataques
malware
vs
potencially
unwanted
application
pua
almacenamiento
vision
historica
medio
almacenamiento
comparativa
ley
moore
el
proceso
data
mining
sesgo
varianza
conjunto
datos
etiquetado
a
nearest
neighbor
nn
b
nearest
neighbor
nn
maximizacion
margen
constitucion
arboles
decicion
random
forest
como
particiona
conjunto
entrenamiento
evolucion
xgboost
decision
trees
representacion
visual
red
neuronal
simple
extracto
archivo
aguvpoccafmyvdyfgbasm
extracto
archivo
anoozdnbpxirmrbscjbytes
procesamiento
archivos
asm
extraccion
dlls
archivos
asm
extraccion
codigos
seccion
archivos
asm
extraccion
codigos
operacion
archivos
asm
calculo
totalizacion
ocurrencias
determinacion
features
mas
importante
cada
familia
top
dlls
mas
relevantes
cada
clase
malware
top
codigos
operacion
mas
relevantes
cada
clase
malware
top
codigos
seccion
mas
relevantes
cada
clase
malware
obtencion
proporciones
ocurrencias
relevantes
archivo
obtencion
proporciones
ocurrencias
relevantes
archivo
para
dlls
codigos
operacion
seccion
snapshots
x
bytes
cada
familia
malware
x
kfold
utilizado
clasiﬁcar
snapshots
muestra
resultados
red
neuronal
clasiﬁcar
snapshots
extraccion
tamanos
archivos
tamano
archivos
compri
midos
proceso
extraccion
ngramas
top
gramas
mas
relevantes
cada
clase
malware
top
gramas
mas
relevantes
cada
clase
malware
top
gramas
mas
relevantes
cada
clase
malware
extraccion
features
archivos
asm
kde
plot
header
kde
plot
ngrama
proc
mov
push
mov
kde
plot
ngrama
add
pop
kde
plot
seccion
seg
importancia
variables
correlacion
pearson
analisis
componentes
kernel
pca
matriz
confusion
k
nearest
neighbors
roc
k
nearest
neighbors
matriz
confusion
random
forest
roc
random
forest
matriz
confusion
xgboost
roc
xgboost
precision
accuracy
modelo
clasiﬁcacion
familias
perdida
loss
modelo
clasiﬁcacion
familias
matriz
confusion
artiﬁcial
neural
networks
roc
artiﬁcial
neural
networks
correlacion
pearson
clasiﬁcacion
malwareno
malware
importancia
variables
clasiﬁcacion
malwareno
malware
analisis
componentes
kernel
pca
clasiﬁcacion
malwa
reno
malware
precision
accuracy
modelo
clasiﬁcacion
malwareno
malware
perdida
loss
modelo
clasiﬁcacion
malwareno
malware
conﬁguracion
hiper
parametros
clasiﬁcacion
malwareno
malware
precision
accuracy
modelo
clasiﬁcacion
malwareno
malware
perdida
accuracy
modelo
clasiﬁcacion
malwareno
malware
matriz
consuﬁcon
modelo
clasiﬁcacion
malwareno
malware
roc
modelo
clasiﬁcacion
malwareno
malware
xi
precision
accuracy
modelo
clasiﬁcacion
malwareno
malware
utilizando
xgboost
error
modelo
clasiﬁcacion
malwareno
malware
utilizando
xgboost
matriz
confusion
clasiﬁcacion
malwareno
malware
utilizando
xgboost
roc
clasiﬁcacion
malwareno
malware
utilizando
xgboost
capitulo
introduccion
vivimos
tecnologıa
cumple
rol
importante
en
nuestras
vidas
las
personas
utilizan
internet
tipo
actividades
desde
tareas
sensibles
pagar
cuentas
realizar
compras
consultar
ban
carios
mantener
contacto
amistades
familiares
ver
pelıculas
y
leer
noticias
esto
frecuentemente
lleva
instalar
distintas
aplicaciones
en
nuestros
dispositivos
aceptar
terminos
condiciones
sitios
companıas
sin
saber
realmente
que
datos
acceso
que
estos
seran
usados
del
mismo
modo
inadvertidamente
no
diariamente
transmitimos
informacion
sensi
ble
contrasenas
datos
tarjetas
credito
canales
conﬁamos
que
sean
seguros
nadie
esta
escuchando
de
forma
similar
compartimos
informa
cion
personal
ubicacion
fotos
redes
sociales
raramente
deteniendonos
a
pensar
que
lado
puede
haber
alguien
malintencionado
creando
perﬁl
nuestro
actividades
el
numero
aplicaciones
utilidades
internet
es
lugar
dudas
ex
tenso
base
misma
software
software
dispositivos
en
servidores
equipos
intermedios
conectan
software
cual
se
ha
mencionado
conﬁamos
diariamente
sin
embargo
este
toda
construccion
humana
susceptible
errores
vulnerabilidades
escenarios
previstos
mas
aun
errores
vulnerabilidades
vez
detectados
llevan
tiempo
corregir
y
en
casos
depende
companıas
usuarios
sistemas
aplicar
la
actualizaciones
solucionan
problemas
en
contexto
hackers
personas
malintencionadas
aprovechan
las
vulnerabilidades
software
yo
conﬁanza
personas
despliegan
co
nocimientos
herramientas
llevar
cabo
objetivos
quizas
mas
comun
de
estas
herramientas
malware
el
termino
malware
malicious
software
un
programa
cuyo
ﬁn
comprometer
cualquier
computadora
dispositivo
inteligen
te
disenado
hacker
ﬁnes
maliciosos
robar
informacion
conﬁ
dencial
penetrar
redes
danar
infraestructuras
crıticas
etc
estos
programas
pueden
capitulo
introduccion
incluir
virus
worms
troyanos
spyware
bots
rootkits
ransomware
otros
segun
el
prestigioso
instituto
dedicado
seguridad
it
avtest
cada
dıa
producen
mas
de
nuevos
programas
maliciosos
malware
potentially
unwanted
applications
pua
incremento
ultimos
anos
cercano
esto
representa
perdidas
millonarias
companıas
gobiernos
para
sitio
cyber
security
ventures
estima
perdida
anual
danos
relacionados
cibercrimen
alcanzarıa
mil
millones
dolares
nivel
global
estos
datos
realmente
alarmantes
representaran
desafıo
humanidad
debera
enfrentar
las
proximas
decadas
empresas
dedicadas
desarrollo
antivirus
tales
norton
avg
mcafee
kaspersky
otros
hacen
mejor
esfuerzo
hacer
frente
problemati
ca
tradicionalmente
programas
utilizaban
metodo
signaturebased
la
deteccion
malware
la
signature
secuencia
corta
bytes
sirve
para
identiﬁcar
malwares
conocidos
embargo
metodo
capaz
proveer
una
forma
identiﬁcar
ataques
zeroday
malwares
polimorﬁcos
utilizando
tecni
cas
ofuscacion
capaces
crear
cientos
variedades
si
mismo
dado
que
no
cuentan
registros
mismos
en
ultimos
anos
grupos
investigadores
junto
comunidad
antimalware
han
reportado
utilizando
tecnicas
machine
learning
deep
learning
para
el
analisis
deteccion
malware
existen
dos
tecnicas
pueden
aplicar
decidir
si
codigo
programa
benigno
maligno
sea
rea
lizando
analisis
estatico
dinamico
las
tecnicas
analisis
estatico
eje
cutan
codigo
sino
solo
examinan
estructura
propiedades
los
datos
binarios
las
tencicas
analisis
dinamico
cambio
ejecutan
codigo
para
observar
comportamiento
ejecucion
red
ambientes
con
trolados
ejemplo
sandbox
muchos
sistemas
deteccion
malwares
utilizan
analisis
estatico
dinamico
incluso
ambos
el
proposito
investigacion
objetivo
central
evaluar
efectivi
dad
utilizar
distintas
tecnicas
analisis
estatico
relacionadas
machine
lear
ning
data
mining
deteccion
clasiﬁcacion
malware
aplicar
tecnicas
para
deteccion
temprana
malware
puede
resultar
particular
utilidad
para
identiﬁcar
ataques
zeroday
un
zeroday
malware
desconocido
decir
nue
vo
tipo
codigo
malicioso
aun
creado
parches
revisiones
que
resuelvan
falla
seguridad
el
proceso
clasiﬁcacion
deteccion
sera
llevado
cabo
trabajo
cons
tara
diversas
etapas
comenzando
obtencion
datos
estos
seran
los
httpswwwavtestorgenstatisticsmalware
httpscybersecurityventurescomcybercrimedamagestrillionby
capitulo
introduccion
que
componen
conjunto
partida
estaran
conformados
casi
mues
tras
archivos
malignos
desensamblados
una
vez
hecho
esto
tendra
lugar
cons
truccion
dataset
partir
extraccion
caracterısticas
mas
relevantes
o
consideradas
mayor
interes
conjunto
datos
a
etapa
deno
minamos
data
mining
consideramos
quizas
mas
importantes
mas
tiempo
demandara
toda
investigacion
el
dataset
generado
debera
ser
sometido
a
distintas
tecnicas
preprocesamiento
depurado
preparacion
que
pueda
ser
utilizado
algoritmos
machine
learning
finalmente
realizara
la
construccion
ajuste
modelos
clasiﬁcacion
deteccion
con
siderados
apropiados
dicha
tarea
los
resultados
obtenidos
seran
analizados
y
comparados
empıricamente
basandonos
distintas
metricas
parte
i
marco
teorico
capitulo
vision
historica
el
avance
tecnologıa
ultimos
tiempos
traıdo
consigo
grandes
bene
ﬁcios
poblacion
usuarios
general
avances
campo
medicina
la
seguridad
intercomunicacion
personas
encuentran
fısicamente
en
distintos
lugares
planeta
incluso
permitido
aparicion
nuevos
mer
cados
gran
potencial
crecimiento
sin
embargo
beneﬁcios
vienen
acompanados
riesgos
casi
manera
inherente
ademas
oportuni
dades
tambien
conlleva
amenzazas
principalmente
industria
software
en
donde
amenazas
malware
incremento
exponencial
haciendolos
a
solo
mas
soﬁsticados
complejos
sino
tambien
mas
difıciles
detectar
en
siguiente
capıtulo
dara
brevemente
marco
historico
relacionado
ries
go
tecnologıa
ataques
informaticos
mientras
mas
adelante
abor
daran
avances
campo
inteligencia
artiﬁcial
ultimos
anos
los
riesgos
inherentes
tecnologıa
las
nuevas
tecnologıas
proposito
otorgar
diferentes
beneﬁcios
las
personas
logrado
exitosamente
sin
embargo
tambien
existen
conse
cuencias
adversas
derivadas
progreso
segun
sitio
welivesecurity
eset
estos
riesgos
asociados
tecnologıa
inteligencia
artiﬁcial
ai
denominado
mayor
avance
tecnologicode
ultimos
anos
comenzado
mostrar
in
dicios
peligro
por
ejemplo
fake
news
noticias
falsas
deep
fakes
imagenes
persona
alterada
digitalmente
ası
ca
racterısticas
aun
vislumbrado
completo
interfaces
computadoracerebro
hıperautomatizacion
quiere
decir
combina
cion
robotica
inteligencia
articial
httpswwwwelivesecuritycomlaesciberataquesprincipalesamenazas
httpswwwesetcom
capitulo
vision
historica
tecnologıa
movil
quinta
generacion
g
estas
nuevas
tecnologıas
depen
den
infraestructuras
alta
velocidad
embargo
dadas
condiciones
actuales
pronostican
deﬁcits
signiﬁcativos
capacidad
cobertura
y
de
inversiones
redes
telecomunicaciones
el
desafıo
estara
cons
truir
infraestructura
moderna
ademas
introducir
sistemas
seguros
y
conﬁables
dentro
capacidades
existentes
computacion
cuantica
la
computacion
cuantica
podrıa
reducir
drasticamen
te
tiempo
necesario
resolver
problemas
matematicos
actual
mente
apoyan
tecnicas
cifrado
esto
importante
cuenta
que
capacidad
procesamiento
podrıa
volver
impracticos
algoritmos
criptograﬁcos
actualidad
correrıa
riego
inutilizar
la
mayorıa
sistemas
actuales
infraestructura
crıtica
seguridad
de
los
datos
computacion
nube
la
computacion
nube
potencial
de
desarrollar
distintos
sectores
expandir
acceso
tecnologico
areas
remotas
ası
vincularse
tecnologıas
mismo
tiempo
mayor
cantidad
datos
alojados
nube
empresas
estan
acumulando
cada
vez
mas
informacion
personal
crea
potenciales
riesgos
privacidad
y
seguridad
datos
ademas
riesgos
asociados
tecnologıa
tambien
destacan
aspectos
re
lacionados
ciberseguridad
ataques
ciberneticos
adoptan
multiples
formas
estan
extendiendo
incluso
ambiente
fısico
en
sentido
cibera
taques
infraestructuras
crıticas
comienzan
aparecer
normalidad
industrias
como
energetica
salud
transporte
afectando
incluso
ciudades
enteras
mientras
tecnologıa
campo
encuentran
permanente
crecimiento
tambien
cibercrimen
ataques
perpetrados
grupos
cada
vez
mas
or
ganizados
cuentan
gran
disponibilidad
herramientas
creadas
para
tales
ﬁnes
probabilidad
baja
ser
detectados
enjui
ciados
los
ataques
informaticos
ciberdelito
la
ultima
decada
mostrado
tener
importante
incremento
ataques
in
formaticos
razon
tal
crecimiento
debe
cambio
motivacion
detras
de
estos
ataques
hasta
hace
anos
principal
objetivo
ver
que
tan
lejos
se
podıa
llegar
sin
embargo
dejado
ser
caso
en
actualidad
ataques
esconden
gran
gama
intereses
redito
economico
mas
preponde
rante
incluso
empezado
realizar
ataques
ﬁn
de
robar
informacion
yo
desestabilizarlos
los
ataques
informaticos
ciberdelito
el
siguiente
graﬁco
permite
observar
claramente
aumento
numero
ata
ques
registran
cada
ano
correspondientes
ultima
decada
figura
estadıstica
total
ataques
avtest
institutos
mas
reconocidos
dedican
testeo
produc
tos
seguridad
it
registra
infecciones
dıa
malware
tipo
de
ataque
conocido
mas
comun
el
graﬁco
encuentra
continuacion
muestra
una
estadıstica
ultimo
ano
correspondiente
dos
principales
tipos
ataques
capitulo
vision
historica
figura
malware
vs
potencially
unwanted
application
pua
estos
ciberataques
solo
impactan
industria
software
sino
tambien
la
economıa
empresas
gobiernos
deben
responder
ante
dichos
ataques
los
avances
inteligencia
artiﬁcial
el
termino
inteligencia
artiﬁcial
acunado
mas
anos
despues
los
avances
campo
sorprendentes
hasta
hace
pocos
anos
la
inteligencia
artiﬁcial
parecıa
asunto
futurista
difıcilmente
podrıa
alternarnos
el
dıa
dıa
corto
plazo
sin
embargo
hoy
realidad
aspira
a
revolucionar
varios
aspectos
sociedad
proximos
anos
los
avances
inteligencia
artiﬁcial
hora
superar
capacidad
humana
en
actividades
ajedrez
juego
go
traduccion
llegan
ahora
titu
lares
inteligencia
artiﬁcial
esta
presente
industria
desde
menos
la
decada
la
razon
recien
ahora
empiezan
verse
aplicaciones
es
utilizarlos
requieren
tres
cosas
alto
poder
calculo
gran
capacidad
almacenamiento
software
apropiado
ası
mismo
tecnologıas
tenıan
tener
costo
aceptable
gran
capacidad
almacenamiento
uno
pilares
mas
importante
data
science
capacidad
almacenar
grandes
volumenes
informacion
ser
guardada
manera
con
ﬁable
transferida
grandes
velocidades
los
avances
inteligencia
artiﬁcial
figura
almacenamiento
vision
historica
en
imagen
anterior
puede
ver
izquierda
unidad
almacenado
de
mega
bytes
cargada
avion
en
unidades
alquiladas
por
dolares
mes
si
ajusta
precio
inﬂacion
casi
anos
despues
mega
bytes
el
doble
capacidad
vendıa
dolares
en
actualidad
apenas
anos
despues
valor
puede
acceder
disco
rıgido
que
provee
veces
almacenado
sin
embargo
ciencia
esta
explorando
en
nuevas
direcciones
prometiendo
revolucionar
modo
almacenamos
informacion
utilizacion
adn
estos
dispositivos
solo
minimizado
drasticamente
tamano
sino
tambien
han
aumentado
capacidad
almacenamiento
exponencialmente
el
siguiente
cuadro
presenta
comparativa
discos
rıgidos
memorias
ﬂash
utiliza
cion
adn
capitulo
vision
historica
figura
medio
almacenamiento
comparativa
alto
poder
procesamiento
para
poder
procesar
grandes
volumenes
datos
actualidad
necesita
un
gran
poder
computo
poder
procesar
la
ley
moore
sigue
observando
fecha
permite
poner
perspectiva
aumento
poder
de
calculo
permite
ver
como
hace
pocos
anos
computadoras
estaban
muy
lejos
lograr
capacidad
computacional
poseen
actualmente
figura
ley
moore
la
ley
moore
establece
basicamente
capacidad
computo
duplica
cada
dieciocho
meses
resumen
software
requerido
el
concepto
computadora
inteligente
surgio
prueba
alan
turing
planteo
desde
entonces
siguientes
quince
veinte
anos
tuvie
ron
lugar
esfuerzos
campo
tales
primeros
robots
compu
tadoras
mark
i
perceptron
capaz
aprender
nuevas
habilidades
por
ensayo
error
sin
embargo
ningun
avance
substancial
cuando
deep
blue
derroto
campeon
ajedrecista
garry
kasparov
a
partir
anos
y
hasta
actualidad
machine
learning
inteligencia
artiﬁcial
areas
relacionadas
empezado
crecer
cada
vez
mas
adoptarse
tipo
cam
pos
al
reﬂexionar
acerca
causas
detras
demora
explosion
estas
disciplinas
mencionado
carencias
materia
procesamiento
alma
cenamiento
pero
tambien
destacar
falta
lenguajes
programacion
acordes
sistemas
bases
datos
procesamiento
paralelo
distribuido
otras
herramientas
resumen
en
presente
capıtulo
menciono
como
avances
tecnologicos
impactan
la
vida
personas
riesgos
inherentes
estos
traen
consigo
tambien
se
abordo
tema
ataques
informaticos
cibercrimen
actualidad
como
estos
ido
incrementando
evolucionado
manera
alarmante
por
ultimo
estudio
avance
inteligencia
artiﬁcial
ultimos
anos
y
cuales
pilares
explicarıan
que
uso
venido
popularizado
los
ultimos
anos
en
proximo
capıtulo
veran
conceptos
relacionados
seguridad
in
formatica
distintas
motivaciones
personas
cometen
ataques
in
formaticos
cuales
metodos
pueden
ser
utilizados
detectar
estos
ataques
el
test
turing
estipula
si
persona
interactuando
maquina
puede
darse
cuenta
esta
persona
entonces
constituye
evidencia
maquina
sea
considerada
inteligente
capitulo
conceptos
seguridad
informatica
cuando
habla
seguridad
informatica
reﬁere
practica
preservar
o
defender
aquel
software
computadoras
servidores
dispositivos
moviles
siste
mas
electronicos
redes
bases
datos
ataques
maliciosos
para
ello
impor
tante
conocer
solo
conceptos
relacionados
dicho
tema
sino
tambien
cuales
son
procesos
protocolos
metodos
herramientas
pueden
utilizar
para
hacer
frente
dicha
amenaza
en
siguiente
capıtulo
describiran
ciertos
conceptos
fundamentales
relaciona
dos
seguridad
informatica
se
comenzara
describiendo
como
pueden
clasiﬁcar
se
diferentes
tipos
hackers
descripcion
distintas
clases
malware
que
pueden
encontrarse
ﬁnalmente
cuales
metodos
detectar
estos
programas
hackers
se
utiliza
termino
ingles
hacker
referirse
aquella
persona
experta
en
computadoras
habilidades
conocimientos
tecnicos
especıﬁcos
resolver
un
problema
los
mismos
pueden
ser
clasiﬁcados
siguientes
categorıas
white
hat
hackers
expertos
seguridad
utilizan
distintas
tecnicas
co
mo
penetration
testing
evaluar
tan
segura
informacion
una
organizacion
back
hat
hackers
categorıa
mas
generica
referirse
hackers
en
general
en
grupo
encuentran
aquellos
crean
virus
inﬁltran
en
redes
etc
script
kiddies
termino
derogatorio
usa
referirse
aque
llos
hackers
pocos
conocimientos
simplemente
utilizan
herramientas
creadas
alguien
mas
capitulo
conceptos
seguridad
informatica
hacktivists
atacantes
motivados
razones
polıticas
yo
tambien
religiosas
que
desean
exponer
personas
actividades
ilıcitas
hackers
patrocinados
estados
hackers
contratados
gobiernos
po
nen
disposicion
recursos
ilimitados
atacar
u
organiza
ciones
hackers
espıas
contratados
empresas
objetivo
inﬁltrar
com
petencia
robar
informacion
ciberterroristas
hackers
que
motivados
creencias
polıticas
religiosas
intentan
crear
miedo
caos
sociedad
atacando
infraestructuras
claves
por
lado
proliferado
comunidades
hackers
comparten
conocimiento
y
ademas
distribuyen
herramientas
forma
gratuita
paga
in
cluso
suelen
encontrarse
publicaciones
ofrece
dinero
cambio
des
cubrimiento
nuevas
vulnerabilidades
determinado
sistema
operativo
soft
ware
malware
la
palabra
malware
proviene
abreviacion
malicious
software
puede
ser
utilizado
comprometer
funciones
sistema
robo
informacion
saltear
con
troles
acceso
cualquier
forma
causar
dano
host
esta
ejecutando
tipos
malware
los
malwares
pueden
dividirse
distintas
categorıas
dependiendo
proposi
to
a
continuacion
describen
clases
adware
cuya
abreviatura
proviene
advertisingsupported
software
tipo
de
malware
cuyo
unico
proposito
visualizacion
publicidad
un
ejemplo
comun
podrıa
ser
utilizacion
ventanas
emergentes
sitios
web
aque
llos
ejecutados
software
muchas
veces
aplicaciones
ofrecen
ver
siones
gratuitaspero
estan
atestadas
publicidades
muchos
de
adwares
patrocinados
empresas
publicidad
utilizados
co
mo
herramienta
generar
ganancias
monetarias
bot
los
bots
programas
escritos
ejecutar
determinadas
operaciones
automaticamente
mientras
bots
creados
propositos
no
daninos
juegos
lınea
subastas
internet
concursos
lınea
etc
uti
lizacion
ﬁnes
maliciosos
incrementando
estos
bots
pueden
ser
utilizados
botnets
conjuntos
computadoras
controladas
por
terceros
efectuar
ataques
ddos
distributed
denial
of
service
spambots
malware
que
muestran
publicidades
sitios
web
web
spiders
recopilan
informa
cion
servidores
distribucion
malware
disfrazado
resulta
dos
busquedas
sitios
descarga
bug
en
contexto
software
bug
falla
programa
produce
resultados
deseados
estas
fallas
normalmente
resultado
error
hu
mano
general
encuentran
codigo
fuente
compilado
un
programa
ciertos
bugs
menores
simplemente
afectaran
funcionamiento
del
programa
pueden
llevar
tiempo
ser
detectados
sin
embargo
bugs
mas
signiﬁcativos
pueden
producir
caıdas
sistema
mismos
dejen
de
funcionar
los
bugs
mas
peligrosos
ponen
riesgo
seguridad
del
sistema
pueden
ser
aprovechados
saltear
controles
de
autenticacion
usuarios
sobrescribir
privilegios
robar
informacion
ransomware
el
ransomware
esencia
forma
malware
toma
cau
tivo
sistema
computadora
afectada
solicita
recompensa
su
recuperacion
este
malware
restringira
acceso
usuario
dicha
compu
tadora
sea
encriptando
archivos
encuentran
disco
duro
o
bloqueando
completamente
sistema
mostrando
mensajes
intencion
de
forzar
usuario
pagar
creador
malware
recompensa
remo
ver
restricciones
modo
recuperar
acceso
computadora
rootkit
un
rootkit
tipo
software
malicioso
disenado
acceder
o
controlar
remotamente
computadora
ser
detectado
usuarios
o
programas
seguridad
una
vez
rootkit
instalado
atacante
podra
ejecutar
archivos
accederrobar
informacion
modiﬁcar
conﬁguracio
nes
sistema
alterar
programas
como
ejemplo
programas
seguri
dad
puedan
detectar
rootkit
instalar
programas
maliciosos
controlar
la
computadora
parte
botnnet
la
prevencion
deteccion
remo
cion
tipo
malware
suele
ser
tarea
difıcil
dada
naturaleza
sigilosa
con
operan
spyware
spyware
tipo
malware
cuya
funcion
espiar
actividad
del
usuario
conocimiento
ejemplo
captar
entradas
teclado
y
colectar
datos
informacion
cuentas
autenticaciones
datos
ﬁnancieros
trojan
horse
un
trojan
horse
tambien
conocido
troyano
tipo
de
malware
disfraza
sı
mismo
archivo
normal
enganar
los
usuarios
descargarlos
instalarlos
un
trojan
puede
darle
atacante
acceso
remoto
computadora
infectada
una
vez
ganado
acceso
este
podra
robar
datos
usuario
instalar
programas
maliciosos
modiﬁcar
archi
vos
monitorear
actividad
usuario
virus
un
virus
tipo
malware
capaz
copiarse
sı
mismo
es
parcirse
computadoras
adjuntandose
mismos
programas
capitulo
conceptos
seguridad
informatica
y
ejecutando
codigo
usuario
inicia
dichos
programas
los
virus
pue
den
ser
utilizados
robar
informacion
danar
computadora
aloja
yo
red
crear
botsnets
robar
dinero
mostrar
publicidad
worm
los
worms
computadoras
encuentran
tipos
mas
comunes
de
malware
se
esparcen
computadoras
explotando
vulnerabilida
des
sistemas
operativos
traves
red
estos
worms
causan
dano
la
red
computadora
aloja
consumiendole
ancho
banda
sobrecar
gando
servidores
web
los
worms
computadoras
pueden
ser
clasiﬁcados
como
tipo
virus
poseen
varias
caracterısticas
distinguen
la
mayor
diferencia
worms
habilidad
auto
replicarse
y
esparcirse
independientemente
mientras
virus
requieren
activi
dades
humanos
esparcirse
ejecutar
programa
abrir
un
archivo
backdoor
el
backdoor
tipo
malware
provee
puerta
trasera
del
sistema
atacantes
en
sı
mismo
causa
ningun
dano
provee
a
atacantes
acceso
sistema
modo
este
pueda
hacer
desee
con
el
keylogger
la
idea
detras
malware
registrar
todas
teclas
presio
nadas
usuario
y
modo
almacenar
datos
provistos
incluyendo
contrasenas
numeros
tarjetas
credito
cualquier
infor
macion
sensible
remoteaccess
trojan
rat
este
tipo
malware
permite
atacante
ganar
acceso
remoto
sistema
realizar
cualquier
modiﬁcacion
este
desee
metodos
deteccion
todas
tecnicas
deteccion
malware
pueden
ser
divididas
dos
gran
des
categorıas
signaturebased
behaviorbased
a
vez
existen
dos
conceptos
fundamentales
relacionados
analisis
cuales
clasiﬁcan
en
analisis
estatico
y
analisis
dinamico
malware
el
analisis
estatico
nombre
indica
rea
lizado
estaticamente
ejecucion
archivo
mientras
que
dinamico
el
programa
analizado
ejecucion
ejemplo
maquina
virtual
analisis
estatico
el
analisis
estatico
puede
ser
visto
lectura
codigo
fuente
analiza
su
sintaxis
estructura
archivo
propiedades
malware
intento
por
inferir
determinar
si
existe
comportamiento
malicioso
el
analisis
estatico
puede
incluir
diversas
tecnicas
metodos
deteccion
escaneo
mediante
antivirus
un
antivirus
herramienta
puede
ser
utilizada
deteccion
software
malicioso
estos
normalmente
poseen
una
base
datos
secciones
codigos
sospechosos
ﬁle
signatures
co
mo
ası
tambien
analisis
concordancia
patrones
comportamientos
para
identiﬁcar
archivos
potencialmente
maliciosos
heuristics
hashing
es
metodo
comun
utiliza
identiﬁcar
unıvoca
mente
malware
a
traves
programa
analiza
software
malicioso
generandose
etiqueta
hash
identiﬁca
entre
funciones
hash
mas
comunmente
utilizadas
encuentran
messagedigest
algorithm
md
y
secure
hash
algorithm
sha
busqueda
strings
la
busqueda
extraccion
strings
puede
ofrecer
infor
macion
funcionalidades
programa
actividad
sospechosa
por
ejemplo
si
malware
crea
archivo
nombre
dicho
archivo
almace
nado
string
binario
si
malware
resuelve
nombre
dominio
controlado
atacante
nombre
dominio
sera
almacenado
un
string
la
extraccion
strings
binario
puede
contener
referencias
a
nombres
archivos
urls
nombres
dominio
direcciones
ip
comandos
de
ataque
claves
registro
etc
si
bien
extraccion
sı
misma
otorga
un
entendimiento
completo
acerca
proposito
capacidades
archi
vo
sı
puede
ofrecer
pistas
pueden
ser
utilizadas
entender
que
es
capaz
malware
empaquetamiento
ofuscamiento
malware
los
creadores
malware
fre
cuentemente
utilizan
tecnicas
empaquetamiento
ofuscamiento
hacer
que
archivos
mas
difıciles
detectar
analizar
los
programas
son
ofuscados
esconder
ejecucion
malware
los
programas
empaqueta
dos
pertenecen
subconjunto
ofuscados
cuyo
objetivo
comprimir
el
programa
malicioso
modo
pueda
ser
analizado
archivo
ejecutable
formato
portable
los
metadatos
formatos
ar
chivo
pueden
revelar
informacion
importante
acerca
funcionalidad
de
un
programa
los
archivos
formato
portable
executable
pe
utilizados
por
sistema
operativo
microsoft
windows
r
archivos
poseen
una
estructura
datos
informacion
utilizada
loader
sistema
operativo
ejecucion
archivos
los
archivos
pe
contienen
enca
bezado
incluye
informacion
acerca
codigo
tipo
aplicacion
las
funciones
librerıas
requeridas
requerimientos
espacio
la
informacion
que
pueden
arrojar
encabezados
gran
valor
analisis
de
malware
linkeo
librerıas
funciones
una
forma
recopilar
informacion
acerca
de
un
ejecutable
analizando
lista
funciones
importadas
el
codigo
de
librerıas
puede
ser
linkeado
estaticamente
tiempo
ejecucion
o
capitulo
conceptos
seguridad
informatica
dinamicamente
de
realizarse
estaticamente
encabezado
archivos
pe
podrıa
verse
que
librerıas
incluidas
codigo
el
problema
es
que
general
metodo
mas
comunmente
utilizado
atacantes
el
linkeo
tiempo
ejecucion
dinamico
modo
librerıas
vayan
a
utilizarse
seran
solicitadas
programa
necesite
desensamblado
la
tecnica
desensamblado
disassembler
consiste
to
mar
codigo
compilado
binario
convertirlo
codigo
assembler
utilizando
ingenierıa
inversa
el
mismo
luego
analizado
inferir
logica
inten
ciones
esta
tecnica
mas
comun
mas
conﬁable
analisis
estatico
el
analisis
estatico
menudo
llevado
cabo
ayuda
ciertas
herramien
tas
pero
mas
alla
simple
analisis
estas
pueden
proveer
informacion
tecni
cas
proteccion
utilizan
malwares
la
principal
ventaja
analisis
estatico
es
posibilidad
descubrir
posibles
escenarios
comportamiento
in
vestigar
codigo
sı
mismo
permite
analista
conocer
mayor
detalle
como
se
ejecuta
malware
limitarse
situacion
actual
incluso
tipo
analisis
es
mas
seguro
dinamico
codigo
necesita
ejecutarse
tan
to
pone
riego
sistema
pero
ejecucion
mas
costosa
tiempo
es
por
ello
que
si
bien
resulta
tecnica
interesante
misma
realizada
ambien
tes
dinamicos
mundo
real
tales
antivirus
sino
mas
bien
propositos
de
investigacion
ejemplo
desarrollo
signatures
malware
zeroday
analisis
dinamico
a
diferencia
analisis
estatico
analisis
dinamico
permite
observar
verda
dero
funcionamiento
malware
porque
ejemplo
sola
existencia
action
string
binario
signiﬁca
accion
efecto
vaya
ejecutarse
el
analisis
dinamico
tambien
forma
eﬁciente
identiﬁcar
funcionalidad
malware
por
ejemplo
si
malware
archivo
log
keylogger
analisis
dinamico
provee
posibilidad
localizar
dicho
archivo
sistema
descubrir
que
tipo
de
registros
almacena
descifrar
donde
enviada
dicha
informacion
etc
este
tipo
de
analisis
interno
difıcil
realizarlo
solo
tecnicas
analisis
estatico
si
bien
analisis
dinamico
tecnica
extremadamente
poderosa
deberıa
ser
realizada
vez
completado
analisis
estatico
mismo
pue
de
poner
riesgo
red
sistema
las
tecnicas
dinamicas
tambien
sus
limitaciones
caminos
posibles
pueden
llegar
ejecutarse
mien
tras
malware
esta
funcionamiento
por
ejemplo
caso
malware
que
se
ejecuta
lınea
comando
requiere
argumentos
cada
argumento
podrıa
ejecutar
diferentes
funcionalidades
programa
mientras
sepan
cuales
son
las
opciones
sera
posible
examinar
dinamicamente
todas
funcionalidades
que
puede
realizar
programa
metodos
deteccion
existen
productos
software
pueden
ser
utilizados
llevar
ca
bo
analisis
dinamico
quiza
mas
popular
uso
tecnologıas
sandbox
un
sanbox
mecanismo
seguridad
permite
ejecutar
programas
inseguros
en
ambiente
seguro
danar
integridad
sistema
real
quiere
prote
ger
los
sandboxes
componen
ambientes
virtualizados
menudo
ofrecen
la
simulacion
red
servicios
ofreciendo
modo
seguro
ejecutar
soft
ware
malware
desea
testear
signaturebased
vs
behaviorbased
el
metodo
analisis
signaturebased
metodo
estatico
basa
signatu
res
predeﬁnidas
estos
pueden
ser
archivos
ﬁngerprints
ejemplo
hashes
genera
das
md
sha
strings
estaticos
archivos
metadata
otros
la
deteccion
del
malware
caso
serıa
llevado
cabo
siguiente
manera
llegada
de
nuevo
archivo
sistema
mismo
estaticamente
analizado
software
del
antivirus
si
existe
alguna
coincidencia
cualquiera
signatures
regis
tradas
dispara
alerta
indicando
archivo
considerado
sospechoso
muchas
veces
tipo
analisis
resultan
suﬁciente
dado
malware
son
detectados
basandose
valor
hash
sin
embargo
creadores
malware
comenzado
desarrollar
programas
que
alguna
manera
capaces
cambiar
signature
esta
caracterıstica
en
los
malwares
referida
polimorﬁsmo
por
tanto
dada
condicion
po
limorﬁca
pueden
ser
detectados
solamente
utilizando
tecnicas
deteccion
ba
sadas
signatures
nueva
signature
creada
esta
situacion
llevo
empresas
desarrolladoras
antivirus
utilizar
nuevas
tecnicas
detec
cion
el
analisis
behaviorbased
tambien
conocido
heuristicsbased
metodo
en
observa
como
comporta
malware
ejecucion
buscando
senales
comportamiento
malicioso
modiﬁcaciones
archivos
host
claves
de
registro
establecimiento
conexiones
sospechosas
etc
en
sı
mismas
cada
de
estas
acciones
representan
necesariamente
senales
malware
combinadas
pueden
elevar
nivel
sospecha
acerca
archivo
existe
cierto
umbral
ni
vel
sospecha
deﬁnido
cualquier
malware
exceda
nivel
dispara
una
alerta
el
nivel
precision
deteccion
malware
basado
comportamiento
behaviorbased
dependera
implementacion
las
mas
populares
utilizan
am
bientes
virtuales
ejemplo
sandbox
ejecutar
archivo
monitorear
su
comportamiento
si
bien
metodo
consume
mas
tiempo
mas
seguro
ya
que
archivo
chequeado
ejecucion
la
principal
ventaja
meto
do
deteccion
basado
comportamiento
behaviorbased
que
teorıa
puede
identiﬁcar
solo
familias
malware
sino
tambien
ataques
zeroday
malware
que
capitulo
conceptos
seguridad
informatica
aun
identiﬁcado
virus
polimorﬁcos
sin
embargo
si
cuen
ta
alto
grado
esparcimiento
malware
tales
analisis
resultan
adecuados
cuando
trata
malwares
nuevos
polimorﬁcos
la
necesidad
machine
learning
como
menciono
anteriormente
detectores
malware
basados
signatures
pueden
ser
efectivos
si
malware
conocido
descubier
to
alguna
herramienta
antivirus
sin
embargo
resultan
utiles
detec
cion
aquellos
virus
polimorﬁcos
capaces
cambiar
signature
por
parte
la
precision
detectores
basados
comportamiento
behaviorbased
siem
pre
resulta
ser
adecuada
deteccion
dando
resultado
cantidades
de
falsos
positivos
falsos
negativos
la
necesidad
encontrar
nuevos
metodos
deteccion
esta
dada
alto
grado
propagacion
poseen
virus
polimorﬁcos
esto
llevado
se
comenzaran
explorar
nuevas
alternativas
capaces
brindar
solucion
este
problema
metodos
deteccion
clasiﬁcacion
utilizando
tecnicas
data
mining
y
machine
learning
arrojado
buenos
resultados
campo
resumen
como
pudo
ver
desarrollo
capıtulo
existen
diferentes
moti
vaciones
encuentran
detras
ataques
informaticos
perpetuados
los
hackers
encuentran
ganancia
economica
dano
infraestructuras
y
satisfaccion
personal
tambien
realizo
estudio
diferentes
tipos
malware
como
pueden
clasiﬁcarse
por
ultimo
analizaron
cuales
tecnicas
deteccion
malware
son
utilizadas
hoy
dıa
que
tecnicas
basadas
machine
learning
data
mining
resultan
buena
alternativa
realizacion
dicha
tarea
el
capıtulo
encuentra
continuacion
tendra
objetivo
realizar
estu
dio
indicando
que
consiste
proceso
data
mining
cuales
etapas
que
lo
componen
tambien
estudiaran
tecnicas
utilizadas
realizar
prepro
cesamiento
datos
capitulo
data
mining
tratamiento
los
datos
los
ultimos
anos
solo
mostrado
importante
incremento
numero
y
complejidad
ataques
informaticos
sino
tambien
cantidad
disponibi
lidad
datos
generados
diariamente
las
capacidades
distintas
disciplinas
como
data
mining
machine
learning
estadısticas
otras
necesitadas
para
abordar
desafıos
ciberseguridad
data
mining
minerıa
datos
extraccion
nombre
indica
la
minerıade
conocimiento
partir
gran
cantidad
datos
los
patrones
o
reglas
descubiertas
tecnicas
pueden
luego
ser
utilizadas
realizar
pre
dicciones
respecto
nuevos
datos
las
tecnicas
data
mining
utilizan
combi
nacion
estadısticas
matematicas
inteligencia
artiﬁcial
reconocimiento
patro
nes
modo
agrupar
extraer
comportamientos
entidades
por
tanto
data
mining
campo
interdisciplinario
emplea
uso
herramientas
anali
sis
modelos
estadısticos
algoritmos
matematicos
metodos
machine
learning
para
descubrir
patrones
validos
relaciones
gran
conjunto
datos
otras
caracterısticas
aun
descubiertas
resulta
util
encontrar
tecnicas
procedimientos
utilizados
hackers
vulnerar
sistemas
obtener
informacion
en
siguiente
capıtulo
describira
que
consiste
proceso
data
mining
y
cuales
pasos
llevarlo
cabo
se
abordaran
tambien
conceptos
relacio
nadas
tratamiento
preprocesamiento
datos
por
ultimo
explicara
de
manera
breve
cuales
etapas
relacionadas
machine
learning
que
com
pone
cada
una
capitulo
data
mining
tratamiento
datos
el
proceso
data
mining
antes
comenzar
proceso
data
mining
importante
determinar
que
lo
que
quiere
lograr
implementandolo
conoce
entendimiento
del
negocioo
business
understanding
esta
fase
consiste
investigar
objetivos
y
requerimientos
decidir
si
data
mining
puede
ser
aplicado
alcanzarlos
determinando
que
tipo
datos
deben
ser
recolectados
construir
modelo
desplegable
la
siguiente
fase
consiste
basicamente
entendimiento
datos
data
undertanding
conjunto
datos
inicial
analizado
estudiado
para
determinar
si
apropiado
futuro
procesamiento
si
calidad
datos
es
deﬁciente
pobre
quiza
necesario
recolectar
nuevos
datos
basado
algun
criterio
mas
riguroso
las
siguientes
tres
etapas
son
preparacion
datos
modelado
eva
luacion
la
fase
preparacion
involucra
tareas
preprocesamiento
datos
crudos
que
partir
estos
algoritmos
machine
learning
puedan
pro
ducir
modelo
esta
etapa
preprocesamiento
puede
incluir
actividades
re
quieran
construccion
modelos
muchas
herramientas
preprocesado
construyen
modelos
internos
datos
transformarlos
de
hecho
prepa
racion
datos
modelado
etapas
van
mano
muchas
veces
se
requiere
iterar
ellas
resultados
obtenidos
modelado
sue
len
dar
nueva
perspectiva
puede
afectar
tecnicas
preprocesamiento
elegidas
la
ultima
fase
quiza
mas
importante
depende
exito
de
modelo
etapa
evaluacion
si
etapa
evaluacion
determina
que
modelo
pobre
sera
necesario
reconsiderar
proyecto
entero
regresar
la
fase
entendimiento
negocio
para
identiﬁcar
objetivos
mas
fructıferos
para
la
recoleccion
datos
en
cambio
si
precision
modelo
suﬁcientemente
alta
entonces
siguiente
paso
sera
realizar
despliegue
lo
normalmente
puede
signiﬁcar
integracion
sistema
mayor
funcionar
sistema
en
si
mismo
obtencion
datos
figura
el
proceso
data
mining
obtencion
datos
para
comenzar
proceso
data
mining
sera
necesario
obtener
datos
estos
deberan
ser
recolectados
extraıdos
mundo
real
dado
dichos
datos
podran
ser
obtenidos
diferentes
fuentes
posible
posean
distintos
formatos
los
formatos
datos
mas
conocidos
forma
mas
comun
recolectarlos
son
csv
los
archivos
csv
comma
separated
values
formato
datos
mas
comunmente
utilizado
es
tambien
formatos
mas
antiguos
todavıa
uno
preferidos
diferentes
sistemas
este
tipo
archivos
puede
contener
diferentes
tipos
datos
separados
coma
pueden
contener
encabezados
una
variante
csv
tsv
delimitador
en
lugar
ser
coma
espacio
tabular
jsonla
java
script
object
notation
json
surgio
alternativa
los
archivos
xml
es
formato
texto
totalmente
independiente
lengua
je
ciertas
convenciones
establecidas
un
archivoobjeto
json
simple
mente
coleccion
pares
clavevalor
tal
estructura
pares
su
correspondiente
representacion
mayorıa
lenguajes
diccio
narios
capitulo
data
mining
tratamiento
datos
xml
xml
extensible
markup
language
lenguaje
marcas
deﬁne
reglas
codiﬁcar
datosdocumentos
al
igual
json
legi
bles
humanos
independiente
plataforma
simple
usar
web
scraping
es
tecnica
utiliza
detectar
extraer
informacion
de
web
principalmente
paginas
web
el
proceso
web
scraping
puede
ser
realizado
manualmente
copiando
datos
utilizando
tecnicas
automati
cas
recorrer
extraer
informacion
paginas
esta
informacion
puede
ser
luego
utilizada
herramientas
analisis
almacenada
el
proceso
de
web
scraping
puede
resumir
siguiente
manera
un
robot
crawler
so
licita
servidores
web
conjunto
direcciones
predeterminadas
las
analiza
identiﬁca
enlaces
externos
esta
pueda
contener
anade
la
lista
direcciones
visitar
repitiendo
proceso
posible
una
vez
obtenidas
paginas
realiza
extraccion
informacion
la
tarea
de
scraping
consiste
utilizar
tecnicas
expresiones
regulares
extraccion
basada
xpath
etiquetas
especıﬁcas
otras
acotar
busqueda
de
informacion
requerida
pagina
sql
las
bases
datos
datan
anos
utilizan
representar
grandes
volumenes
informacion
almacenados
forma
relacional
los
da
tos
encuentran
disponibles
tablas
alguna
forma
estructura
de
datos
si
bien
existen
diferentes
formas
trabajar
bases
datos
quizas
mas
comun
dentro
campo
realizando
consultas
sql
directamente
tipos
datos
en
seccion
previa
describio
distintos
formatos
formas
extraer
infor
macion
ellos
cada
formatos
permiten
representar
datos
distintos
tipos
estos
tipos
datos
forma
original
forman
features
entrada
que
utilizaran
algoritmos
machine
learning
a
continuacion
explicaran
tipos
mas
importantes
datos
puede
llegar
trabajar
numerico
es
mas
simple
tipos
datos
disponible
los
datos
numeri
cos
representan
informacion
escalar
acerca
entidades
estan
ob
servadas
ejemplo
numero
visitas
pagina
web
precio
de
un
producto
peso
persona
etc
para
manipulacion
datos
numericos
utilizan
tecnicas
tales
normalizacion
discretizacion
bin
ning
trata
convertir
valor
numerico
valor
nominal
ordena
do
cuantiﬁcacion
otras
transformacion
datos
numericos
de
acuerdo
requerimientos
texto
es
tipo
dato
compuesto
contenido
alfanumerico
desestruc
turado
mas
comunes
los
datos
textuales
representan
contenido
lenguaje
humano
contienen
estructuras
gramaticales
implıcitas
preprocesamiento
datos
y
signiﬁcado
este
tipo
dato
requiere
esfuerzo
adicional
transfor
marlo
entenderlo
categorico
este
tipo
dato
encuentra
situado
tipo
dato
numeri
co
tipo
texto
las
variables
tipo
categoricas
estan
asociadas
categorıas
de
entidades
estan
trabajando
por
ejemplo
si
tipo
cabe
llo
persona
puede
ser
negro
castano
rubio
pelirrojo
situacion
economica
clase
baja
media
alta
los
valores
pueden
ser
represen
tados
numericos
alfanumericos
segun
considere
de
acuerdo
sus
caracterısticas
variables
categoricas
pueden
ser
nominal
estos
deﬁnen
unicamente
categorıa
datos
tener
en
cuenta
ningun
tipo
orden
por
ejemplo
color
cabello
negro
castano
rubio
pelirrojo
categorıas
poseen
ningun
tipo
de
orden
especıﬁco
ordinal
deﬁne
categorıas
tambien
establece
orden
segun
reglas
del
contexto
por
ejemplo
gente
categorizada
situacion
economica
en
bajo
medio
alto
pueden
ser
consideradas
orden
es
importante
destacar
operaciones
matematicas
estandar
suma
la
resta
multiplicacion
division
signiﬁcado
variables
ca
tegoricas
aunque
sintacticamente
viable
hacerlo
como
caso
variables
categoricas
numericas
preprocesamiento
datos
una
vez
obtenidos
datos
procedera
limpieza
transformacion
dentro
de
tareas
involucra
etapa
encuentran
filtrado
datos
la
limpieza
dataset
involucra
tareas
remocion
mani
pulacion
datos
erroneos
faltantes
imprecisos
valores
atıpicos
outlier
etc
tambien
requiere
tareas
estandarizacion
nombres
atributos
para
hacerlo
mas
intuitivo
legible
casteo
tipos
el
casteo
tipos
conversion
tipos
apropiado
da
tos
partes
mas
importantes
etapa
a
menudo
datos
son
convertidos
tipos
datos
incorrectos
extraıdos
fuente
original
diferentes
sistemas
plataformas
manejan
tipos
datos
ma
nera
distinta
darles
datos
tipo
correcto
tarea
muy
importante
transformacion
adaptacion
estructuras
datos
muchas
veces
es
tructuras
datos
creadas
contener
dataset
ser
modiﬁcadas
con
objetivo
facilitar
mas
pasos
intermedios
procesamiento
del
capitulo
data
mining
tratamiento
datos
modelo
nuevas
columnas
incluso
nuevas
estructuras
transitorias
pueden
ser
creadas
facilitar
calculos
auxiliares
identiﬁcar
determinadas
observa
ciones
casos
necesita
cruzar
dos
mas
estructuras
datos
no
se
dispone
medios
obvios
hacerlos
como
podrıa
ser
identiﬁcador
manejo
valores
faltantes
nulos
los
valores
faltantes
pueden
acarrear
di
versos
problemas
ser
interpretados
incorrectamente
algoritmos
hasta
errores
calculos
yo
resultados
ﬁnales
un
metodo
comun
para
rellenar
valores
faltantes
utilizacion
calculo
media
eliminacion
duplicados
si
bien
cierto
cuantos
mas
datos
dis
pongan
mejor
tambien
debe
tener
presente
datos
duplicados
el
dataset
agregan
ningun
valor
adicional
deberıan
ser
eliminados
manipulacion
datos
categoricos
los
atributos
tipo
categorico
reﬁe
ren
aquellos
datos
alfanumericos
pueden
tomar
numero
limitado
de
valores
por
ejemplo
dataset
contiene
columna
genero
cuyos
va
lores
f
femenino
m
masculino
datos
solo
pueden
tomar
esos
dos
valores
considerados
categoricos
normalizacion
valores
el
proceso
normalizacion
consiste
escalado
de
valores
atributos
a
proceso
normalizacion
tambien
lo
conoce
feature
scaling
manipulacion
strings
cuando
quiere
procesar
lenguaje
na
tural
este
consta
propias
etapas
componen
proceso
tokenization
se
divide
string
unidades
componen
por
ejem
plo
dividir
sentencia
palabras
palabras
caracteres
stemming
lemmatizationconsiste
normalizar
palabras
ob
tener
raız
forma
canonica
mientras
stemming
proceso
heurıstico
lograr
forma
canonica
lemmatization
utiliza
reglas
gramaticales
vocabulario
llegar
raız
remocion
stopwordlos
textos
contienen
ciertas
palabras
apare
cen
alta
frecuencia
agregan
mucha
informacion
signos
de
puntuacion
conjunciones
etc
estas
palabrasfrases
removidas
para
reducir
dimension
complejidad
datos
seleccion
ingenierıa
atributos
una
vez
datos
limpiados
transformados
seleccionaran
aque
llas
propiedades
consideradas
relevantes
formar
conoce
como
atributo
feature
seran
representados
dentro
conjunto
datos
como
columnas
los
atributos
pueden
ser
dos
tipos
inherentes
aquellos
se
seleccion
ingenierıa
atributos
obtienen
directamente
dataset
tener
realizar
ningun
tipo
calculo
ni
ingenierıa
derivados
aquellos
obtienen
partir
atributos
existentes
por
ejemplo
partir
fecha
nacimiento
persona
puede
obtener
su
edad
ingenierıa
atributos
datos
categoricos
los
atributos
features
categoricos
representan
conjunto
ﬁnito
valores
dis
tintos
estos
valores
pueden
ser
tipo
texto
numerico
las
cuales
vez
pueden
pertenecer
variables
categoricas
nominales
ordinales
dentro
atributos
ca
tegoricos
nominales
existe
concepto
orden
mientras
ordinales
sı
cuando
valores
alfanumericos
necesario
transformar
atributos
a
valores
numericos
dado
modelo
matematico
puede
trabajar
va
lores
alfanumericos
esta
transformacion
puede
realizar
utilizando
esquema
denominado
dummy
variable
variables
tontas
cual
si
m
valores
de
categorıa
crearan
m
columnas
cuales
completaran
valor
segun
corresponda
excepto
trate
categorıa
representada
cual
se
completaran
ceros
todas
demas
normalizacion
atributos
muchas
veces
necesario
trabajar
valores
numericos
diferen
tes
sı
si
utilizan
datos
entrada
modelo
valores
tal
como
estan
pueden
resultar
modelos
sesgados
cuyas
magnitudes
demasiado
al
tas
algunas
tecnicas
utilizan
realizar
normalizacion
son
escalado
estandar
intenta
estandarizar
cada
valor
columna
contiene
el
atributo
eliminando
valor
medio
escalando
varianza
cada
uno
de
valores
a
proceso
tambien
conoce
centrado
escalado
y
puede
ser
denotado
matematicamente
como
ssxi
xi
µx
σx
donde
cada
valor
atributo
x
resta
media
µx
resultado
es
dividido
desviacion
estandar
σx
escalado
minmax
se
transforma
escala
cada
valor
atributo
tal
dicho
valor
este
dentro
rango
en
terminos
matematicos
serıa
mmsxi
xi
minx
maxx
minx
capitulo
data
mining
tratamiento
datos
donde
escala
cada
valor
atributo
x
sustrayendole
valor
mınimo
x
y
dividiendo
resultado
diferencia
valores
maximo
mınimo
del
atributo
escalado
robusto
una
gran
desventaja
escala
minmax
muchas
veces
presencia
outliers
valores
atıpicos
afectan
escala
cualquier
valor
atributo
la
escala
robusta
utiliza
medidas
estadısticas
escalar
los
atributos
verse
afectado
outliers
matematicamente
escala
se
puede
representar
como
rsxi
xi
mediax
iqrx
donde
escala
cada
valor
atributo
x
sustrayendole
media
x
y
dividiendo
resultado
iqr
interquartile
range
x
cual
serıa
el
rango
diferencia
primer
cuartil
tercer
cuartil
seleccion
atributos
muchas
veces
resulta
conveniente
trabajar
datasets
cuentan
quiza
cientos
atributos
grandes
conjuntos
atributos
conducen
modelos
complejos
y
difıciles
interpretar
posiblemente
overﬁtting
por
tanto
objetivo
selec
cionar
numero
optimo
atributos
representativos
problema
se
quiere
abordar
modo
evitar
diﬁcultades
mencionadas
las
estrategias
seleccion
atributos
pueden
ser
divididas
tres
grandes
areas
metodos
ﬁltrado
estas
tecnicas
seleccionan
atributos
basandose
puramen
te
metricas
correlacion
informacion
mutua
etc
estos
metodos
no
dependen
resultados
obtenidos
ningun
modelo
normalmente
bus
can
relacion
cada
variable
variable
quiere
predecir
entre
los
mas
populares
encuentran
thresholdbased
method
metodos
basados
en
umbral
tests
estadısticos
metodos
envoltorio
se
estudia
interaccion
multiples
atributos
y
mediante
eliminaciones
recursivas
intenta
obtener
subconjunto
estos
atributos
den
resultado
modelo
mas
eﬁciente
los
metodos
de
seleccion
hacia
atras
seleccion
hacia
adelante
mas
populares
esta
categorıa
metodos
embebidos
estas
tecnicas
combinan
beneﬁcios
dos
metodos
asignandoles
puntaje
score
cada
atributo
basado
impor
tancia
arboles
decision
random
forests
mas
populares
dentro
de
esta
categorıa
visualizacion
datos
entre
tecnicas
mas
representativas
seleccion
atributos
encuen
tran
thresholdbased
method
esta
estrategia
seleccion
atributos
permite
es
tablecer
umbral
limitar
cantidad
atributos
posee
modelo
estos
umbrales
pueden
ser
utilizados
diferentes
formas
ejemplo
esta
blecer
mınimo
yo
maximo
incluso
posible
utilizar
varianza
de
modo
aquellos
atributos
cuya
varianza
baja
removidos
metodos
estadısticos
consiste
seleccion
atributos
basados
utili
zacion
tests
estadısticos
existen
varios
test
estadısticos
pueden
ser
usa
dos
regresion
clasiﬁcacion
incluyen
informacion
mutua
anova
analisis
varianza
tests
chisquare
basados
puntajes
obtenidos
de
estos
tests
posible
seleccionar
atributos
arrojen
mejores
resulta
dos
eliminacion
recursiva
atributos
la
recursive
feature
elimination
tambien
conocida
rfe
tecnica
seleccion
atributos
basados
en
voltorios
permite
ayuda
modelo
estimacion
machine
learning
rankear
asignar
puntajes
atributos
luego
eliminar
recursi
vamente
aquellos
menor
llegue
numero
especıﬁ
co
preestablecido
seleccion
basada
modelo
modelos
basados
arboles
de
cision
modelos
ensamble
random
forest
o
arboles
ensamble
pue
den
ser
utilizados
solo
modelar
sino
tambien
seleccion
atri
butos
estos
modelos
pueden
establecer
importancia
atributos
mien
tras
construidos
seleccionando
aquellos
posean
mejor
puntuacion
y
descartando
aquellos
puntuacion
irrelevante
extraccion
atributos
principal
component
analysis
pca
es
meto
do
estadıstico
utiliza
procesos
algebra
lineal
transformar
con
junto
atributos
alta
dimensionalidad
menor
dimensionalidad
con
mınimo
perdida
informacion
visualizacion
datos
finalmente
lugar
visualizacion
datos
este
punto
impor
tante
utilizacion
ayudas
visuales
graﬁcos
imagenes
mapas
constituyen
herramienta
valor
exploracion
validacion
de
datos
disponibles
ası
tambien
deteccion
correccion
temprana
de
errores
que
manera
podrıan
propagarse
modelos
machine
lear
ning
generando
resultados
inesperados
errores
difıciles
identiﬁcar
corregir
capitulo
data
mining
tratamiento
datos
resumen
el
data
mining
minerıa
datos
proceso
identiﬁcacion
informacion
relevante
extraıda
mayorıa
veces
grandes
volumenes
datos
el
objetivo
descubrir
patrones
tendencias
estructurando
informacion
obtenida
de
modo
comprensible
posterior
utilizacion
este
proceso
iterativo
culminara
informacion
obtenida
satisfaga
las
expectativas
conocimiento
esperadas
ser
ası
proceso
repetira
uti
lizando
nuevas
variables
adoptando
tecnicas
distintas
usadas
procesos
anteriores
obtener
modelo
datos
deseado
el
capıtulo
encuentra
continuacion
enunciara
conceptos
machine
lear
ning
comenzando
deﬁnicion
luego
abordar
caracterısticas
mas
especıﬁ
cos
relacionados
dicho
tema
capitulo
conceptos
machine
learning
de
acuerdo
deﬁnicion
clasica
dada
pionero
intenligencia
artiﬁcial
arthur
lee
samuel
machine
learning
conjunto
metodos
da
compu
tadoras
la
habilidad
aprender
ser
programadas
explıcitamente
en
palabras
algoritmo
machine
learning
descubre
generaliza
ca
racterısticas
subyacentes
datos
observa
con
conocimiento
algorit
mo
puede
inferirlas
propiedades
aquellas
muestras
aun
visto
todas
estas
caracterısticas
obtenidas
datos
formalizados
matematicamente
que
se
conoce
modelo
en
deteccion
malware
muestras
vistas
estarıan
repre
sentadas
archivos
asm
bytes
cuales
podrıan
ser
benignos
malignos
zero
day
malware
en
siguiente
capıtulo
expondran
conceptos
relacionados
machine
learning
comenzando
deﬁnicion
etapas
componen
como
funciona
miento
terminos
matematicos
deﬁnicion
originalmente
termino
machine
learning
acunado
arthur
lee
samuels
en
pionero
area
inteligencia
artiﬁcial
juegos
computadora
y
deﬁnio
la
habilidad
computadoras
aprender
ser
explıcitamente
programadas
en
tom
mitchel
formalizo
deﬁncion
terminos
matematicos
racio
nales
indicando
que
se
dice
programa
computadora
aprende
experiencia
e
si
un
conjunto
tareas
t
realiza
medicion
performance
p
dicha
performance
es
incrementada
experiencia
e
capitulo
conceptos
machine
learning
en
donde
tarea
t
consiste
basicamente
deﬁnir
tareas
ejecutaran
para
resolver
problema
mundo
real
cuales
podrıan
ser
clasiﬁcacion
o
categorizacion
regresion
agrupamiento
clustering
la
experiencia
e
obtie
nen
algoritmos
machine
learning
modelos
partir
aprenden
del
conjunto
datos
este
proceso
ir
ganando
experiencia
iterativo
conocido
como
entrenamiento
modelo
en
cuanto
performance
p
medida
cuantitativa
metrica
determinar
tan
bien
algoritmo
modelo
esta
eje
cutando
tarea
t
experiencia
e
medidas
tıpicas
utilizadas
precision
la
exactitud
etc
surgimiento
machine
learning
como
menciono
seccion
referida
avance
inteligencia
artiﬁcial
la
idea
maquinas
conscientes
inteligentes
nuevo
embargo
gra
cias
avances
ultimos
anos
posible
procesar
grandes
volumenes
de
datos
escala
igual
es
ası
podrıa
decir
vivimos
era
in
formacion
principales
desafıos
empresas
organizaciones
el
poder
obtener
informacion
partir
datos
permita
tomar
mejores
decisiones
datadriven
decisions
para
poder
tomar
decisiones
inteligentes
partir
gran
cantidad
datos
no
alcanza
paradigmas
programacion
tradicionales
el
programador
deberıa
contar
conocimiento
amplio
dominio
esta
trabajando
para
poder
determinar
todas
correlaciones
existentes
distintas
variables
de
datos
luego
deberıa
invertir
tiempo
codiﬁcacion
conjuntos
de
reglas
extremadamente
complejos
intentar
llevar
cabo
cualquier
tipo
de
analisis
un
sistema
basado
reglas
datos
disponibles
esta
aproxima
cion
presenta
varias
deﬁciencias
por
lado
ciertas
relaciones
podrıan
ser
menos
evidentes
podrıan
ser
pasadas
alto
por
lado
tipo
sistemas
son
muy
costosos
mantener
cualquier
cambio
introduzca
crear
una
nueva
regla
modiﬁcar
eliminar
existente
costoso
puede
introducir
errores
ası
surgio
concepto
machine
learning
aprendizaje
automatico
una
necesidad
tomar
decisiones
mas
rapidamente
mejor
calidad
a
diferencia
de
los
paradigmas
tradicionales
basados
reglas
machine
learning
utiliza
conjunto
de
datos
denominados
observaciones
construccion
modelo
este
mo
delo
utilizara
dichos
datos
deducir
todas
posibles
patrones
correlaciones
existentes
ellos
a
partir
conocimiento
adquirido
modelo
sera
capaz
de
predecir
valores
salida
nuevas
observaciones
vistas
anteriormente
este
proceso
aprendizaje
modelo
consta
basicamente
dos
etapas
etapas
proceso
machine
learning
training
entrenamiento
objetivo
aprender
partir
conjunto
de
datos
conocidos
para
ello
deben
utilizar
conceptos
matematicos
principal
mente
algebra
lineal
estadıstica
algoritmos
aprendizaje
testing
evaluacion
partir
aprendido
etapa
anterior
esta
etapa
predicen
inﬁeren
resultados
acuerdo
nuevos
datos
estas
dos
etapas
entrenamiento
evaluacion
tambien
conocidos
como
aprendizaje
prediccion
respectivamente
etapas
proceso
machine
learning
el
proceso
machine
learning
podrıa
ser
generalizado
tres
grandes
tareas
representacion
consiste
representar
problema
utilizando
lenguaje
formal
en
primera
etapa
seleccionara
algoritmo
machi
ne
learning
utilizara
por
ejemplo
si
observando
datos
determina
que
problema
regresion
entonces
probable
elija
regresion
lineal
modelo
representarlo
existen
diferentes
tipos
de
modelos
cuales
pueden
ser
clasiﬁcados
acuerdo
categorıas
y
nomenclaturas
muchos
basan
algoritmo
aprendizaje
que
utilizan
metodo
emplean
construirlos
por
ejemplo
mode
lo
puede
ser
lineal
lineal
parametricos
parametricos
supervisado
no
supervisado
semisupervisado
modelo
ensamble
incluso
mode
lo
basado
deep
learning
es
etapa
tambien
seleccionan
los
parametros
pesos
coeﬁcientes
modelo
seran
utilizados
evaluacion
una
vez
decidida
representacion
posibles
modelos
ne
cesitara
algun
criterio
evaluarlos
y
modo
poder
seleccionar
el
mejor
dentro
conjunto
candidatos
en
general
utilizan
metricas
que
retornan
algun
valor
performance
numerico
ayude
decidir
la
efectividad
candidato
optimizacion
la
etapa
ﬁnal
proceso
optimizacion
la
optimizacion
se
puede
describir
busqueda
traves
todas
combinaciones
posi
bles
hiperparametros
encontrar
aquella
de
resultado
modelo
mas
optimo
el
conjunto
datos
el
conjunto
datos
utilizaran
algoritmos
esta
compuesto
serie
de
observaciones
dominio
esta
estudiando
a
vez
obser
vaciones
estan
conformadas
variables
independientes
tambien
conocidas
como
capitulo
conceptos
machine
learning
input
variables
features
distintas
caracterısticas
propias
observa
cion
variable
dependiente
denominada
tambien
target
label
output
que
representa
aquella
caracterıstica
dominio
desea
estudiar
formalmente
variables
independientes
denotan
letra
x
subındi
ce
identiﬁca
de
manera
x
x
xn
conforman
totalidad
las
variables
independientes
conjunto
datos
por
lado
variable
dependiente
comunmente
denotada
letra
y
ası
podemos
formular
si
guiente
ecuacion
y
f
x
ϵ
la
formula
anterior
expresa
variable
dependiente
y
terminos
fun
cion
f
variables
independientes
x
x
xn
ﬁja
desconocida
tambien
incluye
termino
error
independiente
x
cuya
media
como
vera
mas
adelante
cero
f
representa
informacion
sistematica
la
variable
x
genera
respecto
y
pero
mencionado
funcion
f
suele
ser
desconocida
debe
ser
estimada
partir
informacion
las
observaciones
disponen
tipos
estimacion
existen
dos
grandes
razones
justiﬁcan
necesidad
estimar
f
predecir
e
inferir
predicciones
en
escenarios
dispone
gran
numero
observaciones
dis
tintas
variables
independientes
x
sin
embargo
valor
y
puede
ser
difıcil
de
obtener
siguiendo
bajo
asuncion
termino
error
ϵ
tiende
ser
cero
po
sible
formular
ˆy
ˆf
x
en
ˆf
representa
estimacion
funcion
f
ˆy
prediccion
que
obtiene
y
en
contexto
forma
exacta
ˆf
necesariamente
importante
puede
verse
caja
negra
lo
realmente
importa
que
produzca
predicciones
suﬁciente
aproximadas
y
por
ejemplo
puede
suponer
variables
independientes
x
x
xn
re
presentan
distintas
caracterısticas
muestra
sangre
paciente
siendo
la
variable
dependiente
y
riesgo
paciente
sufra
severa
reaccion
adversa
tipos
estimacion
a
determinado
medicamento
particular
de
modo
resulta
importante
poder
estimar
y
ﬁn
evitar
suministro
droga
aquellos
pacientes
la
prediccion
arroje
valor
alto
dicha
prediccion
ˆy
tendra
precision
estara
marcada
dos
valores
error
reducible
error
irreducible
como
mencionado
anteriormente
estimacion
ˆf
funcion
f
sera
perfecta
el
error
produzca
estimacion
denomina
error
reducible
modelo
utilizado
puede
ser
potencialmente
mejorado
au
mentar
precision
reducir
error
sin
embargo
mejore
el
modelo
siempre
habra
pequeno
error
esto
debe
variable
dependiente
y
esta
tambien
deﬁnida
termino
error
ϵ
en
ejemplo
anterior
error
esta
asociado
factores
tales
salud
general
paciente
un
dıa
particular
pequenas
variaciones
fabricacion
medicamento
todos
es
tos
factores
pueden
ser
medidos
modelo
conocidos
error
irreducible
puede
ser
eliminados
retomando
ecuacion
podemos
plantear
siguiente
ecuacion
ey
ˆy
e
f
x
ϵ
ˆf
x
f
x
ˆf
x
reducible
varϵ
irreducible
el
primer
termino
expresion
representa
esperanza
cuadrado
la
diferencia
valor
real
predicho
f
este
termino
importante
ya
conforma
base
tecnica
calculo
errores
diversos
modelos
conocida
mean
squared
error
inferencias
en
ciertos
casos
foco
esta
puesto
predecir
valor
sino
entender
la
relacion
existen
variables
independientes
variable
dependiente
es
decir
como
y
cambia
funcion
x
x
xn
en
contexto
ˆf
puede
ser
vista
caja
negra
debe
conocerse
forma
exacta
posee
los
puntos
mas
importantes
deben
ser
estudiados
son
que
variables
independientes
estan
asociadas
dependiente
que
grado
este
analisis
puede
determinar
solo
pequeno
subconjunto
de
los
atributos
estan
realmente
relacionados
variable
dependiente
esto
ademas
proveer
valiosa
informacion
permitirıa
reducir
complejidad
del
modelo
cual
relacion
variables
independientes
variable
indepen
diente
no
solo
importante
descubrir
aquellos
atributos
guardan
una
alta
correlacion
variable
dependiente
tambien
necesario
entender
la
capitulo
conceptos
machine
learning
naturaleza
dicha
relacion
incrementos
determinadas
variables
indepen
dientes
pueden
conducir
incrementos
decrementos
variable
target
otras
variables
independientes
pueden
tener
efecto
opuesto
que
tan
compleja
dicha
relacion
una
vez
identiﬁcadas
correlaciones
mas
importantes
establecido
que
manera
inﬂuyen
importante
deter
minar
si
suﬁcientemente
simples
ser
modeladas
traves
de
ecuaciones
lineales
si
relacion
mas
compleja
tipo
modelo
es
necesario
ejemplos
tıpicos
inferencia
observan
areas
ﬁnanzas
marketing
y
ventas
estudian
distintas
caracterısticas
hacen
produc
to
venda
mejor
quiere
analizar
como
inﬂuye
rentabilidad
de
un
negocio
factores
ubicacion
proximidad
competencia
precios
y
esquemas
descuentos
metodos
estimacion
f
en
terminos
generales
existen
dos
formas
estimar
f
pueden
ser
caracteri
zados
metodos
parametricos
metodos
parametricos
metodo
parametrico
el
metodo
parametrico
consta
dos
pasos
se
realiza
alguna
asuncion
forma
f
una
primera
asuncion
bastante
frecuente
f
forma
lineal
de
manera
puede
expresar
f
siguiente
manera
f
x
β
βx
βx
βnxn
dado
distintos
valores
variables
independientes
x
x
xn
son
conocidos
tarea
estimar
f
reduce
signiﬁcativamente
en
lugar
de
tener
estimar
funcion
n
dimensional
aleatoria
solo
necesario
esti
mar
n
coeﬁcientes
una
vez
dispone
modelo
siguiente
paso
consiste
utilizar
la
informacion
observaciones
poseen
entrenar
modelo
en
caso
modelo
lineal
presentado
ecuacion
tarea
consiste
estimar
coeﬁcientes
β
β
βn
modo
tal
que
y
β
βx
βx
βnxn
el
balance
precision
interpretabilidad
metodo
parametrico
a
diferencia
metodos
parametricos
parametricos
realizan
nin
guna
asuncion
cuanto
ﬁgura
f
sino
eje
esta
puesto
encontrar
las
estimaciones
f
mas
aproximen
valor
real
esto
presenta
ventaja
importante
dado
que
asumir
linealidad
modelos
parametricos
tienen
la
posibilidad
adaptarse
mejor
grupo
mas
variado
formas
f
el
balance
precision
interpretabilidad
existen
distintos
tipos
metodos
estimacion
f
algunos
producen
modelos
mas
restrictivos
caso
regresiones
lineales
mencionado
que
solo
capaces
producir
funciones
lineales
menos
restrictivos
que
se
adaptan
mejor
familia
mas
amplia
problemas
los
modelos
menos
ﬂe
xibles
si
bien
mas
faciles
entender
visualizar
producen
resultados
menos
acertados
en
teorıa
razonable
suponer
siempre
elegirıa
aquellos
mode
los
producen
mejores
estimaciones
pero
practica
este
siempre
el
caso
existen
diversas
razones
podrıa
preferir
metodo
inﬂexible
por
ejemplo
casos
inferencias
estudia
correlacion
entre
las
variables
independientes
dependiente
puede
ser
preferible
sacriﬁcar
cierto
nivel
precision
objetivo
obtener
modelo
mas
simple
entender
y
transmitir
evaluacion
precision
modelo
muchas
veces
utilizacion
algun
metodo
aprendizaje
puede
lograr
muy
buenos
resultados
conjunto
determinado
datos
pueden
in
cluso
resultar
mejor
conjunto
peor
otros
por
resulta
muy
importante
poder
establecer
dado
conjunto
datos
tiene
que
metodo
produce
mejor
resultado
calidad
ajuste
quality
of
ﬁt
una
principales
medidas
pueden
tomar
evaluar
performan
ce
modelo
frente
conjunto
observaciones
consiste
evaluar
que
tan
aproximadas
predicciones
respecto
valores
reales
en
mo
delos
regresion
tecnica
usada
mas
frecuentemente
calculo
error
cuadratico
medio
mean
squared
error
dado
siguiente
formula
n
n
i
yi
ˆf
xi
capitulo
conceptos
machine
learning
donde
ˆf
xi
prediccion
ˆf
iesima
observacion
el
mse
por
sus
siglas
ingles
sera
pequeno
mientras
valores
predicciones
man
tengan
relativamente
cercanos
verdaderos
valores
y
el
error
cuadratico
medio
puede
ser
computado
training
set
lo
se
conoce
training
mse
poder
tener
primera
impresion
que
tan
pre
ciso
modelo
sin
embargo
calculo
realmente
importante
dado
que
lo
realmente
interesa
conocer
que
tan
acertadas
seran
predicciones
frente
a
informacion
nueva
matematicamente
ˆf
estimada
partir
training
set
observaciones
x
y
x
y
xn
yn
ası
obtienen
estimaciones
ˆf
x
ˆf
x
ˆf
xn
si
es
tas
estimaciones
bastante
cercanas
y
y
yn
tendra
training
mse
ba
jo
sin
embargo
interes
esta
saber
si
ˆf
x
y
sino
determinar
si
ˆf
x
y
x
y
observacion
nueva
utilizada
entrena
miento
modelo
luego
procesar
numero
considerable
observaciones
no
utilizadas
fase
entrenamiento
puede
calcular
test
mse
como
avey
ˆf
x
donde
x
y
todas
observaciones
nuevas
ave
calculo
pro
medio
average
dichas
observaciones
la
funcion
anterior
es
simplemente
otra
forma
expresar
calculo
error
cuadratico
medio
mostro
ecua
cion
desafortunadamente
siempre
cuenta
test
set
debe
elegirse
otra
alternativa
calcular
test
mse
en
casos
suele
hacer
es
tomar
training
mse
asumiendo
modelo
arrojara
test
set
predicciones
con
mse
semejante
obtenido
etapa
entrenamiento
pero
existe
un
problema
fundamental
estrategia
ninguna
garantıa
que
un
modelo
produjo
fase
entrenamiento
mse
mas
pequeno
tambien
lo
haga
etapa
test
calidad
ajuste
clasiﬁcacion
en
seccion
anterior
hablo
calidad
ajuste
modelos
regre
sion
embargo
conceptos
enunciados
tambien
aplican
mo
delos
clasiﬁcacion
sola
modiﬁcacion
ahora
yi
numerica
por
ejemplo
si
busca
estimar
f
bases
observaciones
en
trenamiento
x
y
xn
yn
ahora
y
yn
cualitativos
forma
mas
comun
cuantiﬁcar
precision
estimador
ˆf
error
rate
entrena
miento
representa
proporcion
errores
si
utilizara
estimador
ˆf
en
las
observaciones
conjunto
entrenamiento
encuentra
dada
por
evaluacion
precision
modelo
n
n
i
iyi
ˆyi
en
donde
ˆyi
label
clase
predicha
observacion
iecima
utilizando
el
estimador
ˆf
y
iyi
ˆyi
indicador
variable
si
yi
ˆyi
si
yi
ˆyi
si
iyi
ˆyi
igual
entonces
iecima
observacion
clasiﬁcada
correctamen
te
metodo
clasiﬁcacion
caso
contrario
clasiﬁcada
incorrectamente
por
lo
tanto
ecuacion
computa
fraccion
clasiﬁcaciones
incorrectas
esta
ecua
cion
hace
referencia
training
error
rate
computada
basada
datos
que
usados
entrenar
clasiﬁcador
al
igual
caso
regresion
lineal
interes
centrado
propor
cion
errores
resultan
aplicar
clasiﬁcador
observaciones
test
que
no
utilizadas
entrenamiento
el
test
error
rate
asociado
conjunto
de
observaciones
test
forma
x
y
esta
dado
por
aveiy
ˆy
en
donde
ˆy
label
clase
predicho
resulta
aplicar
clasiﬁcador
las
observaciones
test
predicador
x
un
buen
clasiﬁcador
sera
aquel
cual
el
error
test
mas
pequeno
overﬁtting
underﬁtting
cuando
esta
entrenando
modelo
deseable
mismo
menor
train
mse
posible
sin
embargo
si
continua
aumentando
ﬂexibilidad
mo
delo
solo
objetivo
reducir
valor
modelo
trabajara
manera
intensa
para
aprender
puede
observaciones
dado
empe
zara
adaptarse
incorporar
el
pequenas
variaciones
observaciones
que
pueden
dadas
ﬂuctuaciones
azarosas
balance
sesgo
varianza
estas
dos
medidas
relacionan
capacidad
ajuste
generalizacion
del
modelo
cuando
logra
buen
ajuste
diferencia
datos
reales
y
la
estimacion
modelo
pequena
cuyo
caso
sesgo
tambien
sera
pequeno
pero
buenos
resultados
van
mano
aumento
complejidad
del
modelo
cuando
aumenta
complejidad
modelo
vuelve
sensible
las
pequenas
variaciones
datos
entrada
ﬂuctuando
funcion
datos
por
varianza
aumenta
razon
sera
importante
encontrar
un
balance
sesgo
varianza
capitulo
conceptos
machine
learning
en
terminos
mas
formales
sesgo
varianza
pueden
ser
descriptos
como
varianza
reﬁere
cantidad
ˆf
variarıa
si
entrenado
al
modelo
training
set
diferente
dado
modelo
aprende
partir
los
datos
suministrados
etapa
entrenamiento
natural
supo
ner
distintos
datos
produciran
modelo
distinto
si
observa
va
rianza
alta
quiere
decir
pequenos
cambios
training
data
tendran
un
alto
impacto
estimacion
ˆf
por
general
asume
cuanto
mas
ﬂexible
metodo
mas
alta
varianza
luego
haber
analizado
la
subseccion
deberıa
resultar
intuitivo
mas
ﬂexible
metodo
mas
tratara
ajustarse
cada
observaciones
disponibles
sesgo
error
inherente
existe
modelo
querer
representar
un
problema
complejo
realidad
en
palabras
sesgo
presenta
una
medida
evaluar
que
tan
bien
metodo
adapta
realidad
esta
intentando
modelar
en
caso
relacion
ﬂexibilidad
modelo
es
inversamente
proporcional
metodo
mas
ﬂexible
producira
sesgo
menor
de
modo
test
mse
esta
dado
velocidad
crecimientodecrecimiento
de
cada
dos
propiedades
puede
ser
expresado
matematicamente
de
siguiente
manera
bias
ˆf
x
e
ˆf
x
f
x
la
ﬁgura
figura
permite
observar
graﬁcamente
impacto
varianza
el
sesgo
modelo
una
varianza
alta
producira
valores
mas
dispersos
por
otro
lado
sesgo
elevado
producira
valores
mas
alejado
centro
el
valor
real
clasiﬁcacion
metodos
aprendizaje
figura
sesgo
varianza
clasiﬁcacion
metodos
aprendizaje
los
metodos
aprendizaje
pueden
ser
clasiﬁcados
cuatro
grandes
categorıas
aprendizaje
supervisado
los
metodos
algoritmos
aprendizaje
super
visado
realizan
entrenamiento
datos
entrada
conjunto
en
trenamiento
respectivos
datos
salida
conocido
etiqueta
la
bel
expresado
terminos
matematicos
idea
consiste
algoritmo
pueda
aprender
relaciones
existen
variables
independientes
x
x
xn
variable
dependiente
y
modo
utilizar
aprendiza
je
realizar
predicciones
nuevos
datos
datos
desconocidos
aprendizaje
supervisado
el
aprendizaje
supervisado
requiere
mo
delo
entrenado
serie
observaciones
variables
in
dependientes
dependiente
sin
embargo
casos
in
formacion
variable
dependiente
desconocida
imposible
entrenar
modelo
son
situaciones
modelo
super
visado
extremadamente
util
estos
algoritmos
intentan
aprender
cuales
son
las
relaciones
patrones
estructuras
internas
inherentes
datos
ningun
tipo
ayuda
supervision
caso
aprendizaje
supervisado
el
aprendizaje
supervisado
concentra
mas
intentar
extraer
concocimien
to
informacion
util
datos
mas
predecir
salidas
aprendizaje
semisupervisado
los
metodos
aprendizaje
semisupervisados
son
combinacion
metodos
supervisados
supervisados
estos
metodos
normalmente
utilizan
entrenamiento
gran
cantidad
de
capitulo
conceptos
machine
learning
datos
poseen
etiqueta
pequena
si
existen
multiples
tecnicas
disponibles
forma
metodos
generativos
graﬁcos
basados
metodos
metodos
basados
heurıstica
etc
aprendizaje
refuerzos
en
caso
metodos
aprendizaje
re
fuerzos
agente
quiere
entrenar
ambiente
determinado
perıodo
tiempo
modo
ir
mejorando
su
performance
basandose
acciones
este
ejecuta
dicho
ambien
te
normalmente
agente
comienza
conjunto
estrategias
reglas
para
interactuar
ambiente
al
observar
dicho
ambiente
toma
acciones
particulares
basandose
reglas
polıticas
observando
actual
del
ambiente
de
acuerdo
accion
tomo
agente
obtiene
recom
pensa
penalizacion
este
proceso
iterativo
algoritmo
ira
aprendiendo
modiﬁcando
estrategia
ser
necesario
modo
obtener
la
recompensa
deseada
categorizacion
metodos
aprendizaje
es
posible
categorizar
metodos
aprendizaje
acuerdo
tipo
salida
que
se
desea
obtener
algoritmos
machine
learning
clasiﬁcacion
los
modelos
clasiﬁcacion
encuentran
dentro
meto
dos
aprendizaje
supervisado
objetivo
principal
predecir
una
categorıa
etiqueta
cada
datos
entrada
basandose
que
el
modelo
aprendido
etapa
entrenamiento
estas
etiquetas
salida
tambien
conocidas
clases
etiquetas
clase
cuales
natura
leza
categoricas
poseen
orden
discretas
por
tanto
cada
etiqueta
salida
pertenecera
clase
categorıa
discreta
especıﬁca
existe
amplia
variedad
algoritmos
pertenecen
familia
pero
quizas
mas
importantes
son
modelos
lineales
ejemplo
logistic
regression
regresion
logısti
ca
naıve
bayes
support
vector
machines
modelos
prametricos
k
vecinos
mas
cercanos
knearest
neigh
bors
metodos
basados
arboles
arboles
decision
decision
trees
metodos
ensamble
random
forest
gradient
boosted
machines
boosting
redes
neuronales
los
modelos
clasiﬁcacion
tambien
pueden
ser
caracterizados
clase
a
la
pertenece
dato
salida
cantidad
categorizacion
metodos
aprendizaje
clasiﬁcacion
binaria
cuando
dos
categorıas
di
ferenciar
entonces
problema
clasiﬁcacion
binaria
un
ejemplo
de
clasiﬁcacion
binaria
podrıa
ser
problema
clasiﬁcacion
emails
en
problema
desea
distinguir
dos
categorıas
es
spam
no
spam
clasiﬁcacion
multiclases
se
considera
extension
problema
cla
siﬁcacion
binario
en
caso
mas
dos
categorıas
clases
al
que
dato
puede
pertenecer
por
ejemplo
problema
clasiﬁcacion
de
multiclases
determinar
categorıa
dıgitos
cero
nueve
que
han
escritos
mano
con
cual
problema
de
clasiﬁcacion
diez
clases
clasiﬁcacion
multietiqueta
estos
problemas
clasiﬁcacion
normal
mente
involucran
datos
pueden
pertenecer
mas
categorıa
o
poseer
mas
etiqueta
label
un
ejemplo
prediccion
ca
tegorıa
artıculos
novedades
pueden
tener
multiples
etiquetas
como
polıtica
ciencia
religion
etc
regresion
al
igual
modelos
clasiﬁcacion
estos
pertenecen
cate
gorıa
algoritmos
aprendizaje
supervisados
lugar
predecir
un
valor
discreto
modelos
predicen
valores
reales
continuos
los
metodos
basados
regresiones
entrenados
partir
conjunto
observacio
nes
cada
conformada
conjunto
variables
independientes
un
valor
continuo
dependiente
ası
modelo
hara
uso
valores
pa
ra
aprender
relaciones
existen
features
target
a
partir
de
este
conocimiento
modelo
sabra
predecir
nuevos
valores
continuos
cuando
suministradas
nuevas
observaciones
vista
anteriormente
los
modelos
regresiones
mas
importantes
son
regresion
lineal
simple
simple
linear
regression
en
tipo
mo
delos
solo
variable
independiente
dependiente
la
variable
dependiente
valor
real
normalmente
sigue
dis
tribucion
normal
en
modelos
regresion
asume
existe
una
relacion
lineal
variable
independiente
dependiente
regresion
lineal
multiple
multiple
linear
regression
es
considerada
una
extension
modelo
regresion
lineal
simple
incluye
mas
variable
independiente
al
igual
modelo
lineal
simple
la
prediccion
valor
real
sigue
distribucion
normal
regresion
lineal
no
linear
regression
un
modelo
regresion
en
el
coeﬁcientes
lineales
puede
ser
considerado
modelo
de
regresion
lineal
por
ejemplo
considerese
y
β
log
βx
ε
estos
modelos
difıciles
aprender
utilizados
capitulo
conceptos
machine
learning
clustering
el
clustering
pertenece
modelos
algoritmos
supervi
sados
cuyo
proceso
consiste
agrupar
datos
similares
sido
previamente
etiquetados
clasiﬁcados
las
salidas
tipo
modelos
son
grupos
datos
segregados
similares
sı
diferentes
a
los
miembros
demas
grupos
la
mayor
diferencia
existe
mo
delos
supervisados
aquı
datos
previamente
clasiﬁcados
para
entrenar
modelo
utiliza
conjunto
datos
como
entrada
los
modelos
clustering
pueden
ser
diferentes
tipos
acuerdo
con
metodologıas
principios
clustering
basado
particion
el
metodo
clustering
basado
par
ticion
deﬁnira
nocion
similaridad
esta
similaridad
ca
racterıstica
deriva
atributos
entrada
luego
aplicadas
funciones
matematicas
luego
vez
encontradas
similitudes
se
agrupan
datos
similares
solo
grupo
separados
de
aquellos
diferentes
los
modelos
clustering
basados
par
ticion
general
desarrollados
utilizando
tecnicas
recursivas
para
clasiﬁcarlos
por
ejemplo
comienza
porcion
arbitraria
los
datos
y
basados
alguna
medida
similitud
continua
reasignando
datos
llegue
punto
estable
segun
algun
criterio
ejem
plos
tecnicas
son
kmeans
kmedoids
clarans
etc
clustering
jerarquico
este
tipo
modelos
diferencian
clustering
basado
particion
manera
desarrollados
como
trabajan
dentro
paradigma
clustering
jerarquico
comienza
con
o
bien
datos
solo
grupo
divisive
clustering
los
datos
diferentes
grupos
aglomerativo
segun
punto
entrada
ele
gido
continua
dividiendo
gran
grupo
grupos
mas
pequenos
o
clusters
basados
algun
criterio
similitud
bien
puede
continuar
juntando
diferentes
grupos
clusters
grupos
mas
grandes
basados
en
el
mismo
criterio
el
proceso
concluye
llega
condicion
preestablecida
clustering
basado
densidad
ambos
metodos
mencionados
son
fuertemente
dependientes
nocion
distancia
lo
conduce
es
tos
algoritmos
encontrar
clusters
datos
forma
esferica
esto
puede
ser
problema
si
datos
encuentran
ubicados
arbitrariamente
esta
limitacion
podrıa
ser
resuelta
si
lugar
tener
cuenta
concep
to
distancia
utilizara
concepto
densidad
datos
para
desarrollar
modelo
entonces
metodologıa
encontrar
puntos
ya
no
consiste
encontrar
puntos
proximos
particular
sino
mas
bien
determinar
areas
contengan
puntos
este
tipo
mode
los
resultan
simples
interpretar
metricas
distancia
resumen
pero
ayudan
clusters
necesariamente
forma
esferica
ejemplos
modelos
dbscan
optics
resumen
se
comenzo
capıtulo
deﬁniendo
machine
learning
razon
surgimiento
tambien
menciono
como
compone
conjunto
datos
utiliza
di
chos
algoritmos
seguidamente
abordaron
conceptos
relacionados
tipos
de
estimacion
pueden
presentar
metodos
existen
evaluacion
pre
cision
modelo
balance
medidas
sesgo
varianza
por
ultimo
se
estudiaron
cuales
distintos
metodos
aprendizaje
pueden
encontrarse
y
como
encuentran
categorizados
en
capıtulo
encuentra
continuacion
abordara
mayor
detalle
los
metodos
tecnicas
pertenecientes
diferentes
modelos
clasiﬁca
cion
existentes
cuales
metricas
utilizan
evaluacion
capitulo
modelos
clasiﬁcacion
desde
perspectiva
machine
learning
problema
deteccion
de
malware
identiﬁcacion
familias
cada
muestras
pueden
ser
considerados
problemas
clasiﬁcacion
en
caso
deteccion
malware
donde
intenta
identiﬁcar
si
muestra
es
efecto
programa
malicioso
clasiﬁcacion
binaria
muestra
malware
en
caso
de
clasiﬁcacion
familias
problema
multiclase
dado
debe
determinarse
a
cual
nueve
familias
pertenece
muestra
a
continuacion
dara
marco
teorico
metodos
tecnicas
pertenecientes
a
categorıa
modelos
cuales
seran
utilizados
investiga
cion
logistic
regression
logistic
regression
algoritmo
puede
ser
utilizado
resolver
pro
blemas
regresion
clasiﬁcacion
logistic
regression
tambien
conocido
como
logic
regression
comunmente
utilizado
estimar
probabilidad
ins
tancia
pertenezca
clase
particular
si
probabilidad
estimada
ma
yor
entonces
modelo
predice
instancia
pertenece
clase
llamada
clase
positiva
etiqueta
predice
clase
pertenece
en
cuyo
caso
la
clase
sera
negativa
etiqueta
esto
convierte
clasiﬁcador
binario
es
simple
utilizar
comprender
resulta
efectivo
problemas
en
cuales
conjunto
variables
entrada
bien
conocidas
ademas
se
encuentran
fuertemente
correlacionadas
salidas
aunque
resulta
tan
efec
tivo
variables
entrada
bien
conocidas
relaciones
entre
dichas
variables
complejas
capitulo
modelos
clasiﬁcacion
knearest
neighbors
el
algoritmo
knearest
neighbors
quizas
mas
sencillos
imple
mentar
dado
conjunto
entrenamiento
datos
etiquetados
funcion
de
distancia
knn
clasiﬁca
punto
x
basandose
k
elementos
dicho
conjunto
que
encuentren
mas
cerca
x
figura
conjunto
datos
etiquetado
por
ejemplo
si
considera
graﬁco
figura
dicho
conjunto
entre
namiento
consta
diez
elementos
tipo
cırculo
azul
cuatro
tipo
cuadrados
rojos
figura
a
nearest
neighbor
nn
knearest
neighbors
figura
b
nearest
neighbor
nn
si
quisiera
clasiﬁcar
diamante
gris
etiquetado
x
figu
ra
utilizando
nn
donde
k
dado
que
punto
mas
cercano
x
cua
drado
rojo
etiquetado
b
x
serıa
clasiﬁcado
cuadrado
rojo
por
otro
lado
si
quisiera
clasiﬁcar
x
utilizando
nn
muestra
figura
se
puede
observar
existen
tres
puntos
cercanos
x
con
respecto
distancia
de
euclides
cuadrado
rojo
etiquetado
b
dos
cırculos
azules
etiquetados
como
r
r
respectivamente
dado
mayorıa
puntos
mas
cercanos
x
son
cırculos
rojos
este
serıa
clasiﬁcado
mismo
tipo
el
numero
vecinos
mas
cercanos
k
medida
distancia
componentes
clave
algoritmo
knearest
neighbors
un
valor
pequeno
k
resultara
en
una
baja
precision
conjuntos
datos
ruido
dado
ca
da
instancia
conjunto
entrenamiento
alto
peso
proceso
de
decision
un
valor
grande
k
disminuye
performance
algoritmo
ademas
si
el
valor
grande
modelo
puede
hacer
sobreajuste
overﬁtting
diﬁcual
tando
separacion
clases
tambien
resultarıa
menos
precision
una
buena
regla
utilizar
k
menor
raız
cuadrada
n
n
numero
total
de
patrones
entrenamiento
existen
diferentes
metricas
calculo
distancia
vecinos
mas
cer
canos
mas
conocidas
distancia
hamming
distancia
manhattan
dis
tancia
minkowsky
la
distancia
euclides
metodo
mas
utilizado
se
trata
variables
continuas
en
donde
si
toman
dos
observaciones
espacio
ndimensional
x
x
xn
x
x
xn
distancia
euclides
entre
estos
dos
puntos
esta
dada
por
distx
x
n
i
xi
xi
capitulo
modelos
clasiﬁcacion
la
distancia
euclides
funciona
bien
problemas
atributos
son
mismo
tipo
para
atributos
diferente
tipo
recomendable
utilizar
la
distancia
manhattan
si
bien
facilidad
sencillez
implementar
aspecto
positivo
este
tipo
algoritmos
resultan
aptos
casos
si
conjunto
datos
se
encuentra
desigualmente
distribuido
algoritmo
knearest
neighbors
tendra
una
buena
perfomance
esto
debe
que
si
clase
domina
ampliamente
las
demas
probable
tener
mas
vecinos
clase
debido
gran
tamano
lo
que
llevara
tener
predicciones
incorrectas
naıve
bayes
naive
bayes
algoritmo
clasiﬁcacion
machine
learning
utiliza
teo
rema
bayes
el
mismo
puede
ser
utilizado
problemas
clasiﬁcacion
binario
multiclases
el
objetivo
principal
metodo
tratar
cada
atributo
independientemente
evaluando
probabilidad
cada
sin
tener
cuenta
ninguna
correlacion
hacer
prediccion
basandose
teorema
de
bayes
este
motivo
llamado
naıve
ingenuo
espanol
ya
que
problemas
mundo
real
siempre
existe
algun
tipo
correlacion
los
atributos
para
comprender
mejor
algoritmo
bayes
necesario
introducir
algunos
conceptos
probabilidad
clase
es
probabilidad
clase
dataset
conjunto
de
datos
en
palabras
si
selecciona
manera
aleatoria
elemento
del
dataset
probabilidad
pertenecer
cierta
clase
probabilidad
condicional
es
probabilidad
valor
atributo
dada
una
clase
en
donde
probabilidad
clase
calculada
simplemente
numero
de
observaciones
clase
dividido
numero
total
observaciones
pc
cantidad
instancias
c
cantidad
instancias
total
la
probabilidad
condicional
calculada
frecuencia
cada
valor
atri
buto
dividido
frecuencia
instancias
clase
pvc
cantidad
instancias
v
c
cantidad
instancias
v
support
vector
machines
dadas
probabilidades
podra
ahora
calcular
probabilidad
instancia
perteneciente
clase
manera
tomar
decisiones
utilizando
teorema
de
bayes
pab
pabpa
pb
las
probabilidades
cada
ıtem
pertenecientes
todas
clases
comparadas
y
clase
mas
alta
probabilidad
seleccionada
resultado
la
ventaja
utilizar
metodo
simplicidad
facilidad
entendimiento
ademas
posee
buena
performance
conjunto
datos
atributos
irrelevantes
ya
probabilidades
contribuyan
salidas
bajas
por
lo
que
mismas
tomadas
cuenta
realizan
predicciones
mas
aun
algoritmo
usualmente
resulta
buena
performance
terminos
del
uso
recursos
dado
solo
necesita
calcular
probabilidades
atributos
y
clases
necesidad
encontrar
ningun
coeﬁciente
otros
algoritmos
support
vector
machines
support
vector
machines
svm
algoritmo
machine
learning
utilizado
generalmente
resolver
problemas
clasiﬁcacion
el
objetivo
principal
un
svm
encontrar
hiperplano
separe
clases
mejor
manera
posible
donde
hiperplano
deﬁnido
subespacio
dimension
menos
que
el
espacio
esta
trabajando
por
ejemplo
si
datos
cuales
se
estan
trabajando
viven
espacio
bidimensional
hiperplano
simplemente
una
lınea
si
hiperplano
existe
dice
datos
linealmente
separables
figura
maximizacion
margen
capitulo
modelos
clasiﬁcacion
a
hora
elegir
hiperplano
svm
busca
maxi
mice
margen
margen
deﬁne
distancia
mınima
hiper
plano
cualquier
elemento
training
set
si
observa
figura
ﬂechas
son
las
representan
margen
los
svm
generalmente
resultan
buena
precision
incluso
conjun
to
datos
pequenos
adicionalmente
si
dataset
pequeno
modelo
ejecu
tara
rapidamente
tiempos
ejecucion
disparan
conforme
tamano
del
conjunto
datos
aumenta
decision
trees
al
igual
support
vector
machines
decision
trees
arboles
decision
son
algoritmos
machine
learning
versatiles
pueden
ejecutar
tareas
de
regresion
clasiﬁcacion
incluso
obtener
multiples
salidas
son
algoritmos
muy
poderosos
capaces
ajustarse
complejos
conjuntos
datos
un
decision
tree
construye
dividiendo
manera
recursiva
conjunto
datos
en
secuencias
subconjuntos
basado
preguntas
estilo
sientonces
ifthen
el
conjunto
entrenamiento
consiste
pares
x
y
x
ird
d
se
corresponde
numero
atributos
disponibles
correspon
diente
etiqueta
el
metodo
aprendizaje
dividira
conjunto
entrenamiento
en
grupos
basandose
x
mientras
intenta
mantener
asignaciones
cada
grupo
tan
uniformes
posible
para
lograrlo
metodo
aprendizaje
debe
elegir
un
atributo
umbral
asociado
dicho
atributo
dividira
datos
el
proceso
entrenamiento
continuara
encontrar
condicion
ﬁn
los
decision
trees
faciles
construir
permiten
creacion
complejos
proce
sos
decision
resultados
intuitivos
interpretar
pero
pueden
facil
mente
producir
overﬁtting
expandiendo
arbol
ramas
reﬂejan
outliers
en
conjunto
datos
una
manera
tratar
sobreajuste
podarel
modelo
ya
evitando
crecimiento
ramas
superﬂuas
prepruning
removiendolas
luego
arbol
crecido
postpruning
metodos
ensamble
para
poder
resolver
problema
metodos
ensamblado
entrenan
multiples
modelos
aprendizaje
a
diferencia
metodos
tıpicos
producen
unico
modelo
luego
entrenamiento
objetivo
construir
conjunto
modelos
apenas
mejores
tirar
moneda
cuyos
resultados
puedan
ser
combinados
de
acuerdo
criterio
alcanzar
performance
casi
perfecta
metodos
ensamble
de
modo
ensamble
esta
compuesto
modelos
aprendizaje
basicos
o
debiles
implementan
algoritmo
tal
arbol
decision
red
neuronal
si
modelos
utilizan
mismo
algoritmo
basico
dice
el
ensamble
homogeneo
mientras
que
si
distintos
modelos
ejecutan
algoritmos
de
distinto
tipo
ensamble
conocido
heterogeneo
existen
dos
principales
metodologıas
metodos
ensamble
bagging
dentro
metodologıa
principal
objetivo
construir
varios
estimadores
independientemente
luego
promediar
predicciones
al
uti
lizar
tecnica
logra
dratica
reduccion
error
que
este
metodo
siempre
buscara
crear
modelos
mas
independientes
posible
un
ejemplo
algoritmos
utilizan
metodos
random
forest
boosting
a
diferencia
metodo
bagging
boosting
construira
estimadores
base
manera
secuencial
objetivo
reducir
sesgo
estimador
combinado
su
motivacion
convertir
modelos
aprendizaje
debiles
mo
delos
robustos
ejemplos
utilicen
tecnica
son
adaboost
gradient
boos
ting
xgboost
otros
random
forest
random
forest
algoritmo
aprendizaje
supervisado
ﬂexible
facil
usar
capaz
producir
buenos
resultados
aun
haber
realizado
ajuste
sus
parametros
el
forestbosque
construye
ensamble
varios
decision
trees
normalmente
entrenado
utilizando
metodo
bagging
el
metodo
bagging
boostrap
aggregation
combinacion
varios
modelos
de
aprendizaje
cuyo
objetivo
consiste
reducir
alta
varianza
decision
tree
su
funcionamiento
consiste
basicamente
crear
varios
subconjuntos
datos
del
conjunto
entrenamiento
original
training
set
elegidos
manera
aleatoria
con
reemplazo
luego
cada
subconjunto
utiliza
entrenar
arboles
decision
dando
resultado
ensamble
diferentes
modelos
el
promedio
pre
dicciones
diferentes
arboles
utiliza
resulta
mas
robusto
que
el
solo
arbol
decision
capitulo
modelos
clasiﬁcacion
figura
constitucion
arboles
decicion
random
forest
como
particiona
conjunto
entrenamiento
la
principal
ventaja
clasiﬁcadores
random
forest
requieren
poco
ajuste
tuning
aun
ası
capaces
proveer
forma
alcanzar
buen
equi
librio
sesgo
varianza
utilizado
promedio
aleatoriedad
ademas
son
rapidos
faciles
entrenar
paralelo
eﬁcaces
momento
predecir
a
diferencia
sencillos
arboles
decision
random
forest
intuitivos
y
pueden
resultar
mas
difıciles
interpretar
xgboost
xgboost
cuyo
nombre
proviene
extreme
gradient
boosting
implemen
tacion
algoritmo
aprendizaje
automatico
gradient
boosting
desarrollado
para
funcionar
manera
altamente
eﬁciente
ﬂexible
portable
gradient
boosting
tecnica
machine
learning
resolver
problemas
re
gresion
clasiﬁcacion
produce
modelo
ensamble
debil
normalemente
un
arbol
decision
este
modelo
construido
misma
manera
hacen
los
httpsxgboostreadthedocsioenlatest
redes
neuronales
artiﬁciales
redes
neuronales
profundas
algoritmos
boosting
nuevos
modelos
agregados
existentes
para
corregir
errores
estos
modelos
incorporados
manera
secuencial
has
ta
detecte
mas
mejoras
realizar
la
tecnica
gradient
boosting
utiliza
descenso
gradiente
minimizar
perdida
agrega
nuevos
modelos
ahı
nombre
figura
evolucion
xgboost
decision
trees
redes
neuronales
artiﬁciales
redes
neuronales
pro
fundas
tanto
redes
neuronales
artiﬁciales
artiﬁcial
neural
network
redes
neuronales
profundas
deep
neural
network
pueden
ser
utilizadas
realizar
ta
reas
clasiﬁcacion
obtener
buenos
resultados
si
bien
pueden
resultar
mas
complejas
programar
existen
muchas
herramientas
frameworks
tensor
flow
desarrolladas
ﬁn
ayudar
programador
dicha
tarea
las
redes
neuronales
tomado
inspiracion
proceso
aprendizaje
que
ocurre
cerebro
humano
se
componen
red
funciones
llamadas
parametros
permitiran
red
aprender
cuales
vez
pueden
ajus
tar
tuning
si
mismos
mediante
analisis
datos
cada
parame
tros
tambien
conocidos
neuronas
funcioon
produce
salida
luego
haber
recibido
mas
entradas
luego
salidas
pasaran
la
siguiente
capa
neuronas
utilizara
entrada
funcion
ge
nerara
propia
salida
estas
nuevas
salidas
enviaran
siguiente
capa
ası
el
proceso
continuara
sucesivamente
haber
considerado
todas
neuronas
que
httpswwwtensorfloworg
capitulo
modelos
clasiﬁcacion
conforman
red
neuronas
terminales
recibido
entrada
las
salidas
de
neuronas
terminales
sera
resultado
ﬁnal
modelo
figura
representacion
visual
red
neuronal
simple
las
redes
neuronales
pueden
ser
efectivas
problemas
alta
dimen
sionalidad
capaces
lidiar
complejas
relaciones
variables
conjuntos
de
categorıas
exhaustivas
complejas
funciones
relacionadas
entrada
sali
da
variables
cuentan
poderosas
opciones
tuning
evitar
overﬁtting
y
underﬁtting
si
bien
redes
pueden
resultar
robustas
poderosas
tambien
pueden
resultar
complejas
difıciles
implementar
aunque
puede
resolverse
facil
mente
si
utiliza
framework
pueden
ser
intuitivas
requieren
mano
experta
ser
ajustadas
en
casos
pueden
requerir
grandes
conjuntos
de
datos
ser
efectivas
evaluacion
modelos
clasiﬁcacion
matriz
confusion
la
matriz
confusion
formas
mas
conocidas
evaluar
modelos
de
clasiﬁcacion
aunque
matriz
sı
misma
metrica
representacion
puede
ser
utilizada
deﬁnir
variedad
metricas
pueden
resultar
muy
importantes
dependiendo
escenario
la
matriz
confusion
puede
ser
utilizada
tanto
modelos
clasiﬁcacion
binaria
ası
tambien
multiclases
la
matriz
confusion
crea
partir
comparacion
etiqueta
pre
dijo
etiqueta
actual
posee
muestra
este
proceso
repite
todos
los
datos
dataset
resultados
representados
matriz
dimension
evaluacion
modelos
clasiﬁcacion
dos
en
donde
primera
columna
contiene
suma
total
verdaderos
negativos
resultados
modelo
predijo
negativos
efecto
son
falsos
ne
gativos
resultados
modelo
predijo
negativos
eran
una
segunda
columna
contiene
suma
total
falsos
positivos
resultados
que
el
modelo
predijo
positivos
eran
verdaderos
positivos
resultados
que
modelo
predijo
positivos
positivos
como
dijo
anteriormente
matriz
sı
metrica
partir
de
los
resultados
arroja
pueden
calcular
ciertas
metricas
resultan
muy
utiles
se
considera
tp
verdaderos
positivos
tn
verdaderos
negativos
fp
falsos
positivos
fn
falsos
negativos
entonces
accuracy
es
metricas
performance
mas
populares
dentro
de
los
modelos
clasiﬁcacion
deﬁne
proporcion
predicciones
correctas
realizadas
modelo
accuracy
tp
tn
tp
fp
tn
fn
precision
es
metrica
deriva
matriz
confusion
se
deﬁne
como
numero
predicciones
hechas
realmente
correctas
el
total
predicciones
positivas
precision
tp
tp
fp
recall
es
medida
modelo
identiﬁca
porcentaje
datos
rele
vantes
se
deﬁne
numero
instancias
clase
positiva
fueron
correctamente
predichas
recall
fp
tp
fn
f
score
en
casos
quiere
optimizacion
balanceada
entre
precision
sensibilidad
f
score
precision
recall
precision
recall
receiver
operating
characteristic
curve
roc
siglas
ingles
concepto
proviene
originalmente
uso
de
radares
este
concepto
puede
extender
modelos
clasiﬁcacion
binarios
y
multiclases
y
puede
ser
interpretada
efectividad
modelo
puede
distinguir
senal
verdadera
ruido
datos
capitulo
modelos
clasiﬁcacion
la
curva
roc
crea
dibujando
fraccion
verdaderos
positivos
versus
frac
cion
falsos
positivos
es
mayormente
aplicable
clasiﬁcadores
scoring
clasi
ﬁcadores
scoring
tipo
clasiﬁcador
retorna
valor
probabili
dad
score
cada
clase
label
la
curva
roc
funciona
siguiente
manera
se
ordenan
salidas
clasiﬁcador
ordenados
score
o
probabilidad
de
pertenecer
clase
positiva
se
comienza
coordenada
por
cada
valor
x
lista
ordenada
pregunta
a
si
x
positiva
mueve
pos
hacia
arriba
b
si
x
negativa
mueve
neg
hacia
derecha
donde
pos
neg
fracciones
positivos
negativos
respectivamente
luego
traza
lınea
diagonal
indicara
si
curva
roc
se
encuentra
encima
quiere
decir
modelo
clasiﬁcacion
mejor
que
el
promedio
resumen
en
presente
capıtulo
explico
mayor
detalle
metodos
exis
tentes
machine
learning
clasiﬁcacion
se
comenzo
explicando
logistic
re
gression
metodo
simple
puede
ser
utilizado
regresion
para
clasiﬁcacion
binaria
luego
estudiaron
algoritmos
kneares
neighbors
naive
bayes
support
vector
machines
metodos
resultan
simples
implemen
tar
comprender
a
continuacion
abordo
algoritmo
decision
tree
que
al
igual
support
vector
machines
algoritmos
versatiles
pueden
ejecu
tar
tareas
regresion
clasiﬁcacion
tambien
explicaron
algunos
metodos
ensamble
random
forest
implementacion
algoritmo
de
gradient
boosting
xgboost
por
ultimo
describieron
redes
neuronales
artiﬁ
ciales
redes
neuronales
profundas
cuales
tambien
pueden
ser
utilizadas
para
taras
clasiﬁcacion
como
paso
ﬁnal
mencionaron
describieron
cuales
tecnicas
mas
uti
lizadas
evaluar
modelos
clasiﬁcacion
cuales
destacan
matriz
de
confusion
curva
roc
parte
ii
implementacion
capitulo
data
mining
generacion
del
dataset
este
capıtulo
enfoca
estudio
decisiones
diseno
implementacion
de
tecnicas
utilizadas
analisis
conjunto
datos
disponible
de
mo
do
comenzara
describiendo
estructura
formato
dichos
datos
crudos
ori
ginales
luego
pasar
enumerar
aquellos
atributos
considerados
mas
relevantes
una
vez
identiﬁcados
dichos
features
mencionaran
describiran
los
metodos
tecnicas
utilizadas
extraerlos
esta
informacion
obtenida
sera
la
que
luego
conformara
dataset
utilizara
algoritmos
clasiﬁcacion
conjunto
inicial
datos
el
conjunto
inicial
datos
realizo
experimento
obtenido
del
sitio
kaggle
el
mismo
compone
conjunto
archivos
tipo
asm
su
correspondiente
archivo
tipo
bytes
equivalentes
aproximadamente
once
mil
malwares
cada
archivos
pertenece
nueve
clases
familias
de
virus
distinta
archivos
asm
los
archivos
asm
programas
escritos
lenguaje
ensamblador
almacenados
bajo
extension
asm
guarda
relacion
cercana
instrucciones
en
codigo
maquina
httpswwwkagglecomcmalwareclassification
capitulo
data
mining
generacion
dataset
textaf
b
ec
mov
ebp
esp
textb
ec
sub
esp
h
textb
d
c
lea
edx
unk_c
textba
fa
bf
d
cmp
edx
dbfh
textc
ed
jb
short
loc_af
textc
push
edx
textc
ff
b
e
push
dword
ptr
edxeh
textc
push
edi
textca
ff
push
dword
ptr
edxh
textcd
e
call
sub_
textd
c
c
add
esp
ch
textd
a
pop
edx
textd
push
edx
textd
ba
mov
edx
textdc
push
edx
textdd
e
call
sub_e
texte
ff
push
dsgetpriorityclass
texte
c
retn
texte
sub_ac
endp
spanalysis
failed
figura
extracto
archivo
aguvpoccafmyvdyfgbasm
nota
los
encabezados
pe
portable
executable
eliminados
cuestiones
de
seguridad
estos
encabezados
poseen
estructura
particular
formados
una
serie
secciones
indican
dynamic
linker
como
debera
mapear
archivo
en
memoria
archivos
bytes
los
archivos
bytes
archivos
contienen
codigo
maquina
ejecuta
ble
representacion
hexadecimal
estos
archivos
almacenan
bajo
extension
bytes
conjunto
inicial
datos
ff
f
ff
f
ff
d
ff
e
b
ec
ec
a
f
fc
bf
e
e
bb
b
c
bb
ff
ff
d
c
f
d
a
eb
d
f
ff
c
b
fc
f
ff
f
ff
f
ff
f
d
f
ff
b
f
f
f
b
f
be
f
e
bb
eb
b
f
b
c
c
e
b
f
f
a
d
e
f
b
c
c
b
ff
c
b
f
ff
c
f
a
c
e
a
e
d
c
e
c
e
d
da
e
e
bc
c
f
e
figura
extracto
archivo
anoozdnbpxirmrbscjbytes
las
familias
malware
como
menciono
previamente
cada
archivos
malware
disponibles
pertenecen
nueve
familias
o
clases
malware
esta
informacion
en
cuentra
disponible
archivo
incluye
cada
muestra
clase
que
pertenece
entre
nueve
familias
encuentran
ramnit
perteneciente
familia
tipo
troyanos
infecta
archivos
ejecutables
tales
exe
html
puede
esparcirse
facilmente
traves
dispositivos
removibles
ejemplo
memorias
usb
lollipop
tipo
adware
pup
potentially
unwanted
program
el
cual
caracteriza
instalar
toolsbar
mostrar
publicidades
medio
de
ventanas
emergentes
kelihos
ver
tipo
botnet
normalmente
encuentra
relacionado
con
robo
bitcoins
envıo
spam
vundo
tipo
troyano
conocido
utilizar
popups
publicidad
progra
mas
antivirus
maliciosos
simda
perteneciente
familia
backdoors
encargado
robar
informa
cion
usuarios
tales
nombres
usuario
contrasenas
certiﬁcados
capitulo
data
mining
generacion
dataset
tracur
familia
troyanos
capaz
redireccionar
busquedas
web
a
sitios
publicidad
fraudulenta
ademas
puede
descargar
ejecutar
archivos
como
ejemplo
malwares
kelihos
ver
botnet
relacionado
envıo
spam
masivo
obfuscatoracy
utiliza
tecnicas
ocultar
decarga
archivos
robo
de
informacion
gatak
perteneciente
familia
troyanos
encargara
reco
pilar
informacion
pc
esta
ejecutando
luego
enviarsela
a
un
hacker
analisis
seleccion
caracterısticas
mas
relevantes
con
archivos
descargados
siguiente
paso
consiste
teorizar
que
carac
terısticas
puede
llegar
ser
utiles
identiﬁcar
programa
similares
para
es
necesario
estudiar
estructura
cada
archivo
tamano
ﬁsonomıa
como
su
funcionamiento
instrucciones
ejecuta
uso
librerıas
a
continuacion
presentan
aquellas
caracterısticas
consideradas
mas
relevantes
librerıas
dll
las
dlls
dynamiclink
library
librerıas
enlace
dinamico
que
cargan
bajo
demanda
ejecucion
programa
los
archi
vos
correspondientes
almacenan
bajo
extension
dll
las
referencias
a
estas
librerıas
pueden
resultar
interes
bajo
supuesto
mismo
tipo
de
malware
hace
uso
mismo
tipo
dlls
codigos
operacion
un
codigo
operacion
identiﬁca
tipo
instruccion
a
ejecutar
de
forma
similar
caso
anterior
presupone
miembros
de
una
misma
familia
malware
ejecutan
mismo
tipo
operaciones
secciones
el
codigo
asm
encuentra
dividido
distintas
secciones
como
header
text
rdata
de
modo
secciones
describen
terminos
generales
estructura
archivo
es
posible
miembros
misma
familia
compartan
misma
estructura
deben
ser
estudiadas
codigos
operacion
n
gramas
un
n
grama
secuencia
contigua
formada
n
items
alguna
muestra
texto
audio
numerica
cual
quier
fuente
este
investigando
tomando
codigos
operacion
anteriormente
mencionados
desea
determinar
si
miembros
misma
fa
milia
ejecutan
misma
secuencia
instrucciones
ngramas
codigos
de
operacion
extraccion
datos
tamanos
archivos
el
tamano
archivo
podrıa
ser
dato
representativo
de
clase
pertenece
dicho
malware
archivos
misma
clase
podrıan
tener
tamanos
similares
tamanos
archivos
comprimidos
al
igual
tamano
archivo
el
tamano
archivo
vez
comprimido
tambien
ratio
compresion
podrıa
resultar
dato
interes
snapshots
primeros
bytes
se
propone
estudiar
primer
kilo
byte
cada
archivo
proposito
determinar
si
malware
pertenecientes
a
misma
familia
comparten
cierta
ﬁsonomıa
extraccion
datos
una
vez
identiﬁcado
aquellos
aspectos
archivos
desean
ser
estudiados
siguiente
tarea
consiste
elaboracion
conjunto
procesos
que
permita
extraccion
datos
a
continuacion
describen
distintos
procesos
conforman
pipeline
de
tareas
minerıa
datos
ejecutadas
construccion
dataset
unico
partir
miles
muestras
malware
dlls
secciones
codigos
operacion
partiendo
supuesto
archivos
misma
familia
similares
en
cuanto
estructura
comportamiento
estudian
codigos
operacion
seccion
y
uso
dlls
ﬁn
determinar
veracidad
supuesto
de
modo
extraccion
atributos
diseno
implemento
pi
peline
procesos
objetivo
ﬁnal
determinar
cuales
features
mas
relevantes
cada
familia
ası
tambien
contabilizar
ocurrencias
de
estos
atributos
relevantes
cada
muestras
disponibles
este
pipeline
puede
descomponerse
siguientes
pasos
calcular
cantidad
ocurrencias
cada
atributo
cada
las
muestras
determinar
atributos
mas
relevantes
cada
familias
crear
lista
unica
atributos
relevantes
total
familias
construir
tablacon
cantidad
ocurrencias
atributos
mas
rele
vantes
todas
muestras
conjunto
datos
capitulo
data
mining
generacion
dataset
procesamiento
archivos
asm
el
primer
paso
proceso
minerıa
features
consiste
tomar
los
archivos
asm
recorrerlos
lınea
lınea
busqueda
posible
ocurrencia
de
atributos
interes
cuales
totalizados
de
manera
construye
una
estructura
contiene
cada
archivo
asm
disponible
cantidad
veces
que
cada
feature
encontrado
la
ﬁgura
ilustra
primer
paso
proceso
figura
procesamiento
archivos
asm
el
resultado
proceso
diccionario
o
mapa
que
cada
muestra
totaliza
cantidad
cada
ocurrencias
encontradas
mismo
como
se
ilustra
continuacion
archivo
seccion
seccion
seccion
archivo
seccion
seccion
seccion
seccion
esta
estructura
guardada
disco
utilizando
messagepack
provee
un
formato
eﬁciente
serializacion
mas
liviano
veloz
json
almacenar
infor
macion
manera
evita
tener
convertirla
representacion
tabular
la
cual
puede
generar
archivo
redundancia
campos
mucha
dispersion
identiﬁcacion
dlls
la
logica
deteccion
dlls
debe
ser
capaz
detectar
cadena
dll
sin
ser
sensible
mayusculas
minusculas
a
vez
debe
evitar
procesar
lıneas
correspondientes
comentarios
tal
ilustra
siguiente
imagen
httpsmsgpackorg
extraccion
datos
figura
extraccion
dlls
archivos
asm
identiﬁcacion
codigos
seccion
cada
lınea
comienza
preﬁjo
identiﬁca
tipo
seccion
la
lınea
cuestion
pertenece
este
codigo
siempre
seguido
dos
puntos
de
este
modo
extraer
codigo
seccion
tan
simple
obtener
cadena
la
izquierda
primer
dos
puntos
figura
extraccion
codigos
seccion
archivos
asm
identiﬁcacion
codigos
operacion
se
asume
codigo
operacion
primer
palabra
lınea
y
en
cuanto
trate
comentario
figura
extraccion
codigos
operacion
archivos
asm
totalizacion
ocurrencias
calculo
proporciones
una
vez
procesados
archivos
asm
obtenido
total
occurrencias
cada
feature
archivo
debe
determinar
cuales
features
carac
terizan
realmente
cada
familias
una
solucion
simple
problema
podrıa
ser
totalizar
ocurrencias
cada
atributo
por
familia
luego
quedar
se
aquellos
produjeron
cuentas
mas
altas
sin
embargo
enfoque
puede
llevar
problemas
inconsistencias
presencia
valores
anomalos
para
graﬁcar
situacion
propone
siguiente
tabla
presenta
cantidad
de
lıneas
cada
seccion
encontrada
cada
siete
archivos
ejemplo
capitulo
data
mining
generacion
dataset
seccion
seccion
seccion
seccion
seccion
seccion
archivo
archivo
archivo
archivo
archivo
archivo
archivo
total
cuadro
ejemplo
totalizacion
secciones
usando
sumas
de
ocurrencias
con
rapida
inspeccion
tabla
anterior
podemos
observar
siete
ar
chivos
contienen
numero
elevado
lıneas
pertenecientes
seccion
seccion
y
seccion
tambien
observan
valores
atıpicos
seccion
archivo
sec
cion
archivo
suponiendo
objetivo
obtener
cuatro
secciones
mas
relevantes
basandonos
cantidad
ocurrencias
estas
serıan
seccion
seccion
seccion
seccion
esto
evidencia
dos
tipos
problemas
sensibilidad
valores
atıpicos
seccion
seccion
aparecen
tan
solo
un
archivo
cada
uno
cantidad
extremadamente
alta
estos
valores
inusuales
hacen
identiﬁcados
features
mas
importantes
omision
features
relevantes
corolario
sensibilidad
valores
atıpi
cos
mencionada
punto
anterior
atributos
deberıan
ser
capturados
como
seccion
desplazados
identiﬁcados
importantes
pa
ra
correspondiente
familia
para
solucionar
dos
problemas
necesario
comprender
hace
mas
relevante
feature
cantidad
ocurrencias
sino
que
proporcion
estas
cantidades
representan
total
ocurrencias
cada
archivo
ası
se
puede
expresar
tabla
utilizando
proporciones
siguiente
manera
extraccion
datos
seccion
seccion
seccion
seccion
seccion
seccion
archivo
archivo
archivo
archivo
archivo
archivo
archivo
total
cuadro
ejemplo
totalizacion
secciones
usando
proporcio
nes
como
puede
observarse
uso
proporciones
produce
resultados
capturan
de
mejor
manera
relevancia
cada
atributo
habiendo
establecido
estrategia
mediante
determinaran
atribu
tos
mas
importantes
siguiente
paso
pipeline
consiste
consumir
archivo
generado
paso
anterior
features
totalizados
archivo
convertir
estos
en
proporciones
acumular
valores
familia
figura
calculo
totalizacion
ocurrencias
de
modo
proceso
genera
dos
archivos
salida
uno
analogo
resultado
del
proceso
anterior
cantidades
ocurrencias
expresadas
propor
ciones
similar
contenido
cuadro
este
archivo
sera
gran
utilidad
para
el
ultimo
paso
proceso
extraccion
atributos
el
segundo
archivo
contie
ne
totalizacion
proporciones
anteriormente
mencionadas
cada
una
de
nueve
familias
malware
disponen
este
archivo
sera
utilizado
el
siguiente
paso
proceso
determinacion
features
mas
relevantes
una
vez
ejecutado
paso
anterior
dispone
proporciones
de
ocurrencias
cada
feature
archivo
de
modo
posible
totalizar
va
lores
familia
permite
determinar
cuales
atributos
mas
relevantes
capitulo
data
mining
generacion
dataset
para
cada
clases
malware
disponible
la
siguiente
ﬁgura
presenta
diagrama
proceso
obtencion
atributos
mas
relevantes
cada
familias
malware
disponibles
figura
determinacion
features
mas
importante
cada
familia
dada
gran
cantidad
atributos
disponible
necesario
poder
reducir
este
numero
sacriﬁcar
demasiada
performance
modelos
machine
learning
para
ello
debe
escoger
punto
corte
adecuado
esta
tarea
puede
realizarse
con
facilidad
si
vuelcan
cantidades
cada
atributo
graﬁco
barras
adi
cionalmente
cabe
destacar
elegira
mismo
numero
features
con
intencion
producir
conjunto
datos
balanceado
las
imagenes
permiten
visualizar
atributos
mas
relevantes
para
cada
clase
comenzando
utilizacion
librerıas
dll
podemos
distinguir
rapida
mente
cada
clase
podemos
observar
que
independiente
orden
generalmente
las
primeras
cinco
librerıas
son
kerneldll
modulo
kernel
windows
que
arranca
sistema
es
cargada
area
protegida
memoria
pueda
ser
modiﬁca
da
ningun
usuario
proceso
userdll
librerıa
contiene
funciones
api
windows
tienen
que
ver
interfaz
usuario
advapidll
provee
api
funciones
relacionadas
llamadas
re
gistry
sistema
funciones
seguridad
msvcrtdll
modulo
parte
librerıa
microsoft
c
runtime
contiene
fun
ciones
estandar
printf
memcpy
cos
gdidll
librerıa
funciones
windows
gdi
graphical
device
inter
face
asiste
sistema
ventanas
creacion
objetos
bidimensio
nales
extraccion
datos
figura
top
dlls
mas
relevantes
cada
clase
malware
similar
resultados
observados
uso
librerıas
dll
codigos
de
operaciones
podemos
identiﬁcar
cinco
valores
mas
recurrentes
los
primeros
puestos
ellos
son
capitulo
data
mining
generacion
dataset
mov
operacion
que
general
toma
dos
operandos
copia
valor
uno
al
otro
push
funcion
recibe
lista
registros
apila
stack
orden
descendiente
call
permite
invocacion
subrutina
etiquetada
parametro
dd
operacion
usa
declarar
dato
cuatro
bytes
db
operacion
utilizada
declaracion
dato
byte
extraccion
datos
figura
top
codigos
operacion
mas
relevantes
cada
clase
malware
capitulo
data
mining
generacion
dataset
figura
top
codigos
seccion
mas
relevantes
cada
clase
malware
estudiando
mas
cerca
codigos
seccion
obtenidos
distingue
que
la
mayorıa
casos
tres
codigos
mas
frecuentes
son
extraccion
datos
data
seccion
dedicada
inicializacion
datos
variables
rdata
similar
data
distincion
ser
solo
lectura
text
seccion
contiene
instrucciones
consolidacion
resultados
habiendo
determinado
atributos
mas
importantes
cada
fa
milias
unico
paso
restante
consiste
construir
lista
unica
atributos
sin
repeticiones
esta
lista
utilizada
conjunto
archivo
generado
segun
do
paso
correspondiente
proporciones
cada
feature
cada
archivo
como
se
describe
ﬁgura
elaboracion
dataset
contiene
cada
archivo
proporcion
ocurrencias
cada
atributos
relevantes
para
cualquiera
familia
la
ﬁgura
representa
ultimo
paso
proceso
de
data
mining
figura
obtencion
proporciones
ocurrencias
relevantes
por
archivo
resumen
la
obtencion
features
mas
importantes
dlls
codigos
operacion
y
de
seccion
trabajo
extenso
laborioso
implico
combinacion
dis
tintas
tareas
requirio
primero
estudio
estructura
archivos
asm
para
poder
determinar
como
localizar
extraer
datos
deseados
luego
procedio
la
construccion
ejecucion
distintos
algoritmos
minerıa
concreta
da
tos
sortearse
inconvenientes
relacionados
principalmente
con
problemas
encoding
archivos
por
lado
lugar
elaboracion
de
herramientas
validar
informacion
generada
traves
graﬁcos
tablas
y
dar
soporte
toma
decisiones
finalmente
escoger
estra
tegia
identiﬁcar
seleccionar
atributos
mas
relevantes
cada
las
familias
malware
una
vision
global
todas
tareas
involucradas
proceso
ofrece
la
ﬁgura
capitulo
data
mining
generacion
dataset
figura
obtencion
proporciones
ocurrencias
relevantes
por
archivo
dlls
codigos
operacion
seccion
snapshots
archivos
asm
el
trabajo
realizado
subseccion
anterior
implica
analisis
minucioso
de
cada
lına
archivos
asm
extraer
informacion
interes
en
punto
sin
embargo
enfoque
centra
estudio
ﬁsonomıa
cada
los
archivos
de
forma
similar
foto
identiﬁcacion
documento
pasaporte
se
propone
sacar
foto
escala
grises
primeros
bytes
cada
archivo
asm
utilizar
imagenes
x
bytes
entrenar
red
neuronal
sea
capaz
de
cada
muestra
analizada
asignar
probabilidad
misma
pertenezca
cada
familias
extraccion
datos
captura
snapshots
el
primer
paso
minerıa
datos
punto
involucro
generacion
de
snapshots
partir
primeros
bytes
cada
archivo
asm
de
modo
se
utilizo
librerıa
python
imageio
creacion
imagenes
x
bytes
en
escalas
grises
la
ﬁgura
presenta
cinco
muestras
cada
familia
una
por
ﬁla
los
snapshots
presentan
escala
tamano
original
para
su
mejor
visualizacion
figura
snapshots
x
bytes
cada
familia
malware
httpspypiorgprojectimageio
capitulo
data
mining
generacion
dataset
entrenamiento
red
neuronal
una
vez
generada
totalidad
imagenes
procedio
construccion
de
una
red
neuronal
utilizando
tensorflow
el
objetivo
proceso
obte
ner
estimacion
mas
precisa
posible
sino
poder
observar
grandes
rasgos
la
similitud
archivos
pertenecientes
misma
clase
teniendo
ser
capaz
de
poder
realizar
estimaciones
cada
muestras
red
construida
utilizando
kfold
la
tecnica
kfold
permite
dividir
conjunto
datos
k
partes
iguales
conocidas
folds
splits
ejecutar
k
corridas
en
cada
corri
da
toma
split
distinto
conjunto
test
restantes
conjunto
de
datos
entrenar
modelo
para
construccion
red
utilizo
k
figura
kfold
utilizado
clasiﬁcar
snapshots
una
vez
ejecutada
red
obtuvo
precision
alrededor
produ
ciendo
resultados
siguiente
formato
figura
muestra
resultados
red
neuronal
cla
siﬁcar
snapshots
tamanos
archivos
compression
rate
continuando
estudio
muestras
punto
vista
ﬁso
nomıa
procedio
determinar
tamano
archivo
asm
tambien
archivo
bytes
cada
malware
conforma
conjunto
datos
adicionalmente
cal
cularon
tamanos
ambos
archivos
vez
comprimidos
mediante
gzip
gzip
gnu
zip
tecnica
compresion
estandar
extraccion
datos
partiendo
supuesto
archivos
misma
familia
deben
tener
tamano
similar
minerıa
atributos
realiza
objetivo
determinar
si
en
efecto
distintos
tamanos
compression
rate
pueden
ser
utiles
identiﬁcacion
de
cada
malware
dentro
correspondiente
familia
ası
construyo
script
que
vez
ejecutado
produjo
archivo
for
mato
ilustra
siguiente
imagen
figura
extraccion
tamanos
archivos
tamano
ar
chivos
comprimidos
ngramas
el
ultimo
conjunto
atributos
extraıdo
archivos
malware
trata
los
gramas
mas
importantes
formados
ocurrencias
codigos
de
operacion
mas
relevantes
esta
fue
quizas
tarea
mas
extensa
todas
obtener
ngramas
partir
secuencia
valores
signiﬁca
recorrer
misma
con
ventana
tamano
n
desplazandose
posicion
vez
registrando
el
valor
observado
el
objetivo
construir
ngramas
analizar
operaciones
ejecutadas
pero
en
lugar
enfocarnos
individualmente
centramos
estudiar
secuen
cias
instrucciones
ejecutan
juntas
se
pretende
determinar
si
miembros
de
una
misma
familia
malware
tienden
ejecutar
misma
serie
operaciones
es
importante
tener
cuenta
trabaja
unicamente
codigos
operaciones
mas
relevantes
minimizar
numero
combinaciones
posibles
de
modo
trabajo
ser
realizado
puede
dividir
dos
tareas
por
un
lado
construccion
ngramas
basados
codigos
operacion
mas
importantes
por
lado
debe
realizar
identiﬁcacion
obtencion
los
ngramas
mas
relevantes
cada
familia
la
primer
tarea
obtencion
ngramas
hace
base
codigos
de
operacion
mas
relevantes
mencionado
subseccion
ası
los
archivos
asm
deben
ser
recorridos
nuevamente
armar
ngramas
el
diagrama
graﬁca
proceso
capitulo
data
mining
generacion
dataset
figura
proceso
extraccion
ngramas
una
vez
extraıdos
ngramas
procede
ﬁltrado
aquellos
mas
relevantes
de
forma
similar
trabajo
realizado
features
dlls
codigos
seccion
y
operacion
a
continuacion
presentan
imagenes
diez
gramas
mas
relevantes
cabe
destacar
decision
utilizar
solamente
codigo
operacion
mas
relevantes
tomo
intencion
reducir
numeros
combinaciones
posi
bles
aun
ası
obtuvieron
gran
numero
ngramas
distintos
por
ejemplo
la
familia
posee
gramas
distintos
extraccion
datos
figura
top
gramas
mas
relevantes
cada
clase
de
malware
capitulo
data
mining
generacion
dataset
figura
top
gramas
mas
relevantes
cada
clase
de
malware
extraccion
datos
figura
top
gramas
mas
relevantes
cada
clase
de
malware
capitulo
data
mining
generacion
dataset
detalles
tecnicos
dada
cantidad
atributos
extraer
diversidad
mismos
aspecto
fundamental
hora
implementar
algoritmos
data
mining
tratar
de
logar
mejor
abstraccion
posible
minimizar
repeticion
codigo
por
ende
la
construccion
procesos
mencionados
seccion
anterior
punto
central
implementacion
dos
clases
con
correspondientes
subclases
categoryfeatureprocessor
clase
responsable
de
partir
nombre
de
atributo
dll
numero
categorıa
realizar
extrac
cion
feature
correspondiente
archivos
familia
gracias
al
patron
diseno
template
method
subclases
concretas
proveen
imple
mentacion
necesaria
obtener
informacion
segun
atributo
este
analizando
lineparser
mismo
tipo
diseno
mencionado
punto
anterior
aplico
para
abstraer
proceso
analisis
lınea
individual
archivo
en
busqueda
informacion
interes
codigo
operacion
seccion
o
la
referencia
librerıa
dll
la
imagen
ofrece
diagrama
clases
puede
observar
mejor
detalle
clases
relacion
figura
extraccion
features
archivos
asm
conclusion
cabe
recordar
estudio
referencia
librerıas
codigos
seccion
y
de
operacion
construccion
ngramas
clasiﬁcacion
snapshots
requirieron
procesos
adicionales
estos
algoritmos
abordados
discutieron
cada
features
conclusion
el
proceso
data
mining
tarea
compleja
demandante
requirio
una
considerable
inversion
tiempo
comprender
naturaleza
estructura
de
los
archivos
malware
original
poder
empezar
trabajar
luego
ne
cesaria
elaboracion
diversos
tipos
algoritmos
capaz
extraer
la
informacion
correspondiente
numero
diverso
atributos
ası
como
tambien
idear
estrategias
calculo
almacenado
resultados
intermedio
adi
cionalmente
debieron
crear
herramientas
apoyo
permitieran
validar
que
los
datos
extraıdos
correctos
ası
tambien
graﬁcar
informacion
obte
nida
poder
determinar
relevancias
puntos
corte
el
resultado
proceso
archivo
csv
atributos
servira
como
dataset
algoritmos
machine
learning
descriptos
proximo
capıtulo
capitulo
analisis
exploratorio
y
preprocesamiento
datos
el
proceso
data
mining
ﬁnalizo
construccion
dataset
producto
de
la
extraccion
distintos
atributos
identiﬁcados
interes
para
el
analisis
clasiﬁcacion
malware
sin
embargo
archivo
csv
resultante
esta
listo
aun
ser
utilizado
entrenar
modelos
machine
learning
sino
que
es
necesario
llevar
cabo
numero
tareas
adicionales
mejorar
calidad
del
mismo
a
traves
analisis
exploratorio
datos
eda
siglas
ingles
se
obtendra
mejor
entendimiento
datos
recolectados
mediante
metricas
y
graﬁcas
a
vez
analisis
permitira
tratamiento
datos
nulos
datos
faltantes
determinar
importancia
variables
proceder
seleccion
y
extraccion
estandarizado
escalado
atributos
se
hizo
uso
extensivo
de
distintas
librerıa
python
pandas
matplotlib
scikitlearn
cabe
destacar
capıtulo
ofrecera
conceptos
teoricos
adicionales
no
cubiertos
marco
teorico
ser
demasiados
especıﬁcos
trabajo
realizado
en
seccion
estructura
contenido
dataset
antes
comenzar
cualquier
tipo
analisis
datos
conforman
dataset
es
necesario
estudiar
propia
estructura
archivo
esto
permite
adquirir
cierta
dimension
volumen
datos
trabajara
utilizando
pandas
cargo
el
archivo
csv
dataframe
y
ası
observo
dataset
resultante
posee
ﬁlas
correspondientes
cada
muestras
disponibles
columnas
con
cada
atributos
extraıdos
enfocandonos
columnas
observan
httpspandaspydataorg
httpsmatplotliborg
httpsscikitlearnorgstableindexhtml
capitulo
analisis
exploratorio
preprocesamiento
datos
codigos
seccion
header
text
idata
rdata
data
rsrc
bss
gnu
deb
tls
code
data
bss
gap
data
xref
crt
seg
seg
code
zenc
seg
yogmamm
iuagwws
qmoyiu
acggagg
tixt
agauixa
code
rdata
seg
seg
icode
unisec
hwa
oinf
dwr
oj
rata
tls
pav
upx
xdata
hc
c
bas
librerıas
dll
kernel
user
advapi
msvcrt
ole
oleaut
gdi
shlwapi
version
urlmon
shell
mlang
wininet
mscoree
secur
comdlg
libgcj
s
setupapi
ntdll
uxtheme
crypt
ws
apphelp
tapi
msvcp
dsound
mscms
msasn
dpnet
ntmarta
opengl
ntshrui
usp
clbcatq
rsaenh
forkernel
foruser
loadperf
msvbvm
codigos
operacion
proc
dword
push
mov
sub
lea
call
pop
add
align
test
jz
xor
cmp
endp
db
jmp
retn
dd
imul
mul
gramas
dword
dword
push
push
push
mov
mov
push
push
lea
mov
call
call
mov
mov
mov
lea
push
push
call
call
add
mov
cmp
cmp
mov
call
pop
pop
pop
proc
mov
mov
add
pop
retn
align
proc
dd
dd
dd
db
jmp
db
add
mov
retn
endp
sub
mov
db
dd
db
db
add
pop
mov
imul
mov
sub
jmp
dd
pop
push
jmp
jmp
db
jmp
gramas
dword
dword
dword
mov
push
push
push
lea
push
push
push
call
push
call
add
push
push
push
mov
mov
push
push
call
mov
push
push
mov
push
mov
push
mov
push
mov
push
mov
mov
mov
mov
mov
mov
mov
add
push
call
pop
mov
push
call
dd
dd
dd
dd
dd
db
mov
add
mov
push
call
push
call
pop
retn
sub
mov
mov
mov
mov
call
lea
push
push
db
dd
db
call
add
pop
proc
mov
push
align
proc
mov
db
dd
dd
push
mov
sub
mov
sub
mov
add
mov
mov
db
db
db
mov
mov
sub
call
pop
push
mov
sub
lea
jmp
jmp
jmp
db
jmp
db
dd
db
jmp
jmp
db
dd
add
pop
push
jmp
dd
db
gramas
dword
dword
dword
dword
lea
push
lea
push
push
lea
push
call
push
push
push
push
push
mov
push
mov
mov
push
mov
mov
mov
mov
mov
add
mov
push
mov
push
pop
retn
align
proc
push
mov
push
push
push
mov
call
mov
dd
dd
dd
dd
dd
dd
dd
db
mov
mov
mov
mov
proc
dword
dword
dword
push
push
push
mov
push
push
push
call
push
push
call
mov
call
mov
mov
mov
lea
push
push
call
push
lea
push
push
push
call
add
pop
proc
mov
push
mov
align
proc
mov
push
mov
push
push
push
mov
push
mov
sub
mov
add
mov
mov
push
push
call
add
push
push
call
pop
proc
mov
push
push
db
db
db
db
mov
mov
add
mov
mov
mov
sub
mov
mov
sub
mov
mov
add
mov
mov
mov
mov
mov
mov
sub
push
call
pop
push
call
pop
push
call
jmp
jmp
jmp
jmp
push
mov
sub
lea
dd
db
jmp
db
db
jmp
db
dd
jmp
db
dd
dd
call
add
pop
push
add
pop
push
push
jmp
db
dd
db
dd
dd
db
jmp
probabilidades
columnas
corresponden
clasiﬁcacion
snaps
hots
primeros
bytes
cada
archivo
asm
valores
nulos
datos
faltantes
tamanos
archivo
columnas
contienen
tamano
cada
archivo
asm
y
bytes
versiones
comprimidas
considerando
curvas
presentadas
secciones
cantidad
de
atributos
cada
feature
parece
ser
razonable
dos
casos
quizas
resulten
de
interes
bajo
numero
codigos
operacion
numero
relativamente
alto
de
codigos
seccion
si
bien
veintitres
codigos
operacion
pueden
parecer
numero
relativamente
bajo
si
compara
features
realidad
cantidad
que
resulta
razonable
dado
cantidad
codigos
operacion
x
varıa
entre
ochenta
cien
segun
version
en
cuanto
codigos
seccion
interesante
analizar
graﬁcos
barras
en
ﬁgura
en
puede
observarse
que
mayorıa
clases
los
valores
relativamente
pequenos
luego
primeros
cuatro
cinco
codigos
ademas
mencionado
anteriormente
identiﬁcan
tres
cuatro
codigos
encabezando
graﬁcas
todas
familias
en
contexto
obtener
total
de
treinta
nueve
codigos
distintos
habla
como
cada
familia
hace
uso
secciones
distintas
resto
valores
nulos
datos
faltantes
la
presencia
datos
faltantes
nulos
dataset
suele
tener
impacto
nega
tivo
performance
modelos
entrenen
base
el
por
motivo
la
primera
tarea
suele
llevarse
cabo
preprocesamiento
datos
de
busqueda
identiﬁcacion
valores
nulos
faltantes
correspondiente
correc
cion
como
describio
seccion
existen
diversas
tecnicas
tratamien
to
datos
nulos
para
seleccionar
mas
adecuada
imperativo
comprender
el
origen
valores
faltantes
analisis
valores
nulos
gracias
distintas
funciones
pandas
pudo
determinarse
naturaleza
de
los
valores
nulos
debıan
dos
razones
archivos
que
problemas
encoding
integridad
pudieron
ser
analiza
dos
ende
ﬁlas
practicamente
vacıas
salvo
columnas
correspondientes
tamanos
archivo
en
total
identiﬁcaron
poco
mas
muestras
capitulo
analisis
exploratorio
preprocesamiento
datos
archivos
registraban
ocurrencias
atributos
resultaron
relevan
tes
familias
el
problema
debe
casos
modo
obtuvieron
los
atributos
mas
relevantes
relacionados
codigos
operacion
seccion
uso
de
librerıas
dll
ngramas
para
ilustrar
situacion
puede
imaginar
que
traba
jando
dlls
identiﬁcaron
librerıas
dll
dll
mas
relevantes
para
familia
ramnit
librerıas
dll
dll
mas
importantes
para
la
familia
lollipop
si
solo
analizando
dos
familias
conjunto
de
atributos
relevantes
estarıa
conformado
dll
dll
dll
adicionalmente
podrıa
agregarse
escenario
ninguna
muestra
familia
ramnit
hace
uso
dll
al
ejecutar
consolidacion
resultados
descripta
subsec
cion
obtendrıa
dataset
columna
correspondiente
dll
estarıa
vacıa
muestras
correspondientes
familia
ramnit
antes
proceder
resolucion
valores
nulos
vale
pena
mencionar
que
el
estudio
valores
faltantes
brindo
mayor
entendimiento
respecto
a
la
relevancia
real
cada
atributos
por
ejemplo
librerıas
kernel
gdi
ole
utilizadas
mayorıa
archivos
unicamente
lo
hacen
secciones
header
rdata
text
aparecen
archivos
de
forma
analoga
atributos
mayor
cantidad
valores
faltantes
corresponden
en
casos
codigos
seccion
tales
oj
oinf
unisec
valores
nulos
esto
habla
como
secciones
si
bien
pueden
haber
resultado
relevantes
familias
realmente
utilizadas
resto
resolucion
valores
nulos
como
describio
subseccion
anterior
datos
faltantes
dos
orıge
nes
distintos
para
resolucion
primer
problema
archivos
corruptos
hubo
otra
alternativa
mas
eliminar
ﬁlas
correspondientes
mientras
el
segundo
problema
columnas
mediciones
completaron
celdas
cero
dado
que
contexto
valor
nulo
representa
cero
ocurrencias
distribucion
valores
habiendo
aplicado
estrategia
solucionar
problema
campos
nu
los
prosigue
estudiar
distribucion
valores
cada
feature
nuevamen
te
podemos
utilizar
funcion
pandas
datos
obtener
mınimos
y
maximos
cada
columna
ası
valor
medio
desviacion
estandar
la
medicion
distintos
cuartiles
distribucion
valores
para
ejempliﬁcar
informacion
obtenida
seleccionan
tres
atributos
mas
im
portantes
de
acuerdo
analisis
realizado
posteriormente
seccion
su
estudio
atributo
mean
std
min
max
header
proc
mov
push
mov
add
pop
cuadro
estadısticas
tres
atributos
mas
importantes
a
partir
tabla
anterior
pueden
realizarse
diversas
observaciones
basandonos
columna
max
ninguno
tres
atributos
utilizado
de
manera
desproporcionada
en
casos
menos
muestras
presentan
ocurrencias
para
atributos
basandose
desviacion
std
puede
considerarse
header
presenta
sus
valores
compactados
alrededor
media
mediante
dos
casos
estos
encuentran
mas
dispersos
a
continuacion
incluyen
imagenes
correspondientes
graﬁcos
las
densidades
cada
tres
atributos
figura
kde
plot
header
capitulo
analisis
exploratorio
preprocesamiento
datos
figura
kde
plot
ngrama
proc
mov
push
mov
figura
kde
plot
ngrama
add
pop
por
lado
tambien
encontraron
archivos
cumpliendo
alguna
con
diciones
estar
compuestos
enteramente
misma
seccion
data
seg
utilizar
solo
unica
librerıa
kerneldll
msvbvmdll
ejecutar
misma
instruccion
dd
db
ejecutar
misma
secuencia
instrucciones
grama
push
push
push
call
finalmente
incluye
graﬁco
densidad
valores
observados
la
seccion
seg
resulta
curioso
amplia
mayorıa
muestras
no
importancia
variables
utilizan
seccion
absoluto
sin
embargo
hacen
hacen
uso
de
ninguna
otra
figura
kde
plot
seccion
seg
importancia
variables
con
dataset
valores
nulos
resueltos
procedio
determinar
im
portancia
variables
hacer
estudio
importancia
variables
de
gran
utilidad
permite
comprender
que
atributos
mas
relevantes
el
modelo
que
vez
aporta
beneﬁcios
adicionales
tales
como
veriﬁcar
correctitud
modelo
evaluar
posibilidades
mejoras
cen
trar
enfoque
aquellos
atributos
mas
importante
modelo
acortar
tiempos
entrenamiento
sacriﬁcar
demasiada
performance
al
seleccionar
atributos
mas
relevantes
descartar
aquellos
cuya
incidencia
es
despreciable
mayor
interpretabilidad
modelo
tener
sacriﬁcar
necesariamente
demasiada
performance
si
hace
seleccion
atributos
adecuada
para
llevar
adelante
proceso
utilizo
algoritmo
clasiﬁcacion
random
forest
se
debe
recordar
random
forest
ensamble
decision
trees
que
utiliza
variacion
metodo
bagging
arboles
independientes
son
entrenados
utilizando
mismo
conjunto
datos
normalmente
forest
puede
contener
varios
cientos
arboles
capitulo
analisis
exploratorio
preprocesamiento
datos
utilizando
librerıa
scikitlearn
posible
establecer
importancia
los
atributos
random
forest
traves
dos
metodos
distintos
el
metodo
de
fecto
computar
importancia
variables
basa
mecanismo
de
decremento
mınimo
impureza
impuridad
gini
la
impuridad
gini
me
dida
determina
cual
probabilidad
nueva
observacion
sido
incorrectamente
clasiﬁcada
cuando
arbol
construido
decision
acerca
de
que
variable
utilizar
separar
cada
nodo
emplea
calculo
impuridad
gini
para
cada
variable
suma
decrementos
gini
cada
arbol
bosque
es
acumulada
cada
vez
variable
elegida
separar
nodo
la
suma
es
luego
dividida
cantidad
arboles
bosque
determinar
prome
dio
este
metodo
cuenta
ventaja
ser
facil
implementar
calculo
es
rapido
por
contrario
dado
estadısticas
computadas
derivan
modelo
de
entrenamiento
estas
podrıan
reﬂejar
habilidad
feature
ser
util
para
realizar
predicciones
generalicen
conjunto
prueba
el
metodo
determinar
importancia
variables
quizas
el
mas
utilizado
consiste
basicamente
observar
que
decrementa
precision
del
modelo
variable
excluida
cada
arbol
propias
observa
ciones
outofbag
oob
cuales
seran
utilizados
construccion
estas
observaciones
seran
utilizadas
calcular
importancia
variable
especıﬁ
ca
primero
precision
prediccion
observaciones
oob
medida
luego
los
valores
variables
observaciones
oob
mezcladas
aleatoriamente
mientras
variables
mantienen
igual
por
ultimo
decremento
la
precision
prediccion
observaciones
mezcladas
medido
el
decremento
mınimo
precision
traves
arboles
reportado
las
ventaja
metodo
puede
ser
aplicado
cualquier
modelo
resulta
bastante
eﬁciente
tecnica
bastante
conﬁable
como
desventaja
resulta
mu
cho
mas
costoso
computacionalmente
metodo
defecto
tambien
podrıa
sobreestimar
importancia
predictor
correlacionado
en
investigacion
utilizo
metodo
defecto
calculo
impor
tancia
variables
ser
menos
costo
computacional
tiene
la
imagen
ilustra
resultado
arrojo
proceso
indicado
anteriormente
en
dicho
graﬁco
puede
observarse
totalidad
variables
ordenadas
ma
yor
menor
relevancia
como
valores
ﬁnal
curva
realmente
muy
pequenos
httpsscikitlearnorg
seleccion
variables
figura
importancia
variables
seleccion
variables
determinar
importancia
variables
solo
puede
ayudar
lograr
una
mejor
interpretacion
datos
sino
tambien
permite
establecer
un
ranking
seleccionar
aquellos
features
realmente
importantes
modelo
de
prediccion
en
contexto
utilizando
librerıa
scikit
learn
seleccion
variables
junto
resultado
obtenido
paso
anterior
utilizo
objeto
selectfrommodel
provisto
mencionada
librerıa
scikit
learn
este
objeto
seleccionara
to
dos
aquellos
features
cuyo
valor
importancia
mayor
umbral
nuestro
caso
media
importancia
features
el
resultado
aplicar
proceso
permitio
reducir
notablemente
comple
jidad
dataset
cual
paso
tener
columnas
tan
solo
cuyas
columnas
se
componen
de
codigos
seccion
librerıas
dll
httpsscikitlearnorgstablemodulesgeneratedsklearnfeature_selection
selectfrommodelhtml
capitulo
analisis
exploratorio
preprocesamiento
datos
codigos
operaciones
gramas
gramas
gramas
columnas
correspondientes
distintos
tamanos
archivo
estandarizacion
atributos
como
menciono
marco
teorico
investigacion
muchas
veces
se
trabaja
datos
magnitudes
resultan
diferentes
sı
esto
puede
ser
problema
estimadores
empleados
algoritmos
machine
learning
sensibles
estandarizado
atributos
podrıan
tener
com
portamiento
errado
si
valores
mas
menos
similares
por
ejemplo
muchos
elementos
utilizados
funcion
objetivo
algoritmo
aprendizaje
tales
rbf
kernel
support
vector
machine
regularizaciones
l
l
los
modelos
lineales
asumen
features
estan
centrados
alrededor
y
tienen
varianza
mismo
orden
si
feature
posee
varianza
cuyo
orden
de
magnitud
superior
otros
este
probablemente
domine
funcion
objetivo
y
haga
estimador
posible
aprender
features
correctamente
dependiendo
problema
quiere
abordar
pueden
utilizar
diferentes
tecnicas
estandarizacion
normalizacion
datos
que
estos
resulten
utiles
hora
poner
marcha
algoritmos
machine
learning
que
desean
para
presente
investigacion
realizo
escalado
estandar
que
para
realizar
extracion
atributos
utilizando
kernel
pca
escalado
estandar
funciona
bien
correlacion
a
traves
calculo
correlacion
pearson
calcula
coeﬁciente
que
establece
medida
dos
variables
correlacionan
pudo
construir
la
graﬁca
los
puntos
mas
claros
indicadores
alta
correlacion
los
pares
mientras
colores
oscuros
indicadores
correlacion
leve
la
ausencia
misma
del
graﬁco
desprenden
correlaciones
obvias
tales
n
gramas
ngramas
similares
dd
dd
dd
dd
dd
valor
tam
bien
percibieron
correlaciones
hablan
estructura
archivos
por
ejemplo
seccion
header
presenta
correlacion
seccion
idata
extraccion
atributos
kernel
pca
por
lado
correlaciones
esperables
dado
funcionamiento
len
guaje
assembler
ejemplo
operacion
mov
llamado
subrutinas
tiene
una
correlacion
operaciones
relacionadas
pasaje
parame
tros
push
pop
figura
correlacion
pearson
extraccion
atributos
kernel
pca
principal
component
analysis
pca
herramienta
utiliza
reducir
la
dimensionalidad
datos
perder
informacion
pca
reduce
dimension
hallando
combinaciones
lineales
ortogonales
componentes
principales
de
las
variables
originales
varianza
mas
alta
el
primer
componente
principal
captura
mayor
parte
varianza
datos
el
segundo
componente
principal
es
ortogonal
primer
componente
captura
varianza
restante
dejada
capitulo
analisis
exploratorio
preprocesamiento
datos
por
primer
componente
principal
ası
sucesivamente
existen
tantos
componen
tes
principales
numero
variables
original
estos
componentes
principales
no
correlacion
encuentran
ordenados
manera
primeros
com
ponentes
principales
expliquen
mayorıa
varianza
datos
originales
dado
pca
metodo
lineal
solo
puede
ser
aplicado
conjuntos
datos
que
linealmente
separables
resultara
buena
herramienta
siempre
cuan
do
datos
cumplan
premisa
kernel
pca
cambio
utiliza
funcion
de
kernel
proyectar
dataset
espacio
dimensional
mas
alto
linealmen
te
separable
manera
similar
hace
support
vector
machine
para
presente
investigacion
analisis
componentes
principales
llevo
a
cabo
utilizando
herramienta
kernel
pca
en
graﬁco
puede
observar
el
resultado
aplicar
dicha
herramienta
figura
analisis
componentes
kernel
pca
al
observar
graﬁco
posible
determinar
como
numero
componen
tes
superior
logra
explicar
gran
porcentaje
varianza
modelo
en
la
practica
numero
componentes
seleccionar
depende
que
tanta
varianza
se
desea
kpca
explique
para
investigacion
ﬁjo
valor
por
lo
tomarse
primeros
componentes
principales
resumen
la
realizacion
pasos
previamente
descriptos
permitio
solo
reducir
drasticamente
dimensionalidad
conjunto
datos
sino
tambien
eli
minar
muestras
posible
trabajar
dado
archivos
de
resumen
origen
encontraban
danados
tambien
logro
mejor
entendimiento
pro
blema
quiere
modelar
algoritmos
machine
learning
ademas
al
rellenar
datos
faltantes
datos
nulos
ceros
ası
tambien
estandari
zacion
logro
llevar
dataset
mas
consistente
completo
capitulo
clasiﬁcacion
malware
en
capıtulo
anterior
describieron
tareas
realizadas
depuracion
selecion
tranformacion
atributos
ası
obtuvo
conjunto
datos
mas
con
sistente
cada
variables
involucradas
realmente
importantes
para
modelo
de
modo
puede
dar
comienzo
construccion
elabora
cion
diversos
algoritmos
machine
learning
estos
algoritmos
permitiran
realizar
las
clasiﬁcaciones
distintas
familias
malware
partir
resultados
que
arrojen
podran
realizar
comparaciones
establecer
conclusiones
algoritmos
clasiﬁcacion
como
menciono
marco
teorico
investigacion
existen
varios
modelos
pueden
ser
utilizados
problemas
clasiﬁcacion
ellos
se
seleccionaron
siguientes
knearest
neighbors
random
forest
xgboost
red
neuronal
todos
seran
ejecutados
tomando
datos
entrada
dataset
prepro
cesado
los
resultados
seran
luego
evaluados
mediante
valor
correspondiente
la
precision
gracias
libreria
scikitlearn
matriz
confusion
graﬁca
roc
receiver
operating
characteristic
knearest
neighbors
en
primera
instancia
realizo
implementacion
algoritmo
knearest
neighbors
como
explicado
marco
teorico
knearest
neighbors
knn
clasiﬁca
cada
punto
mediante
analisis
vecinos
mas
cercanos
dentro
del
capitulo
clasiﬁcacion
malware
conjunto
entrenamiento
el
punto
asignado
clase
mas
comun
en
contrada
dichos
vecinos
es
algoritmo
parametrico
realiza
asunciones
acerca
como
datos
estan
distribuidos
implementacion
para
implementacion
knearest
neighbors
llevaron
cabo
siguientes
pasos
se
leen
archivos
contienen
datos
preprocesados
explicados
ante
riormente
archivo
contiene
etiquetas
utilizando
librerıa
scikitlearn
separa
conjunto
datos
training
y
testing
proporcion
o
respectivamente
se
crean
dos
objetos
clasiﬁcador
kneighborsclassifier
otra
correspondiente
analisis
componentes
vecindario
denominada
neighborhoodcomponent
ambas
pertenecientes
librerıa
scikitlearn
el
neighborhood
component
analy
sis
objetivo
encontrar
transormacion
lineal
maximice
la
precision
clasiﬁcacion
vecino
mas
cercano
estocastico
conjunto
de
entrenamiento
tanto
kneighborsclassifier
valor
k
establecido
valor
por
defecto
neighborhoodcomponentsanalysis
tambien
sus
valores
defecto
colocados
dentro
estructura
llamada
pipeline
el
proposito
pipeline
ensamblar
varios
pasos
puedan
ser
vali
dados
juntos
utilizando
crossvalidation
permitiendo
ası
conﬁguracion
de
diferentes
parametros
ya
ﬁnalizado
paso
anterior
procede
realizar
fit
ajustando
mo
delo
acuerdo
datos
entrenamiento
provistos
el
paso
ﬁnal
sera
prediccion
sera
realizada
conjunto
de
prueba
esto
permitira
luego
proyectar
resultados
matriz
de
confusion
precision
logrado
metricas
evaluacion
para
caso
knearest
neighbors
gracias
utilizacion
analisis
com
ponentes
vecindario
logrado
precision
prediccion
contra
haberla
utilizado
se
debe
recordar
precision
de
las
metricas
mas
comunes
utilizadas
medir
performance
modelo
cla
siﬁcacion
esta
permitira
identiﬁcar
rapidamente
proporcion
aciertos
que
obtuvo
modelo
httpsscikitlearnorgstablemodulesgeneratedsklearnneighbors
neighborhoodcomponentsanalysishtml
httpsscikitlearnorgstablemodulesgeneratedsklearnpipelinepipelinehtml
algoritmos
clasiﬁcacion
como
mencionado
marco
teorico
matriz
confusion
sı
es
una
metrica
sı
permite
realizar
evaluacion
acerca
como
desempeno
nuestro
modelo
cada
clase
en
matriz
confusion
knearest
neighbors
en
graﬁco
pueden
observarse
diagonal
principal
dos
valores
valor
entero
representa
numero
aciertos
obtuvo
modelo
clase
y
valor
proporcional
mientras
debajo
encima
diagonal
se
encuentran
valores
observaciones
clasiﬁcadas
erroneamente
figura
matriz
confusion
k
nearest
neighbors
por
lado
graﬁco
resulta
interesante
analizar
curva
roc
re
ceiver
operating
characteristic
una
curva
roc
graﬁco
representa
per
formance
modelo
umbrales
clasiﬁcacion
esta
curva
dibuja
en
base
dos
parametros
el
ratio
verdaderos
positivos
true
positive
rate
tpr
el
ratio
falsos
positivos
false
positive
rate
fpr
en
donde
tpr
sinonimo
recall
esta
dado
por
tpr
tp
tp
fn
y
fpr
encuentra
deﬁnido
por
fpr
fp
fp
tn
capitulo
clasiﬁcacion
malware
esta
curva
tıpicamente
presenta
ratios
verdaderos
positivos
eje
y
y
ratio
falsos
positivos
eje
x
esto
signiﬁca
esquina
izquierda
la
graﬁca
encuentra
punto
ıdeal
ratio
falsos
positivos
igual
cero
y
un
ratio
verdaderos
positivos
igual
por
tanto
cuanto
mas
grande
area
bajo
curva
area
under
the
curve
auc
mejor
si
bien
curvas
normalmente
utilizadas
clasiﬁcacion
binaria
po
sible
extender
curva
roc
area
roc
clasiﬁcacion
multiclases
utilizando
la
herramienta
scikitlearn
para
realizar
necesario
binarizar
salida
que
luego
clasiﬁcador
aprenda
predecir
cada
clase
otra
el
graﬁco
descripto
knearest
neighbours
puede
encontrar
la
ﬁgura
figura
roc
k
nearest
neighbors
random
forest
a
continuacion
realizo
implementacion
algoritmo
random
forest
ran
dom
forest
compone
ensamble
varios
arboles
decision
utiliza
bootstrapping
seleccion
aleatoria
conjunto
observaciones
reemplazo
varios
subconjuntos
aleatorios
dataset
considera
separacion
cada
nodo
arbol
decision
voto
promedio
mejorar
precision
la
prediccion
controlar
sobreajuste
overﬁtting
implementacion
para
implementacion
random
forest
llevaron
cabo
siguientes
pasos
se
leen
archivos
contienen
datos
preprocesados
explicados
ante
riormente
archivo
contiene
etiquetas
algoritmos
clasiﬁcacion
utilizando
librerıa
scikitlearn
separa
conjunto
datos
training
y
testing
proporcion
o
respectivamente
se
crea
instancia
clasiﬁcador
randomforestclassifier
todos
sus
parametros
valores
defecto
ya
ﬁnalizado
paso
anterior
procede
realizar
fit
ajustando
mo
delo
acuerdo
datos
entrenamiento
provistos
el
paso
ﬁnal
sera
prediccion
sera
realizada
conjunto
de
prueba
esto
permitira
luego
proyectar
resultados
matriz
de
confusion
precision
alcanzado
metricas
evaluacion
para
random
forest
precision
alcanzada
la
matriz
confu
sion
puede
observarse
graﬁco
mientras
curva
roc
encuentra
en
el
graﬁco
figura
matriz
confusion
random
forest
httpsscikitlearnorgstablemodulesgeneratedsklearnensemble
randomforestclassifierhtml
capitulo
clasiﬁcacion
malware
figura
roc
random
forest
xgboost
xgboost
extreme
gradient
boosting
implementaciones
algorit
mos
predictivos
supervisados
mas
utilizados
actualidad
como
menciono
marco
teorico
investigacion
xgboost
utiliza
el
principio
boosting
la
idea
booting
generar
varios
modelos
prediccion
debilessecuencialmente
ﬁn
generar
modelo
mas
fuerte
mayor
poder
predictivo
mayor
estabilidad
resultados
para
lograr
esto
modelo
emplea
algoritmo
optimizacion
denominado
gradient
descent
descenso
del
gradiente
cada
modelos
tomara
resultados
modelo
anterior
com
paraa
si
nuevo
modelo
mejores
resultados
entonces
utilizara
base
para
realizar
modiﬁcaciones
si
cambio
peores
resultados
regresa
al
mejor
modelo
anterior
mismo
sera
modiﬁcado
manera
diferente
este
proceso
iterativo
repetira
punto
diferencia
los
modelos
consecutivos
insigniﬁcante
indicarıa
llego
mejor
mo
delo
posible
llega
numero
iteraciones
maximas
deﬁnidas
el
usuario
implementacion
para
implementacion
xgboost
llevaron
cabo
siguientes
pasos
se
leen
archivos
contienen
datos
preprocesados
explicados
ante
riormente
archivo
contiene
etiquetas
httpsxgboostai
algoritmos
clasiﬁcacion
utilizando
librerıa
scikitlearn
separa
conjunto
datos
training
y
testing
proporcion
o
respectivamente
se
crea
instancia
clasiﬁcador
xgbclassifier
parame
tros
valores
defecto
xgbclassifier
implementacion
la
api
scikitlearn
clasiﬁcacion
xgboost
ya
ﬁnalizado
paso
anterior
procede
realizar
fit
ajustando
mo
delo
acuerdo
datos
entrenamiento
provistos
el
paso
ﬁnal
sera
prediccion
sera
realizada
conjunto
de
prueba
esto
permitira
luego
proyectar
resultados
matriz
de
confusion
precision
logrado
metricas
evaluacion
con
utilizacion
xgboost
logro
precision
clasiﬁca
cion
su
matriz
confusion
puede
verse
ﬁgura
mientras
curva
roc
observa
ﬁgura
figura
matriz
confusion
xgboost
httpsxgboostreadthedocsioenlatestpythonpython_apihtmlhighlight
xgbclassifierxgboostxgbclassifier
capitulo
clasiﬁcacion
malware
figura
roc
xgboost
artiﬁcial
neural
networks
las
artiﬁcial
neural
networks
redes
neuronales
artiﬁciales
construyen
de
simples
elementos
llamados
neuronas
cuales
toman
valor
real
multiplican
por
peso
ejecutan
traves
funciones
activacion
lineales
mediante
la
construccion
multiples
capas
neuronas
cada
cuales
recibe
parte
de
variables
entrada
cuyos
resultados
luego
seran
pasados
siguientes
capas
red
puede
ser
capaz
aprender
funciones
realmente
complejas
teorica
mente
red
neuronal
sera
capaz
aprender
forma
cualquier
funcion
con
simplemente
darle
suﬁciente
poder
computacional
implementacion
la
implementacion
artiﬁcial
neural
networks
llevo
cabo
utilizando
la
herramienta
tensorﬂow
integrado
api
embebida
keras
tensorﬂow
bi
blioteca
software
codigo
abierto
computacion
numerica
utiliza
gra
fos
ﬂujo
datos
los
nodos
grafos
representan
operaciones
matematicas
mientras
aristas
representan
matrices
datos
multidimensionales
ten
sores
comunicadas
ellos
fue
disenado
originalmente
interfaz
para
expresar
implementar
algoritmos
machine
learning
cuales
destacan
las
deep
neural
networks
redes
neuronales
profundas
mientras
keras
una
api
alto
nivel
tensorﬂow
construir
entrenar
modelos
aprendizaje
profundo
tiene
interfaz
sencilla
resulta
facil
usar
implementar
los
pasos
implementacion
siguientes
httpswwwtensorfloworgoverview
httpswwwtensorfloworgguidekerashles
algoritmos
clasiﬁcacion
se
lee
archivo
contiene
datos
preprocesados
explicados
anteriormen
te
archivo
contiene
etiquetas
labels
en
caso
red
neuronal
vez
leıdo
archivo
labels
debio
realizar
sobre
mismo
encoding
datos
red
ası
requiere
utilizando
la
funcion
onehotencoder
provista
scikitlearn
posible
codiﬁcar
va
lores
categoricos
arreglo
numerico
onehot
tambien
conocido
como
oneofk
dummy
variable
este
esquema
codiﬁcado
encoding
crea
co
lumna
binaria
cada
categorıa
capaz
devolver
matriz
dispersa
o
arreglo
denso
segun
requiera
en
caso
opto
arreglo
denso
una
vez
realizado
encoding
labels
utilizando
librerıa
scikitlearn
se
separa
conjunto
datos
training
testing
proporcion
o
y
respectivamente
a
continuacion
creo
modelo
capas
componen
ademas
se
debera
tambien
deﬁnir
cuantas
salidas
tendra
red
caso
que
se
clases
distintas
la
red
contruyo
dos
capas
neronas
cada
una
ambas
funcion
activacion
relu
rectiﬁed
linear
unit
y
la
capa
salida
funcion
activacion
softmax
el
siguiente
paso
consistio
compilar
modelo
los
parametros
fueron
ajustados
optimizador
utilizo
adam
la
optimizacion
adam
metodo
descenso
gradiente
estocastico
basa
la
estimacion
adaptativa
momentos
primer
segundo
orden
funcion
de
perdida
loss
deﬁnio
categoricalcrossentropy
se
utiliza
la
funcion
crossentropy
loss
mas
dos
clases
metrica
para
indicamos
accuracy
finalizada
etapa
anterior
procedio
realizar
ﬁt
ajustando
modelo
a
lo
descripto
como
paso
ﬁnal
sera
prediccion
sera
realizada
conjunto
de
prueba
esto
permitira
luego
proyectar
resultados
matriz
de
confusion
precision
logrado
metricas
evaluacion
la
precision
alcanzada
modelo
propuesto
artiﬁcial
neural
network
fue
de
los
gaﬁcos
muestran
como
comporto
modelo
cuanto
a
precision
accuracy
perdida
loss
respectivamente
se
debe
recordar
que
httpswwwtensorfloworgapi_docspythontfkerasoptimizersadam
httpswwwtensorfloworgapi_docspythontfkeraslosses
categoricalcrossentropy
capitulo
clasiﬁcacion
malware
la
precision
metrica
indicara
tan
acertada
prediccion
mode
lo
comparado
valores
reales
datos
el
valor
representa
debe
leer
como
porcentaje
mientras
funcion
perdida
retornara
valor
real
el
cual
calcula
fase
entrenamiento
validacion
indi
cara
tan
bien
tan
pobre
comporto
modelo
luego
cada
iteracion
cuanto
mas
pequeno
dicho
valor
mejor
resultara
modelo
figura
precision
accuracy
modelo
clasiﬁcacion
fa
milias
figura
perdida
loss
modelo
clasiﬁcacion
familias
en
graﬁco
encuentra
matriz
confusion
representan
todas
predicciones
realizo
modelo
datos
prueba
que
tan
acertadas
fueron
comparaciones
conclusiones
figura
matriz
confusion
artiﬁcial
neural
networks
por
ultimo
graﬁco
representa
curva
roc
red
figura
roc
artiﬁcial
neural
networks
comparaciones
conclusiones
en
primera
instancia
implemento
modelo
knearest
neighbors
dar
luego
lugar
mas
complejos
ejemplo
artiﬁcial
neural
network
capitulo
clasiﬁcacion
malware
el
knearest
neighbors
junto
neighborhood
component
analysis
demos
trado
ser
buena
opcion
alcanzando
valores
optimos
casi
par
cual
quier
implementacion
mas
compleja
yo
costosa
el
neighborhood
com
ponent
analysis
logro
darle
knearest
neighbors
cierta
mejora
algoritmo
ya
por
sı
mismo
habıa
alcanzado
buenos
resultados
agregandole
costo
compu
tacional
practicamente
imperceptible
a
continuacion
modo
comparativo
decidio
implementar
metodo
en
samble
caso
random
forest
estos
poderosos
algoritmos
pueden
resul
tar
buena
alternativa
tienden
realizar
buena
generaliza
cion
datos
evitar
overﬁtting
implementarlo
resulto
bastante
simple
logrando
precision
rango
cercano
tiempo
ejecu
cion
bueno
una
implementacion
algoritmo
gradiente
arboles
reforzados
ha
vuelto
popular
ampliamente
utilizada
comunidad
cientıﬁcos
da
tos
ultimos
anos
xgboost
esta
robusta
eﬁciente
implementacion
de
codigo
abierto
demostrado
realmente
altura
aquello
lo
que
creado
su
velocidad
rendimiento
forman
parte
caracterısticas
prin
cipales
internamente
datos
estructura
matriz
llamada
dmatrix
la
encuentra
optimizada
uso
memoria
velocidad
de
entrenamiento
esta
popularidad
buena
fama
motivo
utilizarlo
la
clasiﬁcacion
familias
malware
para
investigacion
logro
una
precision
cercana
tiempo
ejecucion
mas
aceptable
por
ultimo
opto
desarrollar
red
neuronal
popularidad
y
porque
basicamente
capaces
resolver
casi
cualquier
problema
para
simpliﬁ
car
hacer
mas
rapida
sencilla
implementacion
utilizo
framework
tensor
ﬂow
este
framework
integrado
api
keras
capaces
proveer
interfaz
sencilla
facil
usar
implementacion
artiﬁcal
neural
networks
deep
neural
networks
la
red
neuronal
fue
investigacion
logro
precision
mas
alta
excelente
performace
por
tanto
podrıa
decirse
todas
implementaciones
resultados
ser
opciones
mas
validas
pueden
ser
cuenta
clasiﬁcacion
de
diferentes
familias
malware
haciendo
necesario
llevar
cabo
una
costosa
implementacion
red
neuronal
clasiﬁcar
muestras
ya
que
posible
obtener
practicamente
mismos
resultados
cualquiera
los
otros
algoritmos
clasiﬁcacion
mencionados
investigacion
capitulo
deteccion
malware
en
capıtulo
deﬁnio
cuales
pasos
realizados
llevar
cabo
la
clasiﬁcacion
distintas
familias
malware
a
continuacion
buscara
im
plementar
sistema
clasiﬁcacion
binario
capaz
determinar
si
un
archivo
maligno
benigno
en
presente
capıtulo
explicara
proceso
clasiﬁcacion
realizo
para
determinar
si
archivo
puede
ser
considerado
maligno
no
para
ello
comen
zara
describiendo
que
consistieron
tareas
obtencion
desensamblado
de
los
archivos
pasos
generar
dataset
etapas
preprocesamiento
del
mismo
finalmente
abordaran
describiran
soluciones
propuestas
obtencion
desensamblado
archivos
benignos
el
problema
deteccion
malware
puede
ser
visto
clasiﬁcacion
binaria
debe
determinar
si
dado
archivo
conocido
por
el
algoritmo
este
capaz
determinar
si
mismo
detectado
malware
por
lo
tanto
sera
necesario
contar
muestras
aplicaciones
consideradas
benignas
de
sitios
cnet
sourceforge
descargaron
total
aplicaciones
livianas
una
vez
obtenidas
todas
aplicaciones
procedio
realizarles
des
ensamblado
para
ello
utilizo
aplicacion
interactive
disassembler
mas
conocida
por
acronimo
ida
este
desensamblador
generalmente
utilizado
realizar
ingenierıa
inversa
ejecutables
y
modo
poder
convertir
apli
cacion
archivo
asm
ser
necesario
bytes
estos
formatos
que
se
requeriran
comenzar
proceso
extraccion
informacion
fue
descripto
capıtulo
httpsdownloadcnetcom
httpssourceforgenet
httpswwwhexrayscomproductsidasupportdownload_freeware
capitulo
deteccion
malware
junto
muestras
correspondientes
archivos
benignos
seleccionaron
un
numero
igual
malwares
provenientes
dataset
nueve
familias
por
tanto
total
muestras
realizo
proceso
minerıa
de
datos
encontro
conformado
total
archivos
cuales
se
corresponden
archivos
benignos
archivos
malignos
generacion
nuevo
dataset
de
forma
analoga
trabajo
realizado
clasiﬁcacion
familias
lo
pasos
correspondientes
extraccion
informacion
mencionados
capıtulo
debieron
ser
realizados
nuevamente
generar
nuevo
dataset
analisis
exploratorio
preprocesamiento
con
dataset
creado
procedio
realizar
analisis
exploratorio
prepro
cesamiento
datos
mismo
modo
hizo
dataset
malwares
en
cuanto
analisis
exploratorio
estudio
nuevamente
correlacion
pearson
como
puede
ver
graﬁca
y
analogo
sucedido
clasiﬁcacion
de
familias
pueden
observar
correlaciones
mas
altas
ngramas
similares
como
ejemplo
mov
sub
mov
sub
mov
analisis
exploratorio
preprocesamiento
figura
correlacion
pearson
clasiﬁcacion
malwareno
malware
por
lado
preprocesamiento
explico
capıtulo
consistio
en
tratamiento
datos
nulos
faltantes
determinar
importancia
las
variables
estandarizar
datos
ultimo
seleccion
extraccion
variables
para
tratamiento
valores
nulos
faltantes
tomaron
mismas
deci
siones
caso
dataset
malware
completando
valores
faltantes
con
ceros
en
cuanto
calculo
importancia
variables
mismo
realizo
utili
zando
random
forest
en
graﬁco
puede
observar
resultado
aplicar
dicho
proceso
si
comparamos
graﬁco
podemos
ver
diferentes
capitulo
deteccion
malware
las
variables
allı
consideradas
importantes
aquı
incluso
proba
blemente
encuentren
presentes
excepcion
variable
header
lidera
la
lista
ambos
casos
estas
diferencias
ocurren
extrajo
informa
cion
conjunto
datos
inicial
los
samples
malware
proceso
realizo
la
minerıa
determino
ciertas
caracterısticas
comunes
ellas
mientras
que
cuando
realizar
proceso
nuevamente
considerando
lugar
los
archivos
benignos
caracterısticas
naturalmente
cambiaron
figura
importancia
variables
clasiﬁcacion
malwa
reno
malware
con
respecto
seleccion
atributos
procedio
misma
manera
para
el
caso
clasiﬁcacion
familias
al
aplicar
proceso
seleccion
redujo
el
dataset
compuesto
columnas
puede
ser
considerado
una
reduccion
signiﬁcativa
al
igual
caso
dataset
malwares
realizo
escalado
datos
utilizando
escala
estandar
por
ultimo
aplico
proceso
extraccion
variables
utilizando
kernel
prin
cipal
component
analysis
mas
conocido
acronimo
kpca
en
graﬁco
puede
observarse
varıa
varianza
medida
aumenta
numero
com
ponentes
en
caso
componentes
posible
explicar
va
rianza
implementacion
solucion
figura
analisis
componentes
kernel
pca
clasi
ﬁcacion
malwareno
malware
implementacion
solucion
el
objetivo
implementacion
consiste
poder
determinar
si
archivo
puede
ser
clasiﬁcado
malware
que
grado
precision
para
ello
se
opto
llevar
cabo
implementacion
artiﬁcal
neural
network
que
la
esta
mejor
desempeno
obtuvo
clasiﬁcacion
familias
de
malware
modelo
base
para
ello
comenzo
implementando
red
neuronal
respetando
misma
conﬁ
guracion
utilizo
clasiﬁcacion
familias
vez
clasiﬁcacion
serıa
binaria
malwareno
malware
esta
red
logro
buen
resultado
su
precision
apenas
superior
sin
embargo
si
observamos
graﬁcos
y
puede
ver
como
modelo
esta
precision
mayor
duran
te
entrenamiento
mientras
datos
prueba
obtiene
valores
inferiores
al
podrıa
ser
claro
indicio
modelo
esta
sobreajustando
overﬁtting
valores
datos
entrenamiento
capitulo
deteccion
malware
figura
precision
accuracy
modelo
clasiﬁcacion
malwareno
malware
figura
perdida
loss
modelo
clasiﬁcacion
malwareno
malware
tratamiento
sobreajuste
overﬁtting
como
menciono
previamente
modelo
esta
sobreajustando
valores
los
datos
entrenamiento
existen
varias
alternativas
pueden
llevar
cabo
para
minimizar
incluso
eliminar
problema
a
continuacion
iran
mencionando
y
explicando
que
consisten
cada
como
aplicadas
complejidad
modelo
una
formas
mas
simples
evitar
overﬁtting
simpliﬁcando
com
plejidad
modelo
reduciendo
numero
capas
componen
implementacion
solucion
una
forma
resolver
problema
puede
ser
comenzando
creacion
de
un
modelo
simple
unas
pocas
capas
luego
ir
agregando
gradualmente
mas
capas
llegar
modelo
mejor
desempeno
tenga
tensorﬂow
junto
he
rramienta
interactiva
visualizacion
tensorboard
provee
varias
herramientas
que
ayudan
proceso
identiﬁcar
mejor
experimento
conjunto
hiperparame
tros
mas
prometedor
tasa
aprendizaje
learning
rate
muchas
veces
modelos
responden
mejor
si
tasa
aprendizaje
learning
rate
para
optimizadores
mas
baja
entrenamiento
posible
o
bien
establecer
tasa
ﬁja
inferior
previamente
deﬁnida
ir
decrementandola
gradualmente
medida
avanza
entrenamiento
en
experimento
la
ultima
solucion
mejor
resulto
parada
temprana
early
stop
otra
manera
prevenir
overﬁtting
deteniendo
manera
temprana
proce
so
entrenamiento
lugar
entrenar
cierto
numero
epochs
proceso
se
detiene
tan
pronto
perdida
loss
funcion
este
monitoreando
se
eleve
que
seguir
entrenando
modelo
solo
empeorarıa
para
ello
uti
lizo
utilidad
provista
api
keras
callbacksearlystopping
cual
permite
parametrizar
monitoreo
metrica
desea
paciencia
se
le
tendra
misma
detener
entrenamiento
regularizacion
pesos
weight
regularization
una
manera
comun
mitigar
overﬁtting
imponiendo
restricciones
en
cuanto
complejidad
red
forzando
pesos
solo
tomen
valores
pe
quenos
hace
distribucion
valores
pesos
mas
regular
esto
denomina
weight
regualrization
logra
agregando
funcion
perdida
de
red
penalizacion
asociada
tener
grandes
pesos
y
pueden
ser
divididas
en
l
regularization
donde
costo
adicionado
proporcional
valor
abso
luto
coeﬁcientes
pesos
l
regularization
donde
costo
adicionado
proporcional
cuadra
do
valor
coeﬁcientes
pesos
en
modelo
probado
conﬁgurado
modelo
tomara
dis
tintos
valores
regularizacion
l
httpswwwtensorfloworgtensorboardhles
capitulo
deteccion
malware
agregado
dropout
dropout
tecnicas
mas
efectivas
mas
comunmente
utilizados
de
regularizacion
la
justiﬁcacion
detras
tecnica
indica
que
nodos
indivi
duales
red
pueden
conﬁar
salida
nodos
cada
nodo
debe
generar
propios
features
salida
utiles
sı
mismos
por
tanto
el
dropout
aplicado
capa
consiste
quitar
llevarlo
cero
manera
aleatoria
cierto
numero
features
salida
capa
entrenamiento
con
uso
api
mencionada
usuario
puede
establecer
proporcion
dropout
rate
valores
seran
quitados
todas
estrategias
combinadas
red
realizar
el
ajuste
hiperparametros
el
resultado
aplicar
dicho
proceso
puede
verse
el
graﬁco
allı
observa
listado
todas
pruebas
realizadas
precision
alcanzada
a
partir
resultados
tomaron
decisiones
diseno
la
artiﬁcial
neural
network
ayudaran
mejorar
funcionamiento
misma
implementacion
solucion
figura
conﬁguracion
hiper
parametros
clasiﬁcacion
malwareno
malware
capitulo
deteccion
malware
resultados
obtenidos
una
vez
obtenido
modelo
parametros
optimizados
realizaron
nue
vamente
pruebas
clasiﬁcacion
binaria
esta
vez
modelo
obtuvo
una
precision
apenas
superior
si
bien
logro
mejora
sustancial
respec
to
modelo
base
alrededor
resultados
siguen
ser
buenos
si
se
observa
graﬁca
puede
apreciar
que
aunque
logrado
reducir
ligeramente
sobreajuste
este
todavıa
encuentra
presente
figura
precision
accuracy
modelo
clasiﬁcacion
malwareno
malware
figura
perdida
accuracy
modelo
clasiﬁcacion
malwa
reno
malware
a
continuacion
encuentra
graﬁca
muestra
matriz
confusion
que
dio
resultado
red
implementacion
solucion
figura
matriz
consuﬁcon
modelo
clasiﬁcacion
malwareno
malware
mientras
curva
roc
encuentra
graﬁcada
ﬁgura
figura
roc
modelo
clasiﬁcacion
malwareno
malware
xgboost
solucion
alternativa
en
seccion
anterior
vio
implementacion
red
neuronal
tu
vo
buen
desempeno
realizando
clasiﬁcacion
binaria
dado
posible
reducir
completamente
overﬁtting
a
continuacion
utilizara
algoritmo
de
ensamble
xgboost
estos
igual
random
forest
suelen
realizar
muy
buenas
generalizaciones
modo
evitar
problema
overﬁtting
modelado
al
igual
caso
red
neuronal
modelo
base
xgboost
ser
ajustado
hiperparametros
tampoco
logro
alcanzar
buena
preci
sion
este
modelo
implemento
utilizando
librerıa
scikit
learn
cuenta
con
un
wrapper
interface
xgboost
esta
interfaz
permite
crear
modelo
parametri
zarlo
segun
desee
a
continuacion
mencionaran
parametros
sido
modiﬁcados
investigacion
learning
rate
indica
velocidad
aprendera
modelo
logrando
uno
mas
robusto
ir
encogiendo
pesos
cada
iteracion
capitulo
deteccion
malware
max
depth
este
parametro
determinara
profundidad
puede
tomar
un
arbol
subsample
este
valor
denota
fraccion
samples
u
observaciones
seran
tomadas
manera
aleatoria
cada
arbol
nestimators
este
valor
determina
numero
arboles
o
rondas
tendra
el
modelo
ademas
utilizo
tambien
parada
temprana
early
stopping
rounds
durante
el
entrenamiento
disminuir
overﬁtting
resultados
obtenidos
la
implementacion
modelo
logrado
cierta
mejora
respecto
red
neu
ronal
artiﬁcial
alcanzando
precision
a
continuacion
encuentran
los
graﬁcos
correspondientes
precision
lograda
entrenamiento
en
pruebas
igual
graﬁca
error
clasiﬁcacion
figura
precision
accuracy
modelo
clasiﬁcacion
malwareno
malware
utilizando
xgboost
implementacion
solucion
figura
error
modelo
clasiﬁcacion
malwareno
malwa
re
utilizando
xgboost
su
matriz
confusion
puede
observarse
ﬁgura
graﬁca
roc
en
figura
matriz
confusion
clasiﬁcacion
malwareno
malware
utilizando
xgboost
figura
roc
clasiﬁcacion
malwareno
malware
utilizando
xgboost
capitulo
deteccion
malware
comparaciones
conclusiones
se
comenzo
implementando
red
neuronal
similar
utilizada
la
clasiﬁcacion
familias
malware
esta
red
modelo
base
arrojo
buenos
resultados
motivo
intentar
ajustar
hiperparametros
determinar
si
de
manera
posible
lograr
mejora
modelo
predicciones
para
ello
implementaron
varias
estrategias
ayudarıan
dicho
modelo
lograr
un
mejor
desempeno
este
proceso
realizo
combinando
diversas
maneras
los
distintos
parametros
modelo
luego
determinar
mediante
uso
alguna
metrica
opcion
produce
mejores
resultados
habiendo
hecho
esto
modiﬁco
modelo
inicial
incorporando
parame
tros
procedio
correr
nuevamente
pruebas
clasiﬁcacion
si
bien
logro
cierta
mejora
precision
reducir
overﬁtting
resultado
alcanzo
valores
aceptables
por
lado
alternativa
red
neuronal
llevo
cabo
implemen
tacion
xgboost
modo
evaluar
si
posible
mejorar
precision
la
prediccion
alcanzada
red
neuronal
este
modelo
logro
cierta
mejora
respecto
a
red
precision
superior
embargo
valor
aun
sigue
siendo
bajo
se
identiﬁcaron
menos
tres
problemas
pueden
impactar
negativamente
en
la
performance
ambos
modelos
lado
posible
garantizar
sam
ples
benignos
realmente
benignos
esto
debe
archivos
considerados
benignos
descargados
sitios
pueden
dar
fe
legitimidad
por
otro
lado
encuentra
heterogeneidad
archivos
dispares
entre
sı
los
programas
pueden
variar
plugin
editor
texto
por
ultimo
cabe
mencionar
tamano
dataset
utilizado
solo
dispuso
un
total
samples
mientras
red
clasiﬁcacion
familias
contaba
un
total
superior
samples
capitulo
conclusiones
las
tecnologıas
informacion
facilitan
vida
personas
sinnume
ro
actividades
incluyendo
comunicaciones
comercio
viajes
estudio
trabajo
y
muchas
otras
los
hackers
tambien
utilizan
tecnologıa
lograr
propositos
aprovechando
vulnerabilidades
sistemas
robar
informacion
obtener
acceso
cualquier
actividad
ﬁn
malicioso
los
metodos
tradicionales
de
deteccion
utilizados
antivirus
resultan
efectivos
avances
la
evolucion
ataques
expertos
ciencia
datos
junto
comunidad
anti
malware
aunado
sus
esfuerzos
encontrar
solucion
pueda
hacer
frente
problematica
esto
alento
empresas
dedicadas
industria
software
antivirus
comenzar
a
utilizar
tecnicas
machine
learning
deep
learning
desarrollo
produc
tos
motivados
avances
logrados
campo
decidio
dar
inicio
esta
investigacion
intentar
poner
prueba
dichas
aﬁrmaciones
de
modo
lugar
extenso
numero
tareas
data
mining
dedicadas
a
procesar
conjunto
datos
comprendido
cerca
once
mil
archivos
malware
en
formato
asm
bytes
objetivo
extraer
atributos
estipu
laron
podrıan
llegar
ser
representativos
comportamiento
ﬁsonomıa
de
programas
esta
etapa
investigacion
mas
demandante
requiriendo
solo
es
critura
procesos
extraer
datos
necesarios
sino
tambien
construccion
de
herramientas
permitieran
visualizar
informacion
obtenida
cada
etapa
con
ﬁn
validar
resultados
obtenidos
detectar
errores
manera
temprana
y
apoyar
toma
decisiones
cuestiones
establecimiento
punto
de
corte
seleccion
atributos
relevantes
capitulo
conclusiones
una
vez
consolidados
datos
dataset
unico
fase
preprocesamiento
y
depurado
permitio
evaluar
distintos
aspectos
datos
recabados
reaﬁr
mando
ası
suposiciones
hechas
comienzo
proceso
data
mining
la
importancia
tamanos
archivos
tambien
permitio
descubrir
median
te
estudio
relevancia
correlacion
variables
relaciones
subyacentes
y
descartar
features
habıamos
supuesto
podrıan
ser
utilidad
los
snapshots
primeros
bytes
archivos
asm
ası
mismo
utilizacion
de
distintas
tecnicas
permitio
considerable
reduccion
dimensionalidad
del
problema
sacriﬁcio
practicamente
insigniﬁcante
performance
finalmente
elaboracion
distintos
modelos
machine
learning
cla
siﬁcacion
familias
malware
permitio
poner
prueba
eﬁcacia
datos
extraıdos
obteniendo
resultados
superiores
precision
ca
sos
incluso
superiores
red
neuronal
siquiera
tener
realizar
un
tuning
parametros
por
lado
sobreajuste
observado
modelos
clasiﬁcacion
binaria
nos
llevo
estudio
mas
profundos
distintas
alternativas
resolver
este
problema
aunque
deﬁnitiva
tamano
pequeno
conjunto
datos
resulto
ser
inconveniente
imposible
eludir
en
conclusion
cuenta
resultados
obtenidos
posible
aﬁrmar
que
la
utilizacion
tecnicas
data
mining
machine
learning
clasiﬁcar
familias
de
malware
resultado
efectivas
en
cuanto
clasiﬁcacion
binaria
estamos
seguros
que
contar
conjunto
datos
considerablemente
mas
grande
en
el
orden
miles
realizar
entrenamiento
algoritmos
podrıan
lograr
resultados
ampliamente
superiores
alcanzados
trabajo
capitulo
trabajos
futuros
a
continuacion
enunciaran
caracterısticas
podrıan
resultar
intere
santes
abordar
menos
ser
cuenta
investigacion
futura
seguir
lınea
ejecucion
jumps
momento
extraer
codigos
de
operacion
instrucciones
encuentren
dentro
loops
se
ejecutaran
multiples
veces
solo
una
explorar
distintos
valores
corte
momento
seleccionar
features
mas
relevantes
conjunto
datos
promediar
longitud
secciones
archivos
asm
lugar
solo
contar
cantidad
ocurrencias
formar
ngramas
contenido
archivos
bytes
extender
clasiﬁcacion
incluyendo
mas
familias
malwares
entrenar
algoritmos
clasiﬁcacion
binaria
dataset
considerable
mente
mayor
utilizado
poder
garantizar
archivos
benignos
realmente
benignos
contar
dataset
mas
equilibrados
cantidades
archivos
que
componen
cada
clase
sen
mas
parejas
buscar
nuevas
tecnicas
puedan
reducir
overﬁtting
clasiﬁcacion
binaria
establecer
mejor
ajuste
hiperparametros
algoritmos
clasiﬁ
cacion
binaria
implementar
red
neuronal
profunda
mas
compleja
clasiﬁcacion
de
las
imagenes
escala
grises
bibliografıa
tushar
sharma
dipanjan
sarkar
raghav
bali
practical
machine
learning
with
python
bangalore
karnataka
india
apress
christopher
c
elisan
advanced
malware
analysis
estados
unidos
mc
graw
hill
education
mark
a
hall
christopher
j
pal
ian
hwitten
eibe
frank
data
mining
practi
cal
machine
learning
tools
and
techniques
cambridge
miami
estados
unidos
morgan
kaufmann
t
hastie
j
gareth
d
witten
r
tibshirani
an
introduction
to
statistical
lear
ning
new
york
springer
matthew
kirk
thoughtful
machine
learning
with
python
sebastopol
california
estados
unidos
oreilly
h
liu
m
cocea
granular
computing
based
machine
learning
suiza
springer
t
mitchell
machine
learning
mcgraw
hill
stuart
j
russell
peter
norvig
artiﬁcial
intelligent
a
modern
approach
thrid
edition
inglaterra
pearson
scikitlearn
scikitlearn
tutorials
httpsscikitlearnorgstabletutorial
indexhtml
m
sikorski
a
honig
practical
malware
analysis
san
francisco
no
starch
press
mark
stamp
introduction
to
machine
learning
with
applications
in
information
security
boca
raton
florida
crc
press
tensorﬂow
tensorﬂow
tutorials
httpswwwtensorfloworgtutorials
yehezkel
s
resheff
itay
lieder
tom
hope
learning
tensorﬂow
sebastopol
california
estados
unidos
oreilly
jose
unpingco
python
for
probability
statistics
and
machine
learning
san
die
go
california
estados
unidos
springer
jake
vanderplas
python
data
science
handbook
sebastopol
california
estados
unidos
oreilly
bibliografia
university
of
virginia
computer
science
x
assembly
guide
httpwww
csvirginiaeduevanscsguidesxhtml
summet
dua
xian
du
data
mining
and
machine
learning
in
cybersecurity
boca
raton
florida
estados
unidos
crc
press